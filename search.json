[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "On the R-way to hell",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-aim-of-this-book",
    "href": "index.html#the-aim-of-this-book",
    "title": "On the R-way to hell",
    "section": "The aim of this book",
    "text": "The aim of this book\nThe aim of this book is two-fold:\n\nintroduce you to R, a powerful and flexible interactive environment for statistical computing and research.\nintroduce you to (or reacquaint you with) statistical analysis done in R.\n\nR in itself is not difficult to learn, but as with learning any new language (spoken or computer) the initial learning curve can be steep and somewhat daunting. It is not intended to cover everything (neither with R not with statistics) but simply to help you climb the initial learning curve (potentially faster) and provide you with the basic skills (and confidence!) needed to start your own journey with R and with specific analysis.",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#multilingual-book",
    "href": "index.html#multilingual-book",
    "title": "On the R-way to hell",
    "section": "Multilingual book",
    "text": "Multilingual book\nThe book is provided as a multilingual book breaking that language barrier and potentially allow to facilitate the learn of R and its mainly english-speaking environment. We are always looking for volunteers to help developed the book further and add more languages to the growing list. Please contact us if you want to help\nOn the web version of the book, use  in the navigation bar to switch from one language to another. After switching to your preferred language, you can of course also download the pdf and epub versions in this language if you want to using .\nList of languages:\n\nenglish (work in progresspublish but need polishing)\nfrench (in development, waiting for english to be polished)\nspanish (one day maybe)\n‚Ä¶ volunteers for more ??",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "On the R-way to hell",
    "section": "How to use this book",
    "text": "How to use this book\n\nFor the best experience we recommend that you read the web version of this book which you can find https://biostats-uottawa.github.io/Rway.\nThe web version includes a navbar at the top of the page where you can toggle the sidebars on and off , search through the book , change the page color  and suggest revisions if you spot a typo or mistake . You can also download  a pdf and epub versions of the book.\nWe use a few typographical conventions throughout this book.\nR code and the resulting output are presented in code blocks in our book.\n\n2 + 2\n\n[1] 4\n\n\nFunctions in the text are shown with brackets at the end using code font, i.e.¬†mean() or sd() etc.\nObjects are shown using code font without the round brackets, i.e. obj1, obj2 etc.\nR packages in the text are shown using code font and followed by the üì¶ icon, i.e.¬†tidyverse üì¶.\nA series of actions required to access menu commands in RStudio or VSCode are identified as File -&gt; New File -&gt; R Script which translates to ‚Äòclick on the File menu, then click on New File and then select R Script‚Äô.\nWhen we refer to IDE (Integrated Development Environment software) later in the text we mean either RStudio of VScode.\nWhen we refer to .[Rq]md, we mean either R markdown (.Rmd) or Quarto (.qmd) documents and would generally talk of R markdown documents when referring to either .Rmd or .qmd files.\nThe manual tries to highlight some part of the text using the following boxes and icons.\n\n\n\n\n\n\nExercises\n\n\n\nStuff for you to do\n\n\n\n\n\n\n\n\nSolutions\n\n\n\nR code and explanations\n\n\n\n\n\n\n\n\nWarning\n\n\n\nwarnings\n\n\n\n\n\n\n\n\nImportant\n\n\n\nimportant points\n\n\n\n\n\n\n\n\nNote\n\n\n\nnotes",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-who",
    "href": "index.html#sec-who",
    "title": "On the R-way to hell",
    "section": "Who are we¬†?",
    "text": "Who are we¬†?\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\n\n\nJulien Martin is a Professor at the University of Ottawa working on Evolutionary Ecology and has discovered R with version 1.8.1 and teaches R since v2.4.0.\n\n\n: uOttawa page, lab page\n\n\n: jgamartin\n\n\n: juliengamartin",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "On the R-way to hell",
    "section": "Thanks",
    "text": "Thanks\nThe first part of the book on using R started as a fork on github from the excellent An introduction to R book by Douglas, Roos, Mancini, Couto and Lusseau (Douglas 2023). It was forked on April 23rd, 2023 from Alexd106 github repository then modified and updated following my own needs and teaching perspective of R. The content was neither reviewed nor endorsed by any the previous developers.\nSeveral parts in the book were based on previous lab manuals for biostatistics classes at uOttawa written by Martin, Findlay, Morin and Rundle.\nSite that provided a lot of information for the book:\n\ndplyr introduction\nIntroduction to gam\nIntoduction to gams by Noam Ross",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#image-credits",
    "href": "index.html#image-credits",
    "title": "On the R-way to hell",
    "section": "Image credits",
    "text": "Image credits\nPhotos, images and screenshots are from Julien Martin except when indicated in caption.\nCover image was generated via Nightcafe Ai Art generator. Favicon and hex sticker were created from the cover image.\n\n\n\n\n\n\nNote\n\n\n\nseveral screenshot are currently by Alex Douglas and are being redone to abide by the previous statement",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "On the R-way to hell",
    "section": "License",
    "text": "License\nI share this work under the license License Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n\n\nLicense Creative Commons\n\nIf you teach R, feel free to use some or all of the content in this book to help support your own students. The only thing I ask is that you acknowledge the original source and authors. If you find this book useful or have any comments or suggestions I would love to hear from you (contact info).",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citing-the-book",
    "href": "index.html#citing-the-book",
    "title": "On the R-way to hell",
    "section": "Citing the book",
    "text": "Citing the book\nJulien Martin. (2024). On the R-way to hell. A multilingual introduction to R book. Version: 0.6.0 (2024-10-02).DOI: 10.5281/zenodo.13801263",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#chapters-to-read",
    "href": "index.html#chapters-to-read",
    "title": "On the R-way to hell",
    "section": "Course associated reading",
    "text": "Course associated reading\n\n\n\nTable¬†1: Course associated reading for biostatistical course at uOttawa\n\n\n\n\n\n\n\n\n\n\n\nChapter\nBioXx58 \nBio8940 \n\n\n\nUsing R\n\n\n1.-4.\n‚úÖ‚úÖ\nüòÉ\n\n\n5. Programming\n\n‚úÖ‚úÖ\n\n\n6. Reproducible reports\n‚úîÔ∏è\n‚úÖ‚úÖ\n\n\n7. Version control\n\n‚úÖ‚úÖ\n\n\nStats fundamentals\n\n\nall chapters\n‚úÖ‚úÖ\nüòÉ\n\n\nLinear models\n\n\nall chapters\n‚úÖ‚úÖ\nüòÉ\n\n\nGeneralized linear models\n\n\nall chapters\n‚úîÔ∏è\n‚úÖ‚úÖ\n\n\nMixed models\n\n\nall chapters\n\n‚úÖ‚úÖ\n\n\nGeneralized additive models\n\n\nall chapters\n\n‚úîÔ∏è\n\n\nMultivariate analysis\n\n\nall chapters\n\n‚úîÔ∏è\n\n\nBayesian approach\n\n\nall chapters\n\n‚úÖ‚úÖ\n\n\n\nSuggested ‚úîÔ∏è ; mandatory ‚úÖ‚úÖ ; expected knowledge (might need a refresher) üòÉ",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#hex-sticker",
    "href": "index.html#hex-sticker",
    "title": "On the R-way to hell",
    "section": "Hex Sticker",
    "text": "Hex Sticker\n\n\n\n\n\n\n\n\nDouglas, A. 2023. An introduction to r.",
    "crumbs": [
      "Data & Code",
      "Preface"
    ]
  },
  {
    "objectID": "01-start.html",
    "href": "01-start.html",
    "title": "1¬† Getting started",
    "section": "",
    "text": "Some R pointers\nGood luck and don‚Äôt forget to have fun.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#some-r-pointers",
    "href": "01-start.html#some-r-pointers",
    "title": "1¬† Getting started",
    "section": "",
    "text": "Use R often and use it regularly. This will help build and maintain all important momentum.\nLearning R is not a memory test. One of advantage of a scripting language is that you will always have your (well annotated) code to refer back to when you forget how to do something.\nYou don‚Äôt need to know everything about R to be productive.\nIf you get stuck, search online, it‚Äôs not cheating and writing a good search query is a skill in itself.\nIf you find yourself staring at code for hours trying to figure out why it‚Äôs not working then walk away for a few minutes.\nIn R there are many ways to tackle a particular problem. If your code does what you want it to do in a reasonable time and robustly then don‚Äôt worry about it.\nR is just a tool to help you answer your interesting questions. Don‚Äôt lose sight of what‚Äôs important - your research question(s) and your data. No amount of skill using R will help if your data collection is fundamentally flawed or your question vague.\nRecognize that there will be times when things will get a little tough or frustrating. Try to accept these periods as part of the natural process of learning a new skill (we‚Äôve all been there) and remember, the time and energy you invest now will be more than payed back in the not too distant future.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#installation",
    "href": "01-start.html#installation",
    "title": "1¬† Getting started",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\n\n1.1.1 Installing R\nTo get up and running the first thing you need to do is install R. R is freely available for Windows, Mac and Linux operating systems from the Comprehensive R Archive Network (CRAN) website. For Windows and Mac users we suggest you download and install the pre-compiled binary versions. There are reasonably comprehensive instruction to install R for each OS (Windows,Mac or linux).\nWhichever operating system you‚Äôre using, once you have installed R you need to check its working properly. The easiest way to do this is to start R by double clicking on the R icon (Windows or Mac) or by typing R into the Console (Linux). You should see the R Console and you should be able to type R commands into the Console after the command prompt &gt;. Try typing the following R code and then press enter:\n\nplot(1)\n\n\n\n\n\n\nFigure¬†1.1: Most amazing plot, just useful to test R\n\n\n\n\nA plot with a single point in the center should appear. If you see this, you‚Äôre good to go. If not then we suggest you make a note of any errors produced and then use your preferred search engine to troubleshoot.\n\n1.1.2 Installing an IDE\nWe strongly recommend to use an Integrated Development Environment (IDE) software to work with R. One simple and extremely popular IDE is RStudio. An alternative to RStudio is Visual Studio Code, or VSCode. An IDE can be thought of as an add-on to R which provides a more user-friendly interface, incorporating the R Console, a script editor and other useful functionality (like R markdown and Git Hub integration).\n\n\n\n\n\n\nCaution\n\n\n\nYou must install R before you install an IDE (see Section 1.1.1 for details).\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we refer to IDE later in the text we mean either RStudio of VScode\n\n\n\n1.1.2.1 RStudio\nRStudio is freely available for Windows, Mac and Linux operating systems and can be downloaded from the RStudio site. You should select the ‚ÄòRStudio Desktop‚Äô version.\n\n1.1.2.2 VSCode\nVSCode is freely available for Windows, Mac and Linux operating systems and can be downloaded from the VS Code site.\nIn addition you need to install the R extension to VSCode. To make VSCode a true powerhouse for working with R we strongly recommend you to also install:\n\n\nradian: A modern R console that corrects many limitations of the official R terminal and supports many features such as syntax highlighting and auto-completion.\n\nVSCode-R-Debugger: A VS Code extension to support R debugging capabilities.\n\nhttpgd: An R package üì¶ to provide a graphics device that asynchronously serves SVG graphics via HTTP and WebSockets.\n\n1.1.2.3 Alternatives to RStudio and VSCode\nRather than using an ‚Äòall in one‚Äô IDE many people choose to use R and a separate script editor to write and execute R code. If you‚Äôre not familiar with what a script editor is, you can think of it as a bit like a word processor but specifically designed for writing code. Happily, there are many script editors freely available so feel free to download and experiment until you find one you like. Some script editors are only available for certain operating systems and not all are specific to R. Suggestions for script editors are provided below. Which one you choose is up to you: one of the great things about R is that YOU get to choose how you want to use R.\n\n1.1.2.3.1 Advanced text editors\nA light yet efficient way to work with R is using advanced text editors such as:\n\n\nAtom (all operating systems)\n\nBBedit (Mac OS)\n\ngedit (Linux; comes with most Linux distributions)\n\nMacVim (Mac OS)\n\nNano (Linux)\n\nNotepad++ (Windows)\n\nSublime Text (all operating systems)\n\nvim and its extension NVim-R (Linux)\n\n1.1.2.3.2 Integrated development environments\nThese environments are more powerful than simple text editors, and are similar to RStudio:\n\n\nEmacs and its extension Emacs Speaks Statistics (all operating systems)\n\nRKWard (Linux)\n\nTinn-R (Windows)",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-orient",
    "href": "01-start.html#sec-orient",
    "title": "1¬† Getting started",
    "section": "\n1.2 IDE orientation",
    "text": "1.2 IDE orientation\n\n1.2.1 RStudio\nWhen you open R studio for the first time you should see the following layout (it might look slightly different on a Windows computer).\n\n\n\n\n\n\n\nFigure¬†1.2: R studio main window\n\n\n\n\nThe large window (aka pane) on the left is the Console window. The window on the top right is the Environment / History / Connections pane and the bottom right window is the Files / Plots / Packages / Help / Viewer window. We will discuss each of these panes in turn below. You can customise the location of each pane by clicking on the ‚ÄòTools‚Äô menu then selecting Global Options ‚Äì&gt; Pane Layout. You can resize the panes by clicking and dragging the middle of the window borders in the direction you want. There are a plethora of other ways to customise RStudio.\n\n1.2.1.1 Console\nThe Console is the workhorse of R. This is where R evaluates all the code you write. You can type R code directly into the Console at the command line prompt, &gt;. For example, if you type 2 + 2 into the Console you should obtain the answer 4 (reassuringly). Don‚Äôt worry about the [1] at the start of the line for now.\n\n\n\n\n\n\n\nFigure¬†1.3: R studio console view\n\n\n\n\nHowever, once you start writing more R code this becomes rather cumbersome. Instead of typing R code directly into the Console a better approach is to create an R script. An R script is just a plain text file with a .R file extension which contains your lines of R code. These lines of code are then sourced into the R Console line by line. To create a new R script click on the ‚ÄòFile‚Äô menu then select New File ‚Äì&gt; R Script.\n\n\n\n\n\n\n\nFigure¬†1.4: R studio creating a new script file\n\n\n\n\nNotice that you have a new window (called the Source pane) in the top left of RStudio and the Console is now in the bottom left position. The new window is a script editor and where you will write your code.\n\n\n\n\n\n\n\nFigure¬†1.5: R studio main view with a new script\n\n\n\n\nTo source your code from your script editor to the Console simply place your cursor on the line of code and then click on the ‚ÄòRun‚Äô button in the top right of the script editor pane.\n\n\n\n\n\n\n\nFigure¬†1.6: R studio run button\n\n\n\n\nYou should see the result in the Console window. If clicking on the ‚ÄòRun‚Äô button starts to become tiresome you can use the keyboard shortcut ‚Äòctrl + enter‚Äô (on Windows and Linux) or ‚Äòcmd + enter‚Äô (on Mac). You can save your R scripts as a .R file by selecting the ‚ÄòFile‚Äô menu and clicking on save. Notice that the file name in the tab will turn red to remind you that you have unsaved changes. To open your R script in RStudio select the ‚ÄòFile‚Äô menu and then ‚ÄòOpen File‚Ä¶‚Äô. Finally, its worth noting that although R scripts are saved with a .R extension they are actually just plain text files which can be opened with any text editor.\n\n1.2.1.2 Environment/History/Connections\nThe Environment / History / Connections window shows you lots of useful information. You can access each component by clicking on the appropriate tab in the pane.\n\nThe ‚ÄòEnvironment‚Äô tab displays all the objects you have created in the current (global) environment. These objects can be things like data you have imported or functions you have written. Objects can be displayed as a List or in Grid format by selecting your choice from the drop down button on the top right of the window. If you‚Äôre in the Grid format you can remove objects from the environment by placing a tick in the empty box next to the object name and then click on the broom icon. There‚Äôs also an ‚ÄòImport Dataset‚Äô button which will import data saved in a variety of file formats. However, we would suggest that you don‚Äôt use this approach to import your data as it‚Äôs not reproducible and therefore not robust (see Chapter 3 for more details).\nThe ‚ÄòHistory‚Äô tab contains a list of all the commands you have entered into the R Console. You can search back through your history for the line of code you have forgotten, send selected code back to the Console or Source window. We usually never use this as we always refer back to our R script.\nThe ‚ÄòConnections‚Äô tab allows you to connect to various data sources such as external databases.\n\n1.2.1.3 Files/Plots/Packages/Help/Viewer\n\nThe ‚ÄòFiles‚Äô tab lists all external files and directories in the current working directory on your computer. It works like file explorer (Windows) or Finder (Mac). You can open, copy, rename, move and delete files listed in the window.\nThe ‚ÄòPlots‚Äô tab is where all the plots you create in R are displayed (unless you tell R otherwise). You can ‚Äòzoom‚Äô into the plots to make them larger using the magnifying glass button, and scroll back through previously created plots using the arrow buttons. There is also the option of exporting plots to an external file using the ‚ÄòExport‚Äô drop down menu. Plots can be exported in various file formats such as jpeg, png, pdf, tiff or copied to the clipboard (although you are probably better off using the appropriate R functions to do this - see [Chapter 4 for more details).\nThe ‚ÄòPackages‚Äô tab lists all of the packages that you have installed on your computer. You can also install new packages and update existing packages by clicking on the ‚ÄòInstall‚Äô and ‚ÄòUpdate‚Äô buttons respectively.\nThe ‚ÄòHelp‚Äô tab displays the R help documentation for any function. We will go over how to view the help files and how to search for help in Chapter 2).\nThe ‚ÄòViewer‚Äô tab displays local web content such as web graphics generated by some packages.\n\n1.2.2 VSCode\n\n\n\n\n\n\n\nFigure¬†1.7: VSCode window overview\n\n\n\n\n\n1.2.2.1 Left panel\nContains :\n\nFile manager and file outline\nR support including R environment/ R search / R help / install packages\nGithub interaction\n\n\n\n\n\n\n\n\n\n\n(a) file pane\n\n\n\n\n\n\n\n\n\n(b) git pane\n\n\n\n\n\n\n\n\n\n(c) R pane\n\n\n\n\n\n\nFigure¬†1.8: VS Code left panel\n\n\n\n1.2.2.2 Editor tabs\nIncludes:\n\nplot panel (with history and navigation)\nedition of scripts\npreview panels\n\n\n\n\n\n\n\n\nFigure¬†1.9: VSCode editor tabs and preview panels\n\n\n\n\n\n1.2.2.3 Terminal window\nContains:\n\nthe terminal allowing to have an R session or any other type of terminals needed (bash/tmux/). It can be split and run multiple sessions at the same time\na problems pane highlighting both grammar and coding problems\n\n\n\n\n\n\n\n\nFigure¬†1.10: VSCode terminal window",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-work-d",
    "href": "01-start.html#sec-work-d",
    "title": "1¬† Getting started",
    "section": "\n1.3 Working directories",
    "text": "1.3 Working directories\nThe working directory is the default location where R will look for files you want to load and where it will put any files you save. One of the great things about using RStudio Projects is that when you open a project it will automatically set your working directory to the appropriate location. You can check the file path of your working directory by using either getwd() or here() functions.\n\ngetwd()\n\n[1] \"/home/julien/Documents/courses/biostats/livre/Rway\"\n\n\nIn the example above, the working directory is a folder called ‚ÄòRway‚Äô which is a subfolder of ‚Äúbiostats‚Äô in the ‚Äòcourses‚Äô folder which in turn is in a ‚ÄòDocuments‚Äô folder located in the ‚Äòjulien‚Äô folder which itself is in the ‚Äòhome‚Äô folder. On a Windows based computer our working directory would also include a drive letter (i.e. C:\\home\\julien\\Documents\\courses\\biostats\\Rway).\nIf you weren‚Äôt using an IDE then you would have to set your working directory using the setwd() function at the start of every R script (something we did for many years).\n\nsetwd(\"/home/julien/Documents/courses/biostats/Rway/\")\n\nHowever, the problem with setwd() is that it uses an absolute file path which is specific to the computer you are working on. If you want to send your script to someone else (or if you‚Äôre working on a different computer) this absolute file path is not going to work on your friend/colleagues computer as their directory configuration will be different (you are unlikely to have a directory structure /home/julien/Documents/courses/biostats/ on your computer). This results in a project that is not self-contained and not easily portable. IDEs solves this problem by allowing you to use relative file paths which are relative to the Root project directory. The Root project directory is just the directory that contains the .Rproj file in Rstudio (first_project.Rproj in our case) or the base folder of your workspace in VScode. If you want to share your analysis with someone else, all you need to do is copy the entire project directory and send to your to your collaborator. They would then just need to open the project file and any R scripts that contain references to relative file paths will just work. For example, let‚Äôs say that you‚Äôve created a subdirectory called data in your Root project directory that contains a csv delimited datile called mydata.csv (we will cover directory structures below in Section 1.4). To import this datile in an RStudio project using the read.csv() function (don‚Äôt worry about this now, we will cover this in much more detail in Chapter 3) all you need to include in your R script is\ndat &lt;- read.csv(\"data/mydata.csv\")\nBecause the file path data/mydata.csv is relative to the project directory it doesn‚Äôt matter where you collaborator saves the project directory on their computer it will still work.\nIf you weren‚Äôt using an RStudio project or VScode workspace then you would need to either set the working directory providing the full path to your directory or specify the full path of the data file. Neither option would be reproducible on other computers.\nsetwd(\"/home/julien/Documents/courses/biostats/Rway\")\n\ndat &lt;- read.csv(\"data/mydata.csv\")\nor\ndat &lt;- read.csv(\"/home/julien/Documents/courses/biostats/Rway/data/mydata.csv\")\nFor those of you who want to take the notion of relative file paths a step further, take a look at the here() function in the here package. The here() function allows you to build file paths for any file relative to the project root directory that are also operating system agnostic (works on a Mac, Windows or Linux machine). For example, to import our mydata.csv file from the data directory just use:\nlibrary(here) # you may need to install the here package first\ndat &lt;- read.csv(here(\"data\", \"mydata.csv\"))",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-dir-struc",
    "href": "01-start.html#sec-dir-struc",
    "title": "1¬† Getting started",
    "section": "\n1.4 Directory structure",
    "text": "1.4 Directory structure\nIn addition to using RStudio Projects, it‚Äôs also really good practice to structure your working directory in a consistent and logical way to help both you and your collaborators. We frequently use the following directory structure in our R based projects.\n\n\n\n\n\n\nroot\nrootdot01\nroot-&gt;dot01\ndot1\nroot-&gt;dot1\ndata\ndatadot21\ndata-&gt;dot21\nfunctions\nfunctionsdot22\nfunctions-&gt;dot22\noutputs\noutputsdot23\noutputs-&gt;dot23\nscripts\nscriptsdot24\nscripts-&gt;dot24\nwd\nyour working directoryLOT1\nraw processed metadataLOT2\nR functionsLOT4\nanalysis scripts R markdown documentsLOT3\npdf html figuresdot01-&gt;wd\ndot1-&gt;data\ndot2\ndot1-&gt;dot2\ndot2-&gt;functions\ndot3\ndot2-&gt;dot3\ndot3-&gt;outputs\ndot4\ndot3-&gt;dot4\ndot4-&gt;scripts\ndot21-&gt;LOT1\ndot22-&gt;LOT2\ndot23-&gt;LOT3\ndot24-&gt;LOT4\n\n\n\n\nFigure¬†1.11: Recommended directory structure for analysis with R\n\n\n\n\nIn our working directory we have the following directories:\n\nRoot - This is your project directory containing your .Rproj file. We tend to keep all the R scripts or [Rq]md document necessary for the analysis / report in this root folder or in the scripts folder when there are too many.\ndata - We store all our data in this directory. The subdirectory called data contains raw data files and only raw data files. These files should be treated as read only and should not be changed in any way. If you need to process/clean/modify your data do this in R (not MS Excel) as you can document (and justify) any changes made. Any processed data should be saved to a separate file and stored in the processed_data subdirectory. Information about data collection methods, details of data download and any other useful metadata should be saved in a text document (see README text files below) in the metadata subdirectory.\nfunctions - This is an optional directory where we save all of the custom R functions we‚Äôve written for the current analysis. These can then be sourced into R using the source() function.\nscripts - An optional directory where we save our R markdown documents and/or the main R scripts we have written for the current project are saved here if not in the root folder.\noutput - Outputs from our R scripts such as plots, HTML files and data summaries are saved in this directory. This helps us and our collaborators distinguish what files are outputs and which are source files.\n\nOf course, the structure described above is just what works for us most of the time and should be viewed as a starting point for your own needs. We tend to have a fairly consistent directory structure across our projects as this allows us to quickly orientate ourselves when we return to a project after a while. Having said that, different projects will have different requirements so we happily add and remove directories as required.\nYou can create your directory structure using Windows Explorer (or Finder on a Mac) or within your IDE by clicking on the ‚ÄòNew folder‚Äô button in the ‚ÄòFiles‚Äô pane.\nAn alternative approach is to use the dir.create() functions in the R Console.\n# create directory called 'data'\ndir.create(\"data\")",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-rsprojs",
    "href": "01-start.html#sec-rsprojs",
    "title": "1¬† Getting started",
    "section": "\n1.5 Projects organisation",
    "text": "1.5 Projects organisation\nAs with most things in life, when it comes to dealing with data and data analysis things are so much simpler if you‚Äôre organized. Clear project organisation makes it easier for both you (especially the future you) and your collaborators to make sense of what you‚Äôve done. There‚Äôs nothing more frustrating than coming back to a project months (sometimes years) later and have to spend days (or weeks) figuring out where everything is, what you did and why you did it. A well documented project that has a consistent and logical structure increases the likelihood that you can pick up where you left off with minimal fuss no matter how much time has passed. In addition, it‚Äôs much easier to write code to automate tasks when files are well organized and are sensibly named. This is even more relevant nowadays as it‚Äôs never been easier to collect vast amounts of data which can be saved across 1000‚Äôs or even 100,000‚Äôs of separate data files. Lastly, having a well organized project reduces the risk of introducing bugs or errors into your workflow and if they do occur (which inevitably they will at some point), it makes it easier to track down these errors and deal with them efficiently.\nThere are also a few simple steps you can take right at the start of any project to help keep things shipshape.\nA great way of keeping things organized is to use RStudio Projects or VSCode workspaces, referred after as project. A project keeps all of your R scripts, R markdown documents, R functions and data together in one place. The nice thing about project is that each has its own directory, history and source documents so different analyses that you are working on are kept completely separate from each other. This means that you can very easily switch between projects without fear of them interfering with each other.\n\n1.5.1 RStudio\nTo create a project, open RStudio and select File -&gt; New Project... from the menu. You can create either an entirely new project, a project from an existing directory or a version controlled project (see the Chapter 7 for further details about this). In this Chapter we will create a project in a new directory.\n\n\n\n\n\n\n\nFigure¬†1.12: R Studio creating a Project step 1\n\n\n\n\nYou can also create a new project by clicking on the ‚ÄòProject‚Äô button in the top right of RStudio and selecting ‚ÄòNew Project‚Ä¶‚Äô\n\n\n\n\n\n\n\nFigure¬†1.13: R Studio creating a Project step 2\n\n\n\n\nIn the next window select ‚ÄòNew Project‚Äô.\n\n\n\n\n\n\n\nFigure¬†1.14: R Studio creating a Project step 3\n\n\n\n\nNow enter the name of the directory you want to create in the ‚ÄòDirectory name:‚Äô field (we‚Äôll call it first_project for this Chapter). If you want to change the location of the directory on your computer click the ‚ÄòBrowse‚Ä¶‚Äô button and navigate to where you would like to create the directory. We always tick the ‚ÄòOpen in new session‚Äô box as well. Finally, hit the ‚ÄòCreate Project‚Äô to create the new project.\n\n\n\n\n\n\n\nFigure¬†1.15: R Studio creating a Project step 4\n\n\n\n\nOnce your new project has been created you will now have a new folder on your computer that contains an RStudio project file called first_project.Rproj. This .Rproj file contains various project options (but you shouldn‚Äôt really interact with it) and can also be used as a shortcut for opening the project directly from the file system (just double click on it). You can check this out in the ‚ÄòFiles‚Äô tab in RStudio (or in Finder if you‚Äôre on a Mac or File Explorer in Windows).\n\n\n\n\n\n\n\nFigure¬†1.16: R Studio creating a Project final step\n\n\n\n\nThe last thing we suggest you do is select Tools -&gt; Project Options... from the menu. Click on the ‚ÄòGeneral‚Äô tab on the left hand side and then change the values for ‚ÄòRestore .RData into workspace at startup‚Äô and ‚ÄòSave workspace to .RData on exit‚Äô from ‚ÄòDefault‚Äô to ‚ÄòNo‚Äô. This ensures that every time you open your project you start with a clean R session. You don‚Äôt have to do this (many people don‚Äôt) but we prefer to start with a completely clean workspace whenever we open our projects to avoid any potential conflicts with things we have done in previous sessions (sometimes leading to surprising results and headaches figuring out the problem). The downside to this is that you will need to rerun your R code every time you open your project.\n\n\n\n\n\n\n\nFigure¬†1.17: R Studio creating a Project changing options\n\n\n\n\nNow that you have an RStudio project set up you can start creating R scripts (or R markdown /Quarto documents, see Chapter 6) or whatever you need to complete you project. All of the R scripts will now be contained within the RStudio project and saved in the project folder.\n\n1.5.2 VSCode\nworkspace are similar to RStudio projects. You however need to create a new folder with a R file (or text file) and save as workspace.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-file-names",
    "href": "01-start.html#sec-file-names",
    "title": "1¬† Getting started",
    "section": "\n1.6 File names",
    "text": "1.6 File names\nWhat you name your files matters more than you might think. Naming files is also more difficult than you think. The key requirement for a ‚Äògood‚Äô file name is that it‚Äôs informative whilst also being relatively short. This is not always an easy compromise and often requires some thought. Ideally you should try to avoid the following!\n\n\n\n\n\n\n\nFigure¬†1.18: File renaming song (source:https://xkcd.com/1459/)\n\n\n\n\nAlthough there‚Äôs not really a recognized standard approach to naming files (actually there is, just not everyone uses it), there are a couple of things to bear in mind.\n\nAvoid using spaces in file names by replacing them with underscores or hyphens. Why does this matter? One reason is that some command line software (especially many bioinformatic tools) won‚Äôt recognise a file name with a space and you‚Äôll have to go through all sorts of shenanigans using escape characters to make sure spaces are handled correctly. Even if you don‚Äôt think you will ever use command line software you may be doing so indirectly. Take R markdown for example, if you want to render an R markdown document to pdf using the rmarkdown üì¶ package you will actually be using a command line \\(\\LaTeX\\) engine under the hood. Another good reason not to use spaces in file names is that it makes searching for file names (or parts of file names) using regular expressions in R (or any other language) much more difficult.\nAvoid using special characters (i.e.¬†@¬£$%^&*(:/)) in your file names.\nIf you are versioning your files with sequential numbers (i.e. file1, file2, file3 ‚Ä¶). If you plan to have more than 9 files you should use 01, 02, 03, ‚Ä¶, 10 as this will ensure the files are listed in the correct order. If you plan to have more than 99 files then use 001, 002, 003, ‚Ä¶\nFor dates, use the ISO 8601 format YYYY-MM-DD (or YYYYMMDD) to ensure your files are listed in proper chronological order.\nNever use the word final in any file name - it extremely rarely is!\n\nWhatever file naming convention you decide to use, try to adopt early, stick with it and be consistent.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-proj-doc",
    "href": "01-start.html#sec-proj-doc",
    "title": "1¬† Getting started",
    "section": "\n1.7 Script documentation",
    "text": "1.7 Script documentation\nA quick note or two about writing R code and creating R scripts. Unless you‚Äôre doing something really quick and dirty we suggest that you always write your R code as an R script. R scripts are what make R so useful. Not only do you have a complete record of your analysis, from data manipulation, visualisation and statistical analysis, you can also share this code (and data) with friends, colleagues and importantly when you submit and publish your research to a journal. With this in mind, make sure you include in your R script all the information required to make your work reproducible (author names, dates, sampling design etc). This information could be included as a series of comments # or, even better, by mixing executable code with narrative into an R markdown document (Chapter 6). It‚Äôs also good practice to include the output of the sessionInfo() function at the end of any script which prints the R version, details of the operating system and also loaded packages. A really good alternative is to use the session_info() function from the xfun üì¶ package for a more concise summary of our session environment.\nHere‚Äôs an example of including meta-information at the start of an R script\n# Title: Time series analysis of lasagna consumption\n\n# Purpose : This script performs a time series analyses on\n#           lasagna meals kids want to have each week.\n#           Data consists of counts of (dreamed) lasagna meals per week\n#           collected from 24 kids at the \"Food-dreaming\" school\n#           between 2042 and 2056.\n\n# data file: lasagna_dreams.csv\n\n# Author: A. Stomach\n# Contact details: a.stomach@food.uni.com\n\n# Date script created: Fri Mar 29 17:06:44 2010 -----------\n# Date script last modified: Thu Dec 12 16:07:12 2019 ----\n\n# package dependencies\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nprint(\"put your lovely R code here\")\n\n# good practice to include session information\n\nxfun::session_info()\nThis is just one example and there are no hard and fast rules so feel free to develop a system that works for you. A really useful shortcut in RStudio is to automatically include a time and date stamp in your R script. To do this, write ts where you want to insert your time stamp in your R script and then press the ‚Äòshift + tab‚Äô keys. RStudio will convert ts into the current date and time and also automatically comment out this line with a #. Another really useful RStudio shortcut is to comment out multiple lines in your script with a # symbol. To do this, highlight the lines of text you want to comment and then press ‚Äòctrl + shift + c‚Äô (or ‚Äòcmd + shift + c‚Äô on a mac). To uncomment the lines just use ‚Äòctrl + shift + c‚Äô again.\nIn addition to including metadata in your R scripts it‚Äôs also common practice to create a separate text file to record important information. By convention these text files are named README. We often include a README file in the directory where we keep our raw data. In this file we include details about when data were collected (or downloaded), how data were collected, information about specialised equipment, preservation methods, type and version of any machines used (i.e. sequencing equipment) etc. You can create a README file for your project in RStudio by clicking on the File -&gt; New File -&gt; Text File menu.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#r-style-guide",
    "href": "01-start.html#r-style-guide",
    "title": "1¬† Getting started",
    "section": "\n1.8 R style guide",
    "text": "1.8 R style guide\nHow you write your code is more or less up to you although your goal should be to make it as easy to read as possible (for you and others). Whilst there are no rules (and no code police), we encourage you to get into the habit of writing readable R code by adopting a particular style. We suggest that you follow Google‚Äôs R style guide whenever possible. This style guide will help you decide where to use spaces, how to indent code and how to use square [ ] and curly { } brackets amongst other things.\nTo help you with code formatting:\n\nVSCode there is an embedded formatter in the R extension for VSCode. You can just use the keyboard shortcut to reformat the code nicely and automatically.\nRStudio you can install the styler üì¶ package which includes an RStudio add-in to allow you to automatically restyle selected code (or entire files and projects) with the click of your mouse. You can find more information about the styler üì¶ package including how to install here. Once installed, you can highlight the code you want to restyle, click on the ‚ÄòAddins‚Äô button at the top of RStudio and select the ‚ÄòStyle Selection‚Äô option. Here is an example of poorly formatted R code\n\n\n\n\n\n\n\n\nFigure¬†1.19: Poorly styled code\n\n\n\n\nNow highlight the code and use the styler üì¶ package to reformat\n\n\n\n\n\n\n\nFigure¬†1.20: Styling code with styler\n\n\n\n\nTo produce some nicely formatted code\n\n\n\n\n\n\n\nFigure¬†1.21: Nicely styled code",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#backing-up-projects",
    "href": "01-start.html#backing-up-projects",
    "title": "1¬† Getting started",
    "section": "\n1.9 Backing up projects",
    "text": "1.9 Backing up projects\nDon‚Äôt be that person who loses hard won (and often expensive) data and analyses. Don‚Äôt be that person who thinks it‚Äôll never happen to me - it will! Always think of the absolute worst case scenario, something that makes you wake up in a cold sweat at night, and do all you can to make sure this never happens. Just to be clear, if you‚Äôre relying on copying your precious files to an external hard disk or USB stick this is NOT an effective backup strategy. These things go wrong all the time as you lob them into your rucksack or ‚Äòbag for life‚Äô and then lug them between your office and home. Even if you do leave them plugged into your computer what happens when the building burns down (we did say worst case!)?\nIdeally, your backups should be offsite and incremental. Happily there are numerous options for backing up your files. The first place to look is in your own institute. Most (all?) Universities have some form of network based storage that should be easily accessible and is also underpinned by a comprehensive disaster recovery plan. Other options include cloud based services such as Google Drive and Dropbox (to name but a few), but make sure you‚Äôre not storing sensitive data on these services and are comfortable with the often eye watering privacy policies.\nWhilst these services are pretty good at storing files, they don‚Äôt really help with incremental backups. Finding previous versions of files often involves spending inordinate amounts of time trawling through multiple files named ‚Äòfinal.doc‚Äô, ‚Äòfinal_v2.doc‚Äô and ‚Äòfinal_usethisone.doc‚Äô etc until you find the one you were looking for. The best way we know for both backing up files and managing different versions of files is to use Git and GitHub. To find out more about how you can use RStudio, Git and GitHub together see Chapter 7.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#citing-r-and-r-packages",
    "href": "01-start.html#citing-r-and-r-packages",
    "title": "1¬† Getting started",
    "section": "\n1.10 Citing R and R packages",
    "text": "1.10 Citing R and R packages\nMany people have invested huge amounts of time and energy making R the great piece of software you‚Äôre now using. If you use R in your work (and we hope you do) please remember to give appropriate credit by citing not only R but also all the packages you used. To get the most up to date citation for R you can use the citation() function.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nIf you want to cite a particular package you‚Äôve used for your data analysis, you can also use the citation() function to get the info.\n\ncitation(package = \"here\")\n\nTo cite package 'here' in publications use:\n\n  M√ºller K (2020). _here: A Simpler Way to Find Your Files_. R package\n  version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {here: A Simpler Way to Find Your Files},\n    author = {Kirill M√ºller},\n    year = {2020},\n    note = {R package version 1.0.1},\n    url = {https://CRAN.R-project.org/package=here},\n  }\n\n\nIn our view the most useful tool for citation is the package grateful üì¶ which allow you to generate the citing information in a file, as well as creating either a sentence or a table citing all packages used. This should become the standard in any manuscript honestly. See Table¬†1 for an example output produced with grateful.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "02-basics.html",
    "href": "02-basics.html",
    "title": "2¬† Some R basics",
    "section": "",
    "text": "2.1 Important considerations\nWe provide screenshot of RStudio but everything is really similar when using VSCode.\nBefore we continue, here are a few things to bear in mind as you work through this Chapter:",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#important-considerations",
    "href": "02-basics.html#important-considerations",
    "title": "2¬† Some R basics",
    "section": "",
    "text": "R is case sensitive i.e. A is not the same as a and anova is not the same as Anova.\nAnything that follows a # symbol is interpreted as a comment and ignored by R. Comments should be used liberally throughout your code for both your own information and also to help your collaborators. Writing comments is a bit of an art and something that you will become more adept at as your experience grows.\nIn R, commands are generally separated by a new line. You can also use a semicolon ; to separate your commands but we strongly recommend to avoid using it.\nIf a continuation prompt + appears in the console after you execute your code this means that you haven‚Äôt completed your code correctly. This often happens if you forget to close a bracket and is especially common when nested brackets are used ((((some command))). Just finish the command on the new line and fix the typo or hit escape on your keyboard (see point below) and fix.\nIn general, R is fairly tolerant of extra spaces inserted into your code, in fact using spaces is actively encouraged. However, spaces should not be inserted into operators i.e. &lt;- should not read &lt; - (note the space). See the style guide for advice on where to place spaces to make your code more readable.\nIf your console ‚Äòhangs‚Äô and becomes unresponsive after running a command you can often get yourself out of trouble by pressing the escape key (esc) on your keyboard or clicking on the stop icon in the top right of your console. This will terminate most current operations.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#first-step-in-the-console",
    "href": "02-basics.html#first-step-in-the-console",
    "title": "2¬† Some R basics",
    "section": "\n2.2 First step in the console",
    "text": "2.2 First step in the console\nIn Chapter 1, we learned about the R Console and creating scripts and Projects. We also saw how you write your R code in a script and then source this code into the console to get it to run (if you‚Äôve forgotten how to do this, pop back to the console section (-Section 1.2.1.1) to refresh your memory). Writing your code in a script means that you‚Äôll always have a permanent record of everything you‚Äôve done (provided you save your script) and also allows you to make loads of comments to remind your future self what you‚Äôve done. So, while you‚Äôre working through this Chapter we suggest that you create a new script (or RStudio Project) to write your code as you follow along.\nAs we saw in Chapter 1, at a basic level we can use R much as you would use a calculator. We can type an arithmetic expression into our script, then source it into the console and receive a result. For example, if we type the expression 1 + 1 and then source this line of code we get the answer 2 (üòÉ!)\n\n1 + 1\n\n[1] 2\n\n\nThe [1] in front of the result tells you that the observation number at the beginning of the line is the first observation. This is not much help in this example, but can be quite useful when printing results with multiple lines (we‚Äôll see an example below). The other obvious arithmetic operators are -, *, / for subtraction, multiplication and division respectively. Matrix multiplication operator is %*%. R follows the usual mathematical convention of order of operations. For example, the expression 2 + 3 * 4 is interpreted to have the value 2 + (3 * 4) = 14, not (2 + 3) * 4 = 20. There are a huge range of mathematical functions in R, some of the most useful include; log(), log10(), exp(), sqrt().\n\nlog(1) # logarithm to base e\n\n[1] 0\n\nlog10(1) # logarithm to base 10\n\n[1] 0\n\nexp(1) # natural antilog\n\n[1] 2.718282\n\nsqrt(4) # square root\n\n[1] 2\n\n4^2 # 4 to the power of 2\n\n[1] 16\n\npi # not a function but useful\n\n[1] 3.141593\n\n\nIt‚Äôs important to realize that when you run code as we‚Äôve done above, the result of the code (or value) is only displayed in the console. Whilst this can sometimes be useful it is usually much more practical to store the value(s) in aN object.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#objects-in-r",
    "href": "02-basics.html#objects-in-r",
    "title": "2¬† Some R basics",
    "section": "\n2.3 Objects in R",
    "text": "2.3 Objects in R\nAt the heart of almost everything you will do (or ever likely to do) in R is the concept that everything in R is an object. These objects can be almost anything, from a single number or character string (like a word) to highly complex structures like the output of a plot, a summary of your statistical analysis or a set of R commands that perform a specific task. Understanding how you create objects and assign values to objects is key to understanding R.\n\n2.3.1 Creating objects\nTo create an object we simply give the object a name. We can then assign a value to this object using the assignment operator &lt;- (sometimes called the gets operator). The assignment operator is a composite symbol comprised of a ‚Äòless than‚Äô symbol &lt; and a hyphen - .\n\nmy_obj &lt;- 32\n\nIn the code above, we created an object called my_obj and assigned it a value of the number 32 using the assignment operator (in our head we always read this as ‚Äòmy_obj is 32‚Äô). You can also use = instead of &lt;- to assign values but this is bad practice since it can lead to confusion later on when programming in R (see Chapter 5) and we would discourage you from using this notation.\nTo view the value of the object you simply type the name of the object\n\nmy_obj\n\n[1] 32\n\n\nNow that we‚Äôve created this object, R knows all about it and will keep track of it during this current R session. All of the objects you create will be stored in the current workspace and you can view all the objects in your workspace in RStudio by clicking on the ‚ÄòEnvironment‚Äô tab in the top right hand pane.\n\n\n\n\n\n\n\nFigure¬†2.1: RStudio Environment tab\n\n\n\n\nIf you click on the down arrow on the ‚ÄòList‚Äô icon in the same pane and change to ‚ÄòGrid‚Äô view RStudio will show you a summary of the objects including the type (numeric - it‚Äôs a number), the length (only one value in this object), its ‚Äòphysical‚Äô size and its value (48 in this case). In VSCode, go on the R extension pane, and you can obtain the same information.\n\n\n\n\n\n\n\nFigure¬†2.2: RStudio Environment tab in grid format\n\n\n\n\nThere are many different types of values that you can assign to an object. For example\n\nmy_obj2 &lt;- \"R is cool\"\n\nHere we have created an object called my_obj2 and assigned it a value of R is cool which is a character string. Notice that we have enclosed the string in quotes. If you forget to use the quotes you will receive an error message.\nOur workspace now contains both objects we‚Äôve created so far with my_obj2 listed as type character.\n\n\n\n\n\n\n\nFigure¬†2.3: RStudio environment tab with my_obj2 as a character\n\n\n\n\nTo change the value of an existing object we simply reassign a new value to it. For example, to change the value of my_obj2 from \"R is cool\" to the number 1024\n\nmy_obj2 &lt;- 1024\n\nNotice that the Type has changed to numeric and the value has changed to 1024 in the environment\n\n\n\n\n\n\n\nFigure¬†2.4: RStudio environment tab with updated my_obj2 as numeric\n\n\n\n\nOnce we have created a few objects, we can do stuff with our objects. For example, the following code creates a new object my_obj3 and assigns it the value of my_obj added to my_obj2 which is 1072 (48 + 1024 = 1072).\n\nmy_obj3 &lt;- my_obj + my_obj2\nmy_obj3\n\n[1] 1056\n\n\nNotice that to display the value of my_obj3 we also need to write the object‚Äôs name. The above code works because the values of both my_obj and my_obj2 are numeric (i.e. a number). If you try to do this with objects with character values (character class) you will receive an error\n\nchar_obj &lt;- \"hello\"\nchar_obj2 &lt;- \"world!\"\nchar_obj3 &lt;- char_obj + char_obj2\n# Error in char_obj+char_obj2:non-numeric argument to binary operator\n\nThe error message is essentially telling you that either one or both of the objects char_obj and char_obj2 is not a number and therefore cannot be added together.\nWhen you first start learning R, dealing with errors and warnings can be frustrating as they‚Äôre often difficult to understand (what‚Äôs an argument? what‚Äôs a binary operator?). One way to find out more information about a particular error is to search for a generalised version of the error message. For the above error try searching ‚Äònon-numeric argument to binary operator error + r‚Äô or even ‚Äòcommon r error messages‚Äô.\nAnother error message that you‚Äôll get quite a lot when you first start using R is Error: object 'XXX' not found. As an example, take a look at the code below\nmy_obj &lt;- 48\nmy_obj4 &lt;- my_obj + no_obj\n# Error: object 'no_obj' not found\nR returns an error message because we haven‚Äôt created (defined) the object no_obj yet. Another clue that there‚Äôs a problem with this code is that, if you check your environment, you‚Äôll see that object my_obj4 has not been created.\n\n2.3.2 Naming objects\nNaming your objects is one of the most difficult things you will do in R. Ideally your object names should be kept both short and informative which is not always easy. If you need to create objects with multiple words in their name then use either an underscore or a dot between words or capitalise the different words. We prefer the underscore format and never include uppercase in names (called snake_case)\noutput_summary &lt;- \"my analysis\" # recommended#\noutput.summary &lt;- \"my analysis\"\noutputSummary &lt;- \"my analysis\"\nThere are also a few limitations when it come to giving objects names. An object name cannot start with a number or a dot followed by a number (i.e. 2my_variable or .2my_variable). You should also avoid using non-alphanumeric characters in your object names (i.e. &, ^, /, ! etc). In addition, make sure you don‚Äôt name your objects with reserved words (i.e. TRUE, NA) and it‚Äôs never a good idea to give your object the same name as a built-in function. One that crops up more times than we can remember is\ndata &lt;- read.table(\"mydatafile\", header = TRUE)\nYes, data() is a function in R to load or list available data sets from packages.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-funcs",
    "href": "02-basics.html#sec-funcs",
    "title": "2¬† Some R basics",
    "section": "\n2.4 Using functions in R",
    "text": "2.4 Using functions in R\nUp until now we‚Äôve been creating simple objects by directly assigning a single value to an object. It‚Äôs very likely that you‚Äôll soon want to progress to creating more complicated objects as your R experience grows and the complexity of your tasks increase. Happily, R has a multitude of functions to help you do this. You can think of a function as an object which contains a series of instructions to perform a specific task. The base installation of R comes with many functions already defined or you can increase the power of R by installing one of the 10,000‚Äôs of packages now available. Once you get a bit more experience with using R you may want to define your own functions to perform tasks that are specific to your goals (more about this in Chapter 5).\nThe first function we will learn about is the c() function. The c() function is short for concatenate and we use it to join together a series of values and store them in a data structure called a vector (more on vectors in Chapter 3).\n\nmy_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\n\nIn the code above we‚Äôve created an object called my_vec and assigned it a value using the function c(). There are a couple of really important points to note here. Firstly, when you use a function in R, the function name is always followed by a pair of round brackets even if there‚Äôs nothing contained between the brackets. Secondly, the argument(s) of a function are placed inside the round brackets and are separated by commas. You can think of an argument as way of customising the use or behaviour of a function. In the example above, the arguments are the numbers we want to concatenate. Finally, one of the tricky things when you first start using R is to know which function to use for a particular task and how to use it. Thankfully each function will always have a help document associated with it which will explain how to use the function (more on this later Section 2.6) and a quick web search will also usually help you out.\nTo examine the value of our new object we can simply type out the name of the object as we did before\n\nmy_vec\n\n[1] 2 3 1 6 4 3 3 7\n\n\nNow that we‚Äôve created a vector we can use other functions to do useful stuff with this object. For example, we can calculate the mean, variance, standard deviation and number of elements in our vector by using the mean(), var(), sd() and length() functions\n\nmean(my_vec) # returns the mean of my_vec\n\n[1] 3.625\n\nvar(my_vec) # returns the variance of my_vec\n\n[1] 3.982143\n\nsd(my_vec) # returns the standard deviation of my_vec\n\n[1] 1.995531\n\nlength(my_vec) # returns the number of elements in my_vec\n\n[1] 8\n\n\nIf we wanted to use any of these values later on in our analysis we can just assign the resulting value to another object\n\nvec_mean &lt;- mean(my_vec) # returns the mean of my_vec\nvec_mean\n\n[1] 3.625\n\n\nSometimes it can be useful to create a vector that contains a regular sequence of values in steps of one. Here we can make use of a shortcut using the : symbol.\n\nmy_seq &lt;- 1:10 # create regular sequence\nmy_seq\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmy_seq2 &lt;- 10:1 # in decending order\nmy_seq2\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nOther useful functions for generating vectors of sequences include the seq() and rep() functions. For example, to generate a sequence from 1 to 5 in steps of 0.5\n\nmy_seq2 &lt;- seq(from = 1, to = 5, by = 0.5)\nmy_seq2\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nHere we‚Äôve used the arguments from = and to = to define the limits of the sequence and the by = argument to specify the increment of the sequence. Play around with other values for these arguments to see their effect.\nThe rep() function allows you to replicate (repeat) values a specified number of times. To repeat the value 2, 10 times\n\nmy_seq3 &lt;- rep(2, times = 10) # repeats 2, 10 times\nmy_seq3\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\n\nYou can also repeat non-numeric values\n\nmy_seq4 &lt;- rep(\"abc\", times = 3) # repeats ‚Äòabc‚Äô 3 times\nmy_seq4\n\n[1] \"abc\" \"abc\" \"abc\"\n\n\nor each element of a series\n\nmy_seq5 &lt;- rep(1:5, times = 3) # repeats the series 1 to\n# 5, 3 times\nmy_seq5\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n\nor elements of a series\n\nmy_seq6 &lt;- rep(1:5, each = 3) # repeats each element of the\n# series 3 times\nmy_seq6\n\n [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\n\nWe can also repeat a non-sequential series\n\nmy_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3) # repeats each\n# element of the\n# series 3 times\nmy_seq7\n\n [1]  3  3  3  1  1  1 10 10 10  7  7  7\n\n\nNote in the code above how we‚Äôve used the c() function inside the rep() function. Nesting functions allows us to build quite complex commands within a single line of code and is a very common practice when using R. However, care needs to be taken as too many nested functions can make your code quite difficult for others to understand (or yourself some time in the future!). We could rewrite the code above to explicitly separate the two different steps to generate our vector. Either approach will give the same result, you just need to use your own judgement as to which is more readable.\n\nin_vec &lt;- c(3, 1, 10, 7)\nmy_seq7 &lt;- rep(in_vec, each = 3) # repeats each element of\n# the series 3 times\nmy_seq7\n\n [1]  3  3  3  1  1  1 10 10 10  7  7  7",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-vectors",
    "href": "02-basics.html#sec-vectors",
    "title": "2¬† Some R basics",
    "section": "\n2.5 Working with vectors",
    "text": "2.5 Working with vectors\nManipulating, summarising and sorting data using R is an important skill to master but one which many people find a little confusing at first. We‚Äôll go through a few simple examples here using vectors to illustrate some important concepts but will build on this in much more detail in Chapter 3 where we will look at more complicated (and useful) data structures.\n\n2.5.1 Extracting elements\nTo extract (also known as indexing or subscripting) one or more values (more generally known as elements) from a vector we use the square bracket [ ] notation. The general approach is to name the object you wish to extract from, then a set of square brackets with an index of the element you wish to extract contained within the square brackets. This index can be a position or the result of a logical test.\nPositional index\nTo extract elements based on their position we simply write the position inside the [ ]. For example, to extract the 3rd value of my_vec\n\nmy_vec # remind ourselves what my_vec looks like\n\n[1] 2 3 1 6 4 3 3 7\n\nmy_vec[3] # extract the 3rd value\n\n[1] 1\n\n# if you want to store this value in another object\nval_3 &lt;- my_vec[3]\nval_3\n\n[1] 1\n\n\nNote that the positional index starts at 1 rather than 0 like some other other programming languages (i.e. Python).\nWe can also extract more than one value by using the c() function inside the square brackets. Here we extract the 1st, 5th, 6th and 8th element from the my_vec object\n\nmy_vec[c(1, 5, 6, 8)]\n\n[1] 2 4 3 7\n\n\nOr we can extract a range of values using the : notation. To extract the values from the 3rd to the 8th elements\n\nmy_vec[3:8]\n\n[1] 1 6 4 3 3 7\n\n\n\n2.5.1.1 Logical index\nAnother really useful way to extract data from a vector is to use a logical expression as an index. For example, to extract all elements with a value greater than 4 in the vector my_vec\n\nmy_vec[my_vec &gt; 4]\n\n[1] 6 7\n\n\nHere, the logical expression is my_vec &gt; 4 and R will only extract those elements that satisfy this logical condition. So how does this actually work? If we look at the output of just the logical expression without the square brackets you can see that R returns a vector containing either TRUE or FALSE which correspond to whether the logical condition is satisfied for each element. In this case only the 4th and 8th elements return a TRUE as their value is greater than 4.\n\nmy_vec &gt; 4\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\n\nSo what R is actually doing under the hood is equivalent to\n\nmy_vec[c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE)]\n\n[1] 6 7\n\n\nand only those element that are TRUE will be extracted.\nIn addition to the &lt; and &gt; operators you can also use composite operators to increase the complexity of your expressions. For example the expression for ‚Äògreater or equal to‚Äô is &gt;=. To test whether a value is equal to a value we need to use a double equals symbol == and for ‚Äònot equal to‚Äô we use != (the ! symbol means ‚Äònot‚Äô).\n\nmy_vec[my_vec &gt;= 4] # values greater or equal to 4\n\n[1] 6 4 7\n\nmy_vec[my_vec &lt; 4] # values less than 4\n\n[1] 2 3 1 3 3\n\nmy_vec[my_vec &lt;= 4] # values less than or equal to 4\n\n[1] 2 3 1 4 3 3\n\nmy_vec[my_vec == 4] # values equal to 4\n\n[1] 4\n\nmy_vec[my_vec != 4] # values not equal to 4\n\n[1] 2 3 1 6 3 3 7\n\n\nWe can also combine multiple logical expressions using Boolean expressions. In R the & symbol means AND and the | symbol means OR. For example, to extract values in my_vec which are less than 6 AND greater than 2\n\nval26 &lt;- my_vec[my_vec &lt; 6 & my_vec &gt; 2]\nval26\n\n[1] 3 4 3 3\n\n\nor extract values in my_vec that are greater than 6 OR less than 3\n\nval63 &lt;- my_vec[my_vec &gt; 6 | my_vec &lt; 3]\nval63\n\n[1] 2 1 7\n\n\n\n2.5.2 Replacing elements\nWe can change the values of some elements in a vector using our [ ] notation in combination with the assignment operator &lt;-. For example, to replace the 4th value of our my_vec object from 6 to 500\n\nmy_vec[4] &lt;- 500\nmy_vec\n\n[1]   2   3   1 500   4   3   3   7\n\n\nWe can also replace more than one value or even replace values based on a logical expression\n\n# replace the 6th and 7th element with 100\nmy_vec[c(6, 7)] &lt;- 100\nmy_vec\n\n[1]   2   3   1 500   4 100 100   7\n\n# replace element that are less than or equal to 4 with 1000\nmy_vec[my_vec &lt;= 4] &lt;- 1000\nmy_vec\n\n[1] 1000 1000 1000  500 1000  100  100    7\n\n\n\n2.5.3 Ordering elements\nIn addition to extracting particular elements from a vector we can also order the values contained in a vector. To sort the values from lowest to highest value we can use the sort() function\n\nvec_sort &lt;- sort(my_vec)\nvec_sort\n\n[1]    7  100  100  500 1000 1000 1000 1000\n\n\nTo reverse the sort, from highest to lowest, we can either include the decreasing = TRUE argument when using the sort() function\n\nvec_sort2 &lt;- sort(my_vec, decreasing = TRUE)\nvec_sort2\n\n[1] 1000 1000 1000 1000  500  100  100    7\n\n\nor first sort the vector using the sort() function and then reverse the sorted vector using the rev() function. This is another example of nesting one function inside another function.\n\nvec_sort3 &lt;- rev(sort(my_vec))\nvec_sort3\n\n[1] 1000 1000 1000 1000  500  100  100    7\n\n\nWhilst sorting a single vector is fun, perhaps a more useful task would be to sort one vector according to the values of another vector. To do this we should use the order() function in combination with [ ]. To demonstrate this let‚Äôs create a vector called height containing the height of 5 different people and another vector called p.names containing the names of these people (so Joanna is 180 cm, Charlotte is 155 cm etc)\n\nheight &lt;- c(180, 155, 160, 167, 181)\nheight\n\n[1] 180 155 160 167 181\n\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\np.names\n\n[1] \"Joanna\"    \"Charlotte\" \"Helen\"     \"Karen\"     \"Amy\"      \n\n\nOur goal is to order the people in p.names in ascending order of their height. The first thing we‚Äôll do is use the order() function with the height variable to create a vector called height_ord\n\nheight_ord &lt;- order(height)\nheight_ord\n\n[1] 2 3 4 1 5\n\n\nOK, what‚Äôs going on here? The first value, 2, (remember ignore [1]) should be read as ‚Äòthe smallest value of height is the second element of the height vector‚Äô. If we check this by looking at the height vector above, you can see that element 2 has a value of 155, which is the smallest value. The second smallest value in height is the 3rd element of height, which when we check is 160 and so on. The largest value of height is element 5 which is 181. Now that we have a vector of the positional indices of heights in ascending order (height_ord), we can extract these values from our p.names vector in this order\n\nnames_ord &lt;- p.names[height_ord]\nnames_ord\n\n[1] \"Charlotte\" \"Helen\"     \"Karen\"     \"Joanna\"    \"Amy\"      \n\n\nYou‚Äôre probably thinking ‚Äòwhat‚Äôs the use of this?‚Äô Well, imagine you have a dataset which contains two columns of data and you want to sort each column. If you just use sort() to sort each column separately, the values of each column will become uncoupled from each other. By using the ‚Äòorder()‚Äô on one column, a vector of positional indices is created of the values of the column in ascending order This vector can be used on the second column, as the index of elements which will return a vector of values based on the first column. In all honestly, when you have multiple related vectors you need to use a data.frame type of object (see Chapter 3) instead of multiple independent vectors.\n\n2.5.4 Vectorisation\nOne of the great things about R functions is that most of them are vectorised. This means that the function will operate on all elements of a vector without needing to apply the function on each element separately. For example, to multiple each element of a vector by 5 we can simply use\n\n# create a vector\nmy_vec2 &lt;- c(3, 5, 7, 1, 9, 20)\n\n# multiply each element by 5\nmy_vec2 * 5\n\n[1]  15  25  35   5  45 100\n\n\nOr we can add the elements of two or more vectors\n\n# create a second vector\nmy_vec3 &lt;- c(17, 15, 13, 19, 11, 0)\n\n# add both vectors\nmy_vec2 + my_vec3\n\n[1] 20 20 20 20 20 20\n\n# multiply both vectors\nmy_vec2 * my_vec3\n\n[1] 51 75 91 19 99  0\n\n\nHowever, you must be careful when using vectorisation with vectors of different lengths as R will quietly recycle the elements in the shorter vector rather than throw a wobbly (error).\n\n# create a third vector\nmy_vec4 &lt;- c(1, 2)\n\n# add both vectors - quiet recycling!\nmy_vec2 + my_vec4\n\n[1]  4  7  8  3 10 22\n\n\n\n2.5.5 Missing data\nIn R, missing data is usually represented by an NA symbol meaning ‚ÄòNot Available‚Äô. Data may be missing for a whole bunch of reasons, maybe your machine broke down, maybe you broke down, maybe the weather was too bad to collect data on a particular day etc etc. Missing data can be a pain in the proverbial both from an R perspective and also a statistical perspective. From an R perspective missing data can be problematic as different functions deal with missing data in different ways. For example, let‚Äôs say we collected air temperature readings over 10 days, but our thermometer broke on day 2 and again on day 9 so we have no data for those days\n\ntemp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5)\ntemp\n\n [1] 7.2  NA 7.1 6.9 6.5 5.8 5.8 5.5  NA 5.5\n\n\nWe now want to calculate the mean temperature over these days using the mean() function\n\nmean_temp &lt;- mean(temp)\nmean_temp\n\n[1] NA\n\n\nIf a vector has a missing value then the only possible value to return when calculating a mean is NA. R doesn‚Äôt know that you perhaps want to ignore the NA values (R can‚Äôt read your mind - yet!). If we look at the help file (using ?mean - see the next section Section 2.6 for more details) associated with the mean() function we can see there is an argument na.rm = which is set to FALSE by default.\n\nna.rm - a logical value indicating whether NA values should be stripped before the computation proceeds.\n\nIf we change this argument to na.rm = TRUE when we use the mean() function this will allow us to ignore the NA values when calculating the mean\n\nmean_temp &lt;- mean(temp, na.rm = TRUE)\nmean_temp\n\n[1] 6.2875\n\n\nIt‚Äôs important to note that the NA values have not been removed from our temp object (that would be bad practice), rather the mean() function has just ignored them. The point of the above is to highlight how we can change the default behaviour of a function using an appropriate argument. The problem is that not all functions will have an na.rm = argument, they might deal with NA values differently. However, the good news is that every help file associated with any function will always tell you how missing data are handled by default.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-help",
    "href": "02-basics.html#sec-help",
    "title": "2¬† Some R basics",
    "section": "\n2.6 Getting help",
    "text": "2.6 Getting help\nThis book is intended as a relatively brief introduction to R and as such you will soon be using functions and packages that go beyond this scope of this introductory text. Fortunately, one of the strengths of R is its comprehensive and easily accessible help system and wealth of online resources where you can obtain further information.\n\n2.6.1 R help\nTo access R‚Äôs built-in help facility to get information on any function simply use the help() function. For example, to open the help page for our friend the mean() function.\nhelp(\"mean\")\nor you can use the equivalent shortcut\n?mean\nthe help page is displayed in the ‚ÄòHelp‚Äô tab in the Files pane (usually in the bottom right of RStudio)\n\n\n\n\n\n\n\nFigure¬†2.5: Help page for the mean() function in RStudio Help pane\n\n\n\n\nAdmittedly the help files can seem anything but helpful when you first start using R. This is probably because they‚Äôre written in a very concise manner and the language used is often quite technical and full of jargon. Having said that, you do get used to this and will over time even come to appreciate a certain beauty in their brevity (honest!). One of the great things about the help files is that they all have a very similar structure regardless of the function. This makes it easy to navigate through the file to find exactly what you need.\nThe first line of the help document contains information such as the name of the function and the package where the function can be found. There are also other headings that provide more specific information such as\n\n\n\n\n\n\nHeadings\nDescription\n\n\n\nDescription:\ngives a brief description of the function and what it does.\n\n\nUsage:\ngives the name of the arguments associated with the function and possible default values.\n\n\nArguments:\nprovides more detail regarding each argument and what they do.\n\n\nDetails:\ngives further details of the function if required.\n\n\nValue:\nif applicable, gives the type and structure of the object returned by the function or the operator.\n\n\nSee Also:\nprovides information on other help pages with similar or related content.\n\n\nExamples:\ngives some examples of using the function.\n\n\n\nThe Examples are are really helpful, all you need to do is copy and paste them into the console to see what happens. You can also access examples at any time by using the example() function (i.e. example(\"mean\"))\nThe help() function is useful if you know the name of the function. If you‚Äôre not sure of the name, but can remember a key word then you can search R‚Äôs help system using the help.search() function.\nhelp.search(\"mean\")\nor you can use the equivalent shortcut\n??mean\nThe results of the search will be displayed in RStudio under the ‚ÄòHelp‚Äô tab as before. The help.search() function searches through the help documentation, code demonstrations and package vignettes and displays the results as clickable links for further exploration.\n\n\n\n\n\n\n\nFigure¬†2.6: Output of the help.search() function in RStudio\n\n\n\n\nAnother useful function is apropos(). This function can be used to list all functions containing a specified character string. For example, to find all functions with mean in their name\n\napropos(\"mean\")\n\n [1] \".colMeans\"     \".rowMeans\"     \"colMeans\"      \"kmeans\"       \n [5] \"mean\"          \"mean_temp\"     \"mean.Date\"     \"mean.default\" \n [9] \"mean.difftime\" \"mean.POSIXct\"  \"mean.POSIXlt\"  \"rowMeans\"     \n[13] \"vec_mean\"      \"weighted.mean\"\n\n\nYou can then bring up the help file for the relevant function.\nhelp(\"kmeans\")\nAnother function is RSiteSearch() which enables you to search for keywords and phrases in function help pages and vignettes for all CRAN packages. This function allows you to access the search engine of the R website https://www.r-project.org/search.html directly from the Console with the results displayed in your web browser.\nRSiteSearch(\"regression\")\n\n2.6.2 Other sources of help\nThere really has never been a better time to start learning R. There are a plethora of freely available online resources ranging from whole courses to subject specific tutorials and mailing lists. There are also plenty of paid for options if that‚Äôs your thing but unless you‚Äôve money to burn there really is no need to part with your hard earned cash. Some resources we have found helpful are listed below.\n\n2.6.2.1 General R resources\n\n\nR-Project: User contributed documentation\n\nThe R Journal: Journal of the R project for statistical computing\n\nSwirl: An R package that teaches you R from within R\nRStudio‚Äôs printable cheatsheets\n\nRseek A custom Google search for R-related sites\n\n2.6.2.2 Getting help\n\n[Internet search]: Use your favourite search engine (google, ecosia, duckduckgo, ‚Ä¶ )for any error messages you get. It‚Äôs not cheating and everyone does it! You‚Äôll be surprised how many other people have probably had the same problem and solved it.\nStack Overflow: There are many thousands of questions relevant to R on Stack Overflow. Here are the most popular ones, ranked by vote. Make sure you search for similar questions before asking your own, and make sure you include a reproducible example to get the most useful advice. A reproducible example is a minimal example that lets others who are trying to help you to see the error themselves.\n\n2.6.2.3 R markdown resources\n\nBasic markdown and R markdown reference\nA good markdown reference\nA good 10-minute markdown tutorial\nRStudio‚Äôs R markdown cheatsheet\nR markdown reference sheet\n\nThe R markdown documentation including a getting started guide, a gallery of demos, and several articles for more advanced usage.\n\nThe knitr website has lots of useful reference material about how knitr works.\n\n2.6.2.4 Git and GitHub resources\n\n\nHappy Git: Great resource for using Git and GitHub\n\nVersion control with RStudio: RStudio document for using version control\n\nUsing Git from RStudio: Good 10 minute guide\n\nThe R Class: In depth guide to using Git and GitHub with RStudio\n\n2.6.2.5 R programming\n\n\nR Programming for Data Science: In depth guide to R programming\n\nR for Data Science: Fantastic book, tidyverse orientated",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#saving-stuff-in-r",
    "href": "02-basics.html#saving-stuff-in-r",
    "title": "2¬† Some R basics",
    "section": "\n2.7 Saving stuff in R",
    "text": "2.7 Saving stuff in R\nYour approach to saving work in R and RStudio depends on what you want to save. Most of the time the only thing you will need to save is the R code in your script(s). Remember your script is a reproducible record of everything you‚Äôve done so all you need to do is open up your script in a new RStudio session and source it into the R Console and you‚Äôre back to where you left off.\nUnless you‚Äôve followed our suggestion about changing the default settings for RStudio Projects (see Section 1.5) you will be asked whether you want to save your workspace image every time you exit RStudio. We suggest that 99.9% of the time that you don‚Äôt want do this. By starting with a clean RStudio session each time we come back to our analysis we can be sure to avoid any potential conflicts with things we‚Äôve done in previous sessions.\nThere are, however, some occasions when saving objects you‚Äôve created in R is useful. For example, let‚Äôs say you‚Äôre creating an object that takes hours (even days) of computational time to generate. It would be extremely inconvenient to have to wait all this time each time you come back to your analysis (although we would suggest exporting this to an external file is a better solution). In this case we can save this object as an external .RData file which we can load back into RStudio the next time we want to use it. To save an object to an .RData file you can use the save() function (notice we don‚Äôt need to use the assignment operator here)\nsave(nameOfObject, file = \"name_of_file.RData\")\nor if you want to save all of the objects in your workspace into a single .RData file use the save.image() function\nsave.image(file = \"name_of_file.RData\")\nTo load your .RData file back into RStudio use the load() function\nload(file = \"name_of_file.RData\")",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-packages",
    "href": "02-basics.html#sec-packages",
    "title": "2¬† Some R basics",
    "section": "\n2.8 R packages",
    "text": "2.8 R packages\nThe base installation of R comes with many useful packages as standard. These packages will contain many of the functions you will use on a daily basis. However, as you start using R for more diverse projects (and as your own use of R evolves) you will find that there comes a time when you will need to extend R‚Äôs capabilities. Happily, many thousands of R users have developed useful code and shared this code as installable packages. You can think of a package as a collection of functions, data and help files collated into a well defined standard structure which you can download and install in R. These packages can be downloaded from a variety of sources but the most popular are CRAN, Bioconductor and GitHub. Currently, CRAN hosts over 15000 packages and is the official repository for user contributed R packages. Bioconductor provides open source software oriented towards bioinformatics and hosts over 1800 R packages. GitHub is a website that hosts git repositories for all sorts of software and projects (not just R). Often, cutting edge development versions of R packages are hosted on GitHub so if you need all the new bells and whistles then this may be an option. However, a potential downside of using the development version of an R package is that it might not be as stable as the version hosted on CRAN (it‚Äôs in development!) and updating packages won‚Äôt be automatic.\n\n2.8.1 Using packages\nOnce you have installed a package onto your computer it is not immediately available for you to use. To use a package you first need to load the package by using the library() function. For example, to load the remotes üì¶ package you previously installed\nlibrary(remotes)\nThe library() function will also load any additional packages required and may print out additional package information. It is important to realize that every time you start a new R session (or restore a previously saved session) you need to load the packages you will be using. We tend to put all our library() statements required for our analysis near the top of our R scripts to make them easily accessible and easy to add to as our code develops. If you try to use a function without first loading the relevant R package you will receive an error message that R could not find the function. For example, if you try to use the install_github() function without loading the remotes üì¶ package first you will receive the following error\ninstall_github(\"tidyverse/dplyr\")\n\n# Error in install_github(\"tidyverse/dplyr\") :\n#  could not find function \"install_github\"\nSometimes it can be useful to use a function without first using the library() function. If, for example, you will only be using one or two functions in your script and don‚Äôt want to load all of the other functions in a package then you can access the function directly by specifying the package name followed by two colons and then the function name\nremotes::install_github(\"tidyverse/dplyr\")\nThis is how we were able to use the install() and install_github() functions above without first loading the packages BiocManager üì¶ and remotes üì¶ . Most of the time we recommend using the library() function.\n\n2.8.2 Installing R packages\n\n2.8.2.1 CRAN packages\nTo install a package from CRAN you can use the install.packages() function. For example if you want to install the remotes package enter the following code into the Console window of RStudio (note: you will need a working internet connection to do this)\ninstall.packages(\"remotes\", dependencies = TRUE)\nYou may be asked to select a CRAN mirror, just select ‚Äò0-cloud‚Äô or a mirror near to your location. The dependencies = TRUE argument ensures that additional packages that are required will also be installed.\nIt‚Äôs good practice to regularly update your previously installed packages to get access to new functionality and bug fixes. To update CRAN packages you can use the update.packages() function (you will need a working internet connection for this)\nupdate.packages(ask = FALSE)\nThe ask = FALSE argument avoids having to confirm every package download which can be a pain if you have many packages installed.\n\n2.8.2.2 Bioconductor packages\nTo install packages from Bioconductor the process is a little different. You first need to install the BiocManager üì¶ package. You only need to do this once unless you subsequently reinstall or upgrade R\ninstall.packages(\"BiocManager\", dependencies = TRUE)\nOnce the BiocManagerüì¶ package has been installed you can either install all of the ‚Äòcore‚Äô Bioconductor packages with\nBiocManager::install()\nor install specific packages such as the GenomicRanges üì¶ and edgeR üì¶ packages\nBiocManager::install(c(\"GenomicRanges\", \"edgeR\"))\nTo update Bioconductor packages just use the BiocManager::install() function again\nBiocManager::install(ask = FALSE)\nAgain, you can use the ask = FALSE argument to avoid having to confirm every package download.\n\n2.8.2.3 GitHub packages\nThere are multiple options for installing packages hosted on GitHub. Perhaps the most efficient method is to use the install_github() function from the remotes üì¶ package (you installed this package previously (Section 2.8.2.1)). Before you use the function you will need to know the GitHub username of the repository owner and also the name of the repository. For example, the development version of dplyr üì¶ from Hadley Wickham is hosted on the tidyverse GitHub account and has the repository name ‚Äòdplyr‚Äô (just search for ‚Äògithub dplyr‚Äô). To install this version from GitHub use\nremotes::install_github(\"tidyverse/dplyr\")\nThe safest way (that we know of) to update a package installed from GitHub is to just reinstall it using the above command.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "03-data.html",
    "href": "03-data.html",
    "title": "3¬† Data",
    "section": "",
    "text": "3.1 Data types\nUnderstanding the different types of data and how R deals with these data is important. The temptation is to glaze over and skip these technical details, but beware, this can come back to bite you somewhere unpleasant if you don‚Äôt pay attention. We‚Äôve already seen an example (Section 2.3.1) of this when we tried (and failed) to add two character objects together using the + operator.\nR has six basic types of data; numeric, integer, logical, complex and character. The keen eyed among you will notice we‚Äôve only listed five data types here, the final data type is raw which we won‚Äôt cover as it‚Äôs not useful 99.99% of the time. We also won‚Äôt cover complex numbers, but will let you imagine that part!\nR is (usually) able to automatically distinguish between different classes of data by their nature and the context in which they‚Äôre used although you should bear in mind that R can‚Äôt actually read your mind and you may have to explicitly tell R how you want to care a data type. You can find out the type (or class) of any object using the class() function.\nnum &lt;- 2.2\nclass(num)\n\n[1] \"numeric\"\n\nchar &lt;- \"hello\"\nclass(char)\n\n[1] \"character\"\n\nlogi &lt;- TRUE\nclass(logi)\n\n[1] \"logical\"\nAlternatively, you can ask if an object is a specific class using using a logical test. The is.[classOfData]() family of functions will return either a TRUE or a FALSE.\nis.numeric(num)\n\n[1] TRUE\n\nis.character(num)\n\n[1] FALSE\n\nis.character(char)\n\n[1] TRUE\n\nis.logical(logi)\n\n[1] TRUE\nIt can sometimes be useful to be able to change the class of a variable using the as.[className]() family of coercion functions, although you need to be careful when doing this as you might receive some unexpected results (see what happens below when we try to convert a character string to a numeric).\n# coerce numeric to character\nclass(num)\n\n[1] \"numeric\"\n\nnum_char &lt;- as.character(num)\nnum_char\n\n[1] \"2.2\"\n\nclass(num_char)\n\n[1] \"character\"\n\n# coerce character to numeric!\nclass(char)\n\n[1] \"character\"\n\nchar_num &lt;- as.numeric(char)\n\nWarning: NAs introduced by coercion\nHere‚Äôs a summary table of some of the logical test and coercion functions available to you.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#data-types",
    "href": "03-data.html#data-types",
    "title": "3¬† Data",
    "section": "",
    "text": "Numeric data are numbers that contain a decimal. Actually they can also be whole numbers but we‚Äôll gloss over that.\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There‚Äôs also another special type of logical called NA to represent missing values.\nCharacter data are used to represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). We‚Äôll cover factors later.\n\n\n\n\n\n\n\n\n\n\nType\nLogical test\nCoercing\n\n\n\nCharacter\nis.character\nas.character\n\n\nNumeric\nis.numeric\nas.numeric\n\n\nLogical\nis.logical\nas.logical\n\n\nFactor\nis.factor\nas.factor\n\n\nComplex\nis.complex\nas.complex",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#data-structures",
    "href": "03-data.html#data-structures",
    "title": "3¬† Data",
    "section": "\n3.2 Data structures",
    "text": "3.2 Data structures\nNow that you‚Äôve been introduced to some of the most important classes of data in R, let‚Äôs have a look at some of main structures that we have for storing these data.\n\n3.2.1 Scalars and vectors\nPerhaps the simplest type of data structure is the vector. You‚Äôve already been introduced to vectors in Section 2.4 although some of the vectors you created only contained a single value. Vectors that have a single value (length 1) are called scalars. Vectors can contain numbers, characters, factors or logicals, but the key thing to remember is that all the elements inside a vector must be of the same class. In other words, vectors can contain either numbers, characters or logical but not mixtures of these types of data. There is one important exception to this, you can include NA (remember this is special type of logical) to denote missing data in vectors with other data types.\n\n\n\n\n\n\n\nFigure¬†3.1: Scalar and vector data structure\n\n\n\n\n\n3.2.2 Matrices and arrays\nAnother useful data structure used in many disciplines such as population ecology, theoretical and applied statistics is the matrix. A matrix is simply a vector that has additional attributes called dimensions. Arrays are just multidimensional matrices. Again, matrices and arrays must contain elements all of the same data class.\n\n\n\n\n\n\n\nFigure¬†3.2: Matrix and array data structure\n\n\n\n\nA convenient way to create a matrix or an array is to use the matrix() and array() functions respectively. Below, we will create a matrix from a sequence 1 to 16 in four rows (nrow = 4) and fill the matrix row-wise (byrow = TRUE) rather than the default column-wise. When using the array() function we define the dimensions using the dim = argument, in our case 2 rows, 4 columns in 2 different matrices.\n\nmy_mat &lt;- matrix(1:16, nrow = 4, byrow = TRUE)\nmy_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nmy_array &lt;- array(1:16, dim = c(2, 4, 2))\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]    9   11   13   15\n[2,]   10   12   14   16\n\n\nSometimes it‚Äôs also useful to define row and column names for your matrix but this is not a requirement. To do this use the rownames() and colnames() functions.\n\nrownames(my_mat) &lt;- c(\"A\", \"B\", \"C\", \"D\")\ncolnames(my_mat) &lt;- c(\"a\", \"b\", \"c\", \"d\")\nmy_mat\n\n   a  b  c  d\nA  1  2  3  4\nB  5  6  7  8\nC  9 10 11 12\nD 13 14 15 16\n\n\nOnce you‚Äôve created your matrices you can do useful stuff with them and as you‚Äôd expect, R has numerous built in functions to perform matrix operations. Some of the most common are given below. For example, to transpose a matrix we use the transposition function t()\n\nmy_mat_t &lt;- t(my_mat)\nmy_mat_t\n\n  A B  C  D\na 1 5  9 13\nb 2 6 10 14\nc 3 7 11 15\nd 4 8 12 16\n\n\nTo extract the diagonal elements of a matrix and store them as a vector we can use the diag() function\n\nmy_mat_diag &lt;- diag(my_mat)\nmy_mat_diag\n\n[1]  1  6 11 16\n\n\nThe usual matrix addition, multiplication etc can be performed. Note the use of the %*% operator to perform matrix multiplication.\n\nmat.1 &lt;- matrix(c(2, 0, 1, 1), nrow = 2) # notice that the matrix has been filled\nmat.1 # column-wise by default\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    0    1\n\nmat.2 &lt;- matrix(c(1, 1, 0, 2), nrow = 2)\nmat.2\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    2\n\nmat.1 + mat.2 # matrix addition\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    1    3\n\nmat.1 * mat.2 # element by element products\n\n     [,1] [,2]\n[1,]    2    0\n[2,]    0    2\n\nmat.1 %*% mat.2 # matrix multiplication\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    2\n\n\n\n3.2.3 Lists\nThe next data structure we will quickly take a look at is a list. Whilst vectors and matrices are constrained to contain data of the same type, lists are able to store mixtures of data types. In fact we can even store other data structures such as vectors and arrays within a list or even have a list of a list. This makes for a very flexible data structure which is ideal for storing irregular or non-rectangular data (see Chapter 5 for an example).\nTo create a list we can use the list() function. Note how each of the three list elements are of different classes (character, logical, and numeric) and are of different lengths.\n\nlist_1 &lt;- list(\n  c(\"black\", \"yellow\", \"orange\"),\n  c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),\n  matrix(1:6, nrow = 3)\n)\nlist_1\n\n[[1]]\n[1] \"black\"  \"yellow\" \"orange\"\n\n[[2]]\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n[[3]]\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nElements of the list can be named during the construction of the list\n\nlist_2 &lt;- list(\n  colours = c(\"black\", \"yellow\", \"orange\"),\n  evaluation = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),\n  time = matrix(1:6, nrow = 3)\n)\nlist_2\n\n$colours\n[1] \"black\"  \"yellow\" \"orange\"\n\n$evaluation\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n$time\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nor after the list has been created using the names() function\n\nnames(list_1) &lt;- c(\"colours\", \"evaluation\", \"time\")\nlist_1\n\n$colours\n[1] \"black\"  \"yellow\" \"orange\"\n\n$evaluation\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n$time\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\n3.2.4 Data frames\nBy far the most commonly used data structure to store data in is the data frame. A data frame is a powerful two-dimensional object made up of rows and columns which looks superficially very similar to a matrix. However, whilst matrices are restricted to containing data all of the same type, data frames can contain a mixture of different types of data. Typically, in a data frame each row corresponds to an individual observation and each column corresponds to a different measured or recorded variable. This setup may be familiar to those of you who use LibreOffice Calc or Microsoft Excel to manage and store your data. Perhaps a useful way to think about data frames is that they are essentially made up of a bunch of vectors (columns) with each vector containing its own data type but the data type can be different between vectors.\nAs an example, the data frame below contains the results of an experiment to determine the effect of parental care (with or without) of unicorns (Unicornus magnificens) on offsprings growth under 3 different food availability regime. The data frame has 8 variables (columns) and each row represents an individual unicorn. The variables care and food are factors (categorical variables). The p_care variable has 2 levels (care and no_care) and the food level variable has 3 levels (low, medium and high). The variables height, weight, mane_size and fluffyness are numeric and the variable horn_rings is an integer representing the number of rings on the horn. Although the variable block has numeric values, these do not really have any order and could also be treated as a factor (i.e. they could also have been called A and B).\n\n\n\nTable¬†3.1: Imported unicorn data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_care\nfood\nblock\nheight\nweight\nmane_size\nfluffyness\nhorn_rings\n\n\n\ncare\nmedium\n1\n7.5\n7.62\n11.7\n31.9\n1\n\n\ncare\nmedium\n1\n10.7\n12.14\n14.1\n46.0\n10\n\n\ncare\nmedium\n1\n11.2\n12.76\n7.1\n66.7\n10\n\n\ncare\nmedium\n1\n10.4\n8.78\n11.9\n20.3\n1\n\n\ncare\nmedium\n1\n10.4\n13.58\n14.5\n26.9\n4\n\n\ncare\nmedium\n1\n9.8\n10.08\n12.2\n72.7\n9\n\n\nno_care\nlow\n2\n3.7\n8.10\n10.5\n60.5\n6\n\n\nno_care\nlow\n2\n3.2\n7.45\n14.1\n38.1\n4\n\n\nno_care\nlow\n2\n3.9\n9.19\n12.4\n52.6\n9\n\n\nno_care\nlow\n2\n3.3\n8.92\n11.6\n55.2\n6\n\n\nno_care\nlow\n2\n5.5\n8.44\n13.5\n77.6\n9\n\n\nno_care\nlow\n2\n4.4\n10.60\n16.2\n63.3\n6\n\n\n\n\n\n\n\n\nThere are a couple of important things to bear in mind about data frames. These types of objects are known as rectangular data (or tidy data) as each column must have the same number of observations. Also, any missing data should be recorded as an NA just as we did with our vectors.\nWe can construct a data frame from existing data objects such as vectors using the data.frame() function. As an example, let‚Äôs create three vectors p.height, p.weight and p.names and include all of these vectors in a data frame object called dataf.\n\np.height &lt;- c(180, 155, 160, 167, 181)\np.weight &lt;- c(65, 50, 52, 58, 70)\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndataf &lt;- data.frame(height = p.height, weight = p.weight, names = p.names)\ndataf\n\n  height weight     names\n1    180     65    Joanna\n2    155     50 Charlotte\n3    160     52     Helen\n4    167     58     Karen\n5    181     70       Amy\n\n\nYou‚Äôll notice that each of the columns are named with variable name we supplied when we used the data.frame() function. It also looks like the first column of the data frame is a series of numbers from one to five. Actually, this is not really a column but the name of each row. We can check this out by getting R to return the dimensions of the dataf object using the dim() function. We see that there are 5 rows and 3 columns.\n\ndim(dataf) # 5 rows and 3 columns\n\n[1] 5 3\n\n\nAnother really useful function which we use all the time is str() which will return a compact summary of the structure of the data frame object (or any object for that matter).\n\nstr(dataf)\n\n'data.frame':   5 obs. of  3 variables:\n $ height: num  180 155 160 167 181\n $ weight: num  65 50 52 58 70\n $ names : chr  \"Joanna\" \"Charlotte\" \"Helen\" \"Karen\" ...\n\n\nThe str() function gives us the data frame dimensions and also reminds us that dataf is a data.frame type object. It also lists all of the variables (columns) contained in the data frame, tells us what type of data the variables contain and prints out the first five values. We often copy this summary and place it in our R scripts with comments at the beginning of each line so we can easily refer back to it whilst writing our code. We showed you how to comment blocks in RStudio Section 1.7.\nAlso notice that R has automatically decided that our p.names variable should be a character (chr) class variable when we first created the data frame. Whether this is a good idea or not will depend on how you want to use this variable in later analysis. If we decide that this wasn‚Äôt such a good idea we can change the default behaviour of the data.frame() function by including the argument stringsAsFactors = TRUE. Now our strings are automatically converted to factors.\n\np.height &lt;- c(180, 155, 160, 167, 181)\np.weight &lt;- c(65, 50, 52, 58, 70)\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndataf &lt;- data.frame(\n  height = p.height, weight = p.weight, names = p.names,\n  stringsAsFactors = TRUE\n)\nstr(dataf)\n\n'data.frame':   5 obs. of  3 variables:\n $ height: num  180 155 160 167 181\n $ weight: num  65 50 52 58 70\n $ names : Factor w/ 5 levels \"Amy\",\"Charlotte\",..: 4 2 3 5 1",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#importing-data",
    "href": "03-data.html#importing-data",
    "title": "3¬† Data",
    "section": "\n3.3 Importing data",
    "text": "3.3 Importing data\nAlthough creating data frames from existing data structures is extremely useful, by far the most common approach is to create a data frame by importing data from an external file. To do this, you‚Äôll need to have your data correctly formatted and saved in a file format that R is able to recognize. Fortunately for us, R is able to recognize a wide variety of file formats, although in reality you‚Äôll probably end up only using two or three regularly.\n\n3.3.1 Saving files to import\nThe easiest method of creating a data file to import into R is to enter your data into a spreadsheet using either Microsoft Excel or LibreOffice Calc and save the spreadsheet as a comma delimited file. We prefer LibreOffice Calc as it‚Äôs open source, platform independent and free but MS Excel is OK too (but see here for some gotchas). Here‚Äôs the data from the petunia experiment we discussed previously displayed in LibreOffice. If you want to follow along you can download the data file (‚Äòunicorn.xlsx‚Äô) from Appendix A.\n\n\n\n\n\n\n\nFigure¬†3.3: Unicorn data in LibreOffice Calc\n\n\n\n\nFor those of you unfamiliar with the tab delimited file format it simply means that data in different columns are separated with a ‚Äò,‚Äô character and is usually saved as a file with a ‚Äò.csv‚Äô extension.\nTo save a spreadsheet as a comma delimited file in LibreOffice Calc select File -&gt; Save as ... from the main menu. You will need to specify the location you want to save your file in the ‚ÄòSave in folder‚Äô option and the name of the file in the ‚ÄòName‚Äô option. In the drop down menu located above the ‚ÄòSave‚Äô button change the default ‚ÄòAll formats‚Äô to ‚ÄòText CSV (.csv)‚Äô.\n\n\n\n\n\n\n\nFigure¬†3.4: Choosing csv format when saving with LibreOffice Calc\n\n\n\n\nClick the Save button and then select the ‚ÄòUse Text CSV Format‚Äô option. Click on OK to save the file.\nThere are a couple of things to bear in mind when saving files to import into R which will make your life easier in the long run. Keep your column headings (if you have them) short and informative. Also avoid spaces in your column headings by replacing them with an underscore or a dot (i.e. replace mane size with mane size or mane.size) and avoid using special characters (i.e. leaf area (mm^2) or uppercase to simpy your life). Remember, if you have missing data in your data frame (empty cells) you should use an NA to represent these missing values or have an empty cell. This will keep the data frame tidy.\n\n3.3.2 Import functions\nOnce you‚Äôve saved your data file in a suitable format we can now read this file into R. The workhorse function for importing data into R is the read.table() function (we discuss some alternatives later in the chapter). The read.table() function is a very flexible function with a shed load of arguments (see ?read.table) but it‚Äôs quite simple to use. Let‚Äôs import a comma delimited file called unicorns.csv which contains the data we saw previously in this Chapter (Section 3.2.4) and assign it to an object called unicorns. The file is located in a data directory which itself is located in our root directory (Section 1.4). The first row of the data contains the variable (column) names. To use the read.table() function to import this file\n\nunicorns &lt;- read.table(\n  file = \"data/unicorns.csv\", header = TRUE, sep = \",\", dec = \".\",\n  stringsAsFactors = TRUE\n)\n\nThere are a few things to note about the above command. First, the file path and the filename (including the file extension) needs to be enclosed in either single or double quotes (i.e. the data/unicorns.txt bit) as the read.table() function expects this to be a character string. If your working directory is already set to the directory which contains the file, you don‚Äôt need to include the entire file path just the filename. In the example above, the file path is separated with a single forward slash /. This will work regardless of the operating system you are using and we recommend you stick with this. However, Windows users may be more familiar with the single backslash notation and if you want to keep using this you will need to include them as double backslashes.\n\n\n\n\n\n\nWarning\n\n\n\nNote though that the double backslash notation will not work on computers using Mac OSX or Linux operating systems. We thus strongly discourage it since it is not reproducible\n\n\nThe header = TRUE argument specifies that the first row of your data contains the variable names (i.e. food, block etc). If this is not the case you can specify header = FALSE (actually, this is the default value so you can omit this argument entirely). The sep = \",\" argument tells R what is file delimiter.\nOther useful arguments include dec = and na.strings =. The dec = argument allows you to change the default character (.) used for a decimal point. This is useful if you‚Äôre in a country where decimal places are usually represented by a comma (i.e. dec = \",\"). The na.strings = argument allows you to import data where missing values are represented with a symbol other than NA. This can be quite common if you are importing data from other statistical software such as Minitab which represents missing values as a * (na.strings = \"*\").\nHonestly, from the read.table() a series of predefined functions are available. They are all using read.table() but define format specific options. We can simply read.csv()to read a csv file, with ‚Äú,‚Äù separation and ‚Äú.‚Äù for decimals. In countries were ‚Äú,‚Äù is used for decimals, csv files use ‚Äú;‚Äù as a separator. In this case using read.csv2() would be needed. When working with tab delimited files, the functions read.delim() and read.delim2() can be used with ‚Äú.‚Äù and ‚Äú,‚Äù as decimal respectively.\nAfter importing our data into R , to see the contents of the data frame we could just type the name of the object as we have done previously. BUT before you do that, think about why you‚Äôre doing this. If your data frame is anything other than tiny, all you‚Äôre going to do is fill up your Console with data. It‚Äôs not like you can easily check whether there are any errors or that your data has been imported correctly. A much better solution is to use our old friend the str() function to return a compact and informative summary of your data frame.\n\nstr(unicorns)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : Factor w/ 2 levels \"care\",\"no_care\": 1 1 1 1 1 1 1 1 1 1 ...\n $ food      : Factor w/ 3 levels \"high\",\"low\",\"medium\": 3 3 3 3 3 3 3 3 3 3 ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nHere we see that unicorns is a ‚Äòdata.frame‚Äô object which contains 96 rows and 8 variables (columns). Each of the variables are listed along with their data class and the first 10 values. As we mentioned previously in this Chapter, it can be quite convenient to copy and paste this into your R script as a comment block for later reference.\nNotice also that your character string variables (care and food) have been imported as factors because we used the argument stringsAsFactors = TRUE. If this is not what you want you can prevent this by using the stringsAsFactors = FALSE or from R version 4.0.0 you can just leave out this argument as stringsAsFactors = FALSE is the default.\n\nunicorns &lt;- read.delim(file = \"data/unicorns.txt\")\nstr(unicorns)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : chr  \"care\" \"care\" \"care\" \"care\" ...\n $ food      : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nIf we just wanted to see the names of our variables (columns) in the data frame we can use the names() function which will return a character vector of the variable names.\n\nnames(unicorns)\n\n[1] \"p_care\"     \"food\"       \"block\"      \"height\"     \"weight\"    \n[6] \"mane_size\"  \"fluffyness\" \"horn_rings\"\n\n\nYou can even import spreadsheet files from MS Excel or other statistics software directly into R but our advice is that this should generally be avoided if possible as it just adds a layer of uncertainty between you and your data. In our opinion it‚Äôs almost always better to export your spreadsheets as tab or comma delimited files and then import them into R using one of the read.table() derivative function. If you‚Äôre hell bent on directly importing data from other software you will need to install the foreign package which has functions for importing Minitab, SPSS, Stata and SAS files. For MS Excel and LO Calc spreadsheets, there are a few packages that can be used.\n\n3.3.3 Common import frustrations\nIt‚Äôs quite common to get a bunch of really frustrating error messages when you first start importing data into R. Perhaps the most common is\nError in file(file, \"rt\") : cannot open the connection\nIn addition: Warning message:\nIn file(file, \"rt\") :\n  cannot open file 'unicorns.txt': No such file or directory\nThis error message is telling you that R cannot find the file you are trying to import. It usually rears its head for one of a couple of reasons (or all of them!). The first is that you‚Äôve made a mistake in the spelling of either the filename or file path. Another common mistake is that you have forgotten to include the file extension in the filename (i.e. .txt). Lastly, the file is not where you say it is or you‚Äôve used an incorrect file path. Using RStudio Projects (Section 1.5) and having a logical directory structure (Section 1.4) goes a long way to avoiding these types of errors.\nAnother really common mistake is to forget to include the header = TRUE argument when the first row of the data contains variable names. For example, if we omit this argument when we import our unicorns.txt file everything looks OK at first (no error message at least)\n\nunicorns_bad &lt;- read.table(file = \"data/unicorns.txt\", sep = \"\\t\")\n\nbut when we take a look at our data frame using str()\n\nstr(unicorns_bad)\n\n'data.frame':   97 obs. of  8 variables:\n $ V1: chr  \"p_care\" \"care\" \"care\" \"care\" ...\n $ V2: chr  \"food\" \"medium\" \"medium\" \"medium\" ...\n $ V3: chr  \"block\" \"1\" \"1\" \"1\" ...\n $ V4: chr  \"height\" \"7.5\" \"10.7\" \"11.2\" ...\n $ V5: chr  \"weight\" \"7.62\" \"12.14\" \"12.76\" ...\n $ V6: chr  \"mane_size\" \"11.7\" \"14.1\" \"7.1\" ...\n $ V7: chr  \"fluffyness\" \"31.9\" \"46\" \"66.7\" ...\n $ V8: chr  \"horn_rings\" \"1\" \"10\" \"10\" ...\n\n\nWe can see an obvious problem, all of our variables have been imported as factors and our variables are named V1, V2, V3 ‚Ä¶ V8. The problem happens because we haven‚Äôt told the read.table() function that the first row contains the variable names and so it treats them as data. As soon as we have a single character string in any of our data vectors, R treats the vectors as character type data (remember all elements in a vector must contain the same type of data (Section 3.2.1)).\nThis is just one more argument to use read.csv() or read.delim() function with appropriate default values for arguments.\n\n3.3.4 Other import options\nThere are numerous other functions to import data from a variety of sources and formats. Most of these functions are contained in packages that you will need to install before using them. We list a couple of the more useful packages and functions below.\nThe fread() function from the data.table üì¶ package is great for importing large data files quickly and efficiently (much faster than the read.table() function). One of the great things about the fread() function is that it will automatically detect many of the arguments you would normally need to specify (like sep = etc). One of the things you will need to consider though is that the fread() function will return a data.table object not a data.frame as would be the case with the read.table() function. This is usually not a problem as you can pass a data.table object to any function that only accepts data.frame objects. To learn more about the differences between data.table and data.frame objects see here.\n\nlibrary(data.table)\nall_data &lt;- fread(file = \"data/unicorns.txt\")\n\nVarious functions from the readr package are also very efficient at reading in large data files. The readr package is part of the ‚Äòtidyverse‚Äô collection of packages and provides many equivalent functions to base R for importing data. The readr functions are used in a similar way to the read.table() or read.csv() functions and many of the arguments are the same (see ?readr::read_table for more details). There are however some differences. For example, when using the read_table() function the header = TRUE argument is replaced by col_names = TRUE and the function returns a tibble class object which is the tidyverse equivalent of a data.frame object (see here for differences).\n\n\n\n\n\n\nWarning\n\n\n\nSome functions are not happy to handle the data format produced by tidyverse and might require you to transform them to data.frame format using data.frame().\n\n\n\nlibrary(readr)\n# import white space delimited files\nall_data &lt;- read_table(file = \"data/unicorns.txt\", col_names = TRUE)\n\n# import comma delimited files\nall_data &lt;- read_csv(file = \"data/unicorns.csv\")\n\n# import tab delimited files\nall_data &lt;- read_delim(file = \"data/unicorns.txt\", delim = \"\\t\")\n\n# or use\nall_data &lt;- read_tsv(file = \"data/unicorns.txt\")\n\nIf your data file is ginormous, then the ff and bigmemory packages may be useful as they both contain import functions that are able to store large data in a memory efficient manner. You can find out more about these functions here and here.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#wrangling-data-frames",
    "href": "03-data.html#wrangling-data-frames",
    "title": "3¬† Data",
    "section": "\n3.4 Wrangling data frames",
    "text": "3.4 Wrangling data frames\nNow that you‚Äôre able to successfully import your data from an external file into R our next task is to do something useful with our data. Working with data is a fundamental skill which you‚Äôll need to develop and get comfortable with as you‚Äôll likely do a lot of it during any project. The good news is that R is especially good at manipulating, summarising and visualising data. Manipulating data (often known as data wrangling or munging) in R can at first seem a little daunting for the new user but if you follow a few simple logical rules then you‚Äôll quickly get the hang of it, especially with some practice.\nLet‚Äôs remind ourselves of the structure of the unicorns data frame we imported in the previous section.\n\nunicorns &lt;- read.table(file = \"data/unicorns.txt\", header = TRUE, sep = \"\\t\")\nstr(unicorns)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : chr  \"care\" \"care\" \"care\" \"care\" ...\n $ food      : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nTo access the data in any of the variables (columns) in our data frame we can use the $ notation. For example, to access the height variable in our unicorns data frame we can use unicorns$height. This tells R that the height variable is contained within the data frame unicorns.\n\nunicorns$height\n\n [1]  7.5 10.7 11.2 10.4 10.4  9.8  6.9  9.4 10.4 12.3 10.4 11.0  7.1  6.0  9.0\n[16]  4.5 12.6 10.0 10.0  8.5 14.1 10.1  8.5  6.5 11.5  7.7  6.4  8.8  9.2  6.2\n[31]  6.3 17.2  8.0  8.0  6.4  7.6  9.7 12.3  9.1  8.9  7.4  3.1  7.9  8.8  8.5\n[46]  5.6 11.5  5.8  5.6  5.3  7.5  4.1  3.5  8.5  4.9  2.5  5.4  3.9  5.8  4.5\n[61]  8.0  1.8  2.2  3.9  8.5  8.5  6.4  1.2  2.6 10.9  7.2  2.1  4.7  5.0  6.5\n[76]  2.6  6.0  9.3  4.6  5.2  3.9  2.3  5.2  2.2  4.5  1.8  3.0  3.7  2.4  5.7\n[91]  3.7  3.2  3.9  3.3  5.5  4.4\n\n\nThis will return a vector of the height data. If we want we can assign this vector to another object and do stuff with it, like calculate a mean or get a summary of the variable using the summary() function.\n\nf_height &lt;- unicorns$height\nmean(f_height)\n\n[1] 6.839583\n\nsummary(f_height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   4.475   6.450   6.840   9.025  17.200 \n\n\nOr if we don‚Äôt want to create an additional object we can use functions ‚Äòon-the-fly‚Äô to only display the value in the console.\n\nmean(unicorns$height)\n\n[1] 6.839583\n\nsummary(unicorns$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   4.475   6.450   6.840   9.025  17.200 \n\n\nJust as we did with vectors (Section 2.5), we also can access data in data frames using the square bracket [ ] notation. However, instead of just using a single index, we now need to use two indexes, one to specify the rows and one for the columns. To do this, we can use the notation my_data[rows, columns] where rows and columns are indexes and my_data is the name of the data frame. Again, just like with our vectors our indexes can be positional or the result of a logical test.\n\n3.4.1 Positional indexes\nTo use positional indexes we simple have to write the position of the rows and columns we want to extract inside the [ ]. For example, if for some reason we wanted to extract the first value (1st row ) of the height variable (4th column)\n\nunicorns[1, 4]\n\n[1] 7.5\n\n# this would give you the same\nunicorns$height[1]\n\n[1] 7.5\n\n\nWe can also extract values from multiple rows or columns by specifying these indexes as vectors inside the [ ]. To extract the first 10 rows and the first 4 columns we simple supply a vector containing a sequence from 1 to 10 for the rows index (1:10) and a vector from 1 to 4 for the column index (1:4).\n\nunicorns[1:10, 1:4]\n\n   p_care   food block height\n1    care medium     1    7.5\n2    care medium     1   10.7\n3    care medium     1   11.2\n4    care medium     1   10.4\n5    care medium     1   10.4\n6    care medium     1    9.8\n7    care medium     1    6.9\n8    care medium     1    9.4\n9    care medium     2   10.4\n10   care medium     2   12.3\n\n\nOr for non sequential rows and columns then we can supply vectors of positions using the c() function. To extract the 1st, 5th, 12th, 30th rows from the 1st, 3rd, 6th and 8th columns\n\nunicorns[c(1, 5, 12, 30), c(1, 3, 6, 8)]\n\n   p_care block mane_size horn_rings\n1    care     1      11.7          1\n5    care     1      14.5          4\n12   care     2      12.6          6\n30   care     2      11.6          5\n\n\nAll we are doing in the two examples above is creating vectors of positions for the rows and columns that we want to extract. We have done this by using the skills we developed in Section 2.4 when we generated vectors using the c() function or using the : notation.\nBut what if we want to extract either all of the rows or all of the columns? It would be extremely tedious to have to generate vectors for all rows or for all columns. Thankfully R has a shortcut. If you don‚Äôt specify either a row or column index in the [ ] then R interprets it to mean you want all rows or all columns. For example, to extract the first 4 rows and all of the columns in the unicorns data frame\n\nunicorns[1:4, ]\n\n  p_care   food block height weight mane_size fluffyness horn_rings\n1   care medium     1    7.5   7.62      11.7       31.9          1\n2   care medium     1   10.7  12.14      14.1       46.0         10\n3   care medium     1   11.2  12.76       7.1       66.7         10\n4   care medium     1   10.4   8.78      11.9       20.3          1\n\n\nor all of the rows and the first 3 columns1.\nunicorns[, 1:3]\n\n\n    p_care   food block\n1     care medium     1\n2     care medium     1\n3     care medium     1\n4     care medium     1\n5     care medium     1\n92 no_care    low     2\n93 no_care    low     2\n94 no_care    low     2\n95 no_care    low     2\n96 no_care    low     2\n\n\nWe can even use negative positional indexes to exclude certain rows and columns. As an example, lets extract all of the rows except the first 85 rows and all columns except the 4th, 7th and 8th columns. Notice we need to use -() when we generate our row positional vectors. If we had just used -1:85 this would actually generate a regular sequence from -1 to 85 which is not what we want (we can of course use -1:-85).\n\nunicorns[-(1:85), -c(4, 7, 8)]\n\n    p_care food block weight mane_size\n86 no_care  low     1   6.01      17.6\n87 no_care  low     1   9.93      12.0\n88 no_care  low     1   7.03       7.9\n89 no_care  low     2   9.10      14.5\n90 no_care  low     2   9.05       9.6\n91 no_care  low     2   8.10      10.5\n92 no_care  low     2   7.45      14.1\n93 no_care  low     2   9.19      12.4\n94 no_care  low     2   8.92      11.6\n95 no_care  low     2   8.44      13.5\n96 no_care  low     2  10.60      16.2\n\n\nIn addition to using a positional index for extracting particular columns (variables) we can also name the variables directly when using the square bracket [ ] notation. For example, let‚Äôs extract the first 5 rows and the variables care, food and mane_size. Instead of using unicorns[1:5, c(1, 2, 6)] we can instead use\n\nunicorns[1:5, c(\"p_care\", \"food\", \"mane_size\")]\n\n  p_care   food mane_size\n1   care medium      11.7\n2   care medium      14.1\n3   care medium       7.1\n4   care medium      11.9\n5   care medium      14.5\n\n\nWe often use this method in preference to the positional index for selecting columns as it will still give us what we want even if we‚Äôve changed the order of the columns in our data frame for some reason.\n\n3.4.2 Logical indexes\nJust as we did with vectors, we can also extract data from our data frame based on a logical test. We can use all of the logical operators that we used for our vector examples so if these have slipped your mind maybe have a look at Section 2.5.1.1 and refresh your memory. Let‚Äôs extract all rows where height is greater than 12 and extract all columns by default (remember, if you don‚Äôt include a column index after the comma it means all columns).\n\nbig_unicorns &lt;- unicorns[unicorns$height &gt; 12, ]\nbig_unicorns\n\n   p_care   food block height weight mane_size fluffyness horn_rings\n10   care medium     2   12.3  13.48      16.1       36.9          8\n17   care   high     1   12.6  18.66      18.6       54.0          9\n21   care   high     1   14.1  19.12      13.1      113.2         13\n32   care   high     2   17.2  19.20      10.9       89.9         14\n38   care    low     1   12.3  11.27      13.7       28.7          5\n\n\nNotice in the code above that we need to use the unicorns$height notation for the logical test. If we just named the height variable without the name of the data frame we would receive an error telling us R couldn‚Äôt find the variable height. The reason for this is that the height variable only exists inside the unicorns data frame so you need to tell R exactly where it is.\nbig_unicorns &lt;- unicorns[height &gt; 12, ]\nError in `[.data.frame`(unicorns, height &gt; 12, ) : \n  object 'height' not found\nSo how does this work? The logical test is unicorns$height &gt; 12 and R will only extract those rows that satisfy this logical condition. If we look at the output of just the logical condition you can see this returns a vector containing TRUE if height is greater than 12 and FALSE if height is not greater than 12.\n\nunicorns$height &gt; 12\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[37] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSo our row index is a vector containing either TRUE or FALSE values and only those rows that are TRUE are selected.\nOther commonly used operators are shown below\n\nunicorns[unicorns$height &gt;= 6, ] # values greater or equal to 6\n\nunicorns[unicorns$height &lt;= 6, ] # values less than or equal to 6\n\nunicorns[unicorns$height == 8, ] # values  equal to 8\n\nunicorns[unicorns$height != 8, ] # values  not equal to 8\n\nWe can also extract rows based on the value of a character string or factor level. Let‚Äôs extract all rows where the food level is equal to high (again we will output all columns). Notice that the double equals == sign must be used for a logical test and that the character string must be enclosed in either single or double quotes (i.e. \"high\").\n\nfood_high &lt;- unicorns[unicorns$food == \"high\", ]\nrbind(head(food_high, n = 10), tail(food_high, n = 10))\n\n    p_care food block height weight mane_size fluffyness horn_rings\n17    care high     1   12.6  18.66      18.6       54.0          9\n18    care high     1   10.0  18.07      16.9       90.5          3\n19    care high     1   10.0  13.29      15.8      142.7         12\n20    care high     1    8.5  14.33      13.2       91.4          5\n21    care high     1   14.1  19.12      13.1      113.2         13\n22    care high     1   10.1  15.49      12.6       77.2         12\n23    care high     1    8.5  17.82      20.5       54.4          3\n24    care high     1    6.5  17.13      24.1      147.4          6\n25    care high     2   11.5  23.89      14.3      101.5         12\n26    care high     2    7.7  14.77      17.2      104.5          4\n71 no_care high     1    7.2  15.21      15.9      135.0         14\n72 no_care high     1    2.1  19.15      15.6      176.7          6\n73 no_care high     2    4.7  13.42      19.8      124.7          5\n74 no_care high     2    5.0  16.82      17.3      182.5         15\n75 no_care high     2    6.5  14.00      10.1      126.5          7\n76 no_care high     2    2.6  18.88      16.4      181.5         14\n77 no_care high     2    6.0  13.68      16.2      133.7          2\n78 no_care high     2    9.3  18.75      18.4      181.1         16\n79 no_care high     2    4.6  14.65      16.7       91.7         11\n80 no_care high     2    5.2  17.70      19.1      181.1          8\n\n\nOr we can extract all rows where food level is not equal to medium (using !=) and only return columns 1 to 4.\n\nfood_not_medium &lt;- unicorns[unicorns$food != \"medium\", 1:4]\nrbind(head(food_not_medium, n = 10), tail(food_not_medium, n = 10))\n\n    p_care food block height\n17    care high     1   12.6\n18    care high     1   10.0\n19    care high     1   10.0\n20    care high     1    8.5\n21    care high     1   14.1\n22    care high     1   10.1\n23    care high     1    8.5\n24    care high     1    6.5\n25    care high     2   11.5\n26    care high     2    7.7\n87 no_care  low     1    3.0\n88 no_care  low     1    3.7\n89 no_care  low     2    2.4\n90 no_care  low     2    5.7\n91 no_care  low     2    3.7\n92 no_care  low     2    3.2\n93 no_care  low     2    3.9\n94 no_care  low     2    3.3\n95 no_care  low     2    5.5\n96 no_care  low     2    4.4\n\n\nWe can increase the complexity of our logical tests by combining them with Boolean expressions just as we did for vector objects. For example, to extract all rows where height is greater or equal to 6 AND food is equal to medium AND care is equal to no_care we combine a series of logical expressions with the & symbol.\n\nlow_no_care_heigh6 &lt;- unicorns[unicorns$height &gt;= 6 & unicorns$food == \"medium\" &\n  unicorns$p_care == \"no_care\", ]\nlow_no_care_heigh6\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n51 no_care medium     1    7.5  13.60      13.6      122.2         11\n54 no_care medium     1    8.5  10.04      12.3      113.6          4\n61 no_care medium     2    8.0  11.43      12.6       43.2         14\n\n\nTo extract rows based on an ‚ÄòOR‚Äô Boolean expression we can use the | symbol. Let‚Äôs extract all rows where height is greater than 12.3 OR less than 2.2.\n\nheight2.2_12.3 &lt;- unicorns[unicorns$height &gt; 12.3 | unicorns$height &lt; 2.2, ]\nheight2.2_12.3\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n17    care   high     1   12.6  18.66      18.6       54.0          9\n21    care   high     1   14.1  19.12      13.1      113.2         13\n32    care   high     2   17.2  19.20      10.9       89.9         14\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n\n\nAn alternative method of selecting parts of a data frame based on a logical expression is to use the subset() function instead of the [ ]. The advantage of using subset() is that you no longer need to use the $ notation when specifying variables inside the data frame as the first argument to the function is the name of the data frame to be subsetted. The disadvantage is that subset() is less flexible than the [ ] notation.\n\ncare_med_2 &lt;- subset(unicorns, p_care == \"care\" & food == \"medium\" & block == 2)\ncare_med_2\n\n   p_care   food block height weight mane_size fluffyness horn_rings\n9    care medium     2   10.4  10.48      10.5       57.8          5\n10   care medium     2   12.3  13.48      16.1       36.9          8\n11   care medium     2   10.4  13.18      11.1       56.8         12\n12   care medium     2   11.0  11.56      12.6       31.3          6\n13   care medium     2    7.1   8.16      29.6        9.7          2\n14   care medium     2    6.0  11.22      13.0       16.4          3\n15   care medium     2    9.0  10.20      10.8       90.1          6\n16   care medium     2    4.5  12.55      13.4       14.4          6\n\n\nAnd if you only want certain columns you can use the select = argument.\n\nuni_p_care &lt;- subset(unicorns, p_care == \"care\" & food == \"medium\" & block == 2,\n  select = c(\"p_care\", \"food\", \"mane_size\")\n)\nuni_p_care\n\n   p_care   food mane_size\n9    care medium      10.5\n10   care medium      16.1\n11   care medium      11.1\n12   care medium      12.6\n13   care medium      29.6\n14   care medium      13.0\n15   care medium      10.8\n16   care medium      13.4\n\n\n\n3.4.3 Ordering data frames\nRemember when we used the function order() to order one vector based on the order of another vector (way back in Section 2.5.3). This comes in very handy if you want to reorder rows in your data frame. For example, if we want all of the rows in the data frame unicorns to be ordered in ascending value of height and output all columns by default.\n\nheight_ord &lt;- unicorns[order(unicorns$height), ]\nhead(height_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n84 no_care    low     1    2.2   9.97       9.6       63.1          2\n82 no_care    low     1    2.3   7.28      13.8       32.8          6\n89 no_care    low     2    2.4   9.10      14.5       78.7          8\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n69 no_care   high     1    2.6  16.57      17.1      141.1          3\n\n\nWe can also order by descending order of a variable (i.e. mane_size) using the decreasing = TRUE argument.\n\nmane_size_ord &lt;- unicorns[order(unicorns$mane_size, decreasing = TRUE), ]\nhead(mane_size_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n13    care medium     2    7.1   8.16      29.6        9.7          2\n24    care   high     1    6.5  17.13      24.1      147.4          6\n65 no_care   high     1    8.5  22.53      20.8      166.9         16\n23    care   high     1    8.5  17.82      20.5       54.4          3\n66 no_care   high     1    8.5  17.33      19.8      184.4         12\n73 no_care   high     2    4.7  13.42      19.8      124.7          5\n80 no_care   high     2    5.2  17.70      19.1      181.1          8\n17    care   high     1   12.6  18.66      18.6       54.0          9\n49 no_care medium     1    5.6  11.03      18.6       49.9          8\n\n\nWe can even order data frames based on multiple variables. For example, to order the data frame unicorns in ascending order of both block and height.\n\nblock_height_ord &lt;- unicorns[order(unicorns$block, unicorns$height), ]\nhead(block_height_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n84 no_care    low     1    2.2   9.97       9.6       63.1          2\n82 no_care    low     1    2.3   7.28      13.8       32.8          6\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n69 no_care   high     1    2.6  16.57      17.1      141.1          3\n87 no_care    low     1    3.0   9.93      12.0       56.6          6\n53 no_care medium     1    3.5  12.93      16.6      109.3          3\n88 no_care    low     1    3.7   7.03       7.9       36.7          5\n\n\nWhat if we wanted to order unicorns by ascending order of block but descending order of height? We can use a simple trick by adding a - symbol before the unicorns$height variable when we use the order() function. This will essentially turn all of the height values negative which will result in reversing the order. Note, that this trick will only work with numeric variables.\n\nblock_revheight_ord &lt;- unicorns[order(unicorns$block, -unicorns$height), ]\nrbind(head(block_revheight_ord, n = 10), tail(block_revheight_ord, n = 10))\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n21    care   high     1   14.1  19.12      13.1      113.2         13\n17    care   high     1   12.6  18.66      18.6       54.0          9\n38    care    low     1   12.3  11.27      13.7       28.7          5\n3     care medium     1   11.2  12.76       7.1       66.7         10\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n2     care medium     1   10.7  12.14      14.1       46.0         10\n4     care medium     1   10.4   8.78      11.9       20.3          1\n5     care medium     1   10.4  13.58      14.5       26.9          4\n22    care   high     1   10.1  15.49      12.6       77.2         12\n18    care   high     1   10.0  18.07      16.9       90.5          3\n64 no_care medium     2    3.9  12.97      17.0       97.5          5\n93 no_care    low     2    3.9   9.19      12.4       52.6          9\n91 no_care    low     2    3.7   8.10      10.5       60.5          6\n94 no_care    low     2    3.3   8.92      11.6       55.2          6\n92 no_care    low     2    3.2   7.45      14.1       38.1          4\n42    care    low     2    3.1   8.74      16.1       39.1          3\n76 no_care   high     2    2.6  18.88      16.4      181.5         14\n89 no_care    low     2    2.4   9.10      14.5       78.7          8\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n\n\nIf we wanted to do the same thing with a factor (or character) variable like food we would need to use the function xtfrm() for this variable inside our order() function.\n\nblock_revheight_ord &lt;- unicorns[order(-xtfrm(unicorns$food), unicorns$height), ]\nrbind(head(block_revheight_ord, n = 10), tail(block_revheight_ord, n = 10))\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n53 no_care medium     1    3.5  12.93      16.6      109.3          3\n58 no_care medium     2    3.9   9.07       9.6       90.4          7\n64 no_care medium     2    3.9  12.97      17.0       97.5          5\n52 no_care medium     1    4.1  12.58      13.9      136.6         11\n16    care medium     2    4.5  12.55      13.4       14.4          6\n60 no_care medium     2    4.5  13.68      14.8      125.5          9\n55 no_care medium     1    4.9   6.89       8.2       52.9          3\n29    care   high     2    9.2  13.26      11.3      108.0          9\n78 no_care   high     2    9.3  18.75      18.4      181.1         16\n18    care   high     1   10.0  18.07      16.9       90.5          3\n19    care   high     1   10.0  13.29      15.8      142.7         12\n22    care   high     1   10.1  15.49      12.6       77.2         12\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n25    care   high     2   11.5  23.89      14.3      101.5         12\n17    care   high     1   12.6  18.66      18.6       54.0          9\n21    care   high     1   14.1  19.12      13.1      113.2         13\n32    care   high     2   17.2  19.20      10.9       89.9         14\n\n\nNotice that the food variable has been reverse ordered alphabetically and height has been ordered by increasing values within each level of food.\nIf we wanted to order the data frame by food but this time order it from low -&gt; medium -&gt; high instead of the default alphabetically (high, low, medium), we need to first change the order of our levels of the food factor in our data frame using the factor() function. Once we‚Äôve done this we can then use the order() function as usual. Note, if you‚Äôre reading the pdf version of this book, the output has been truncated to save space.\n\nunicorns$food &lt;- factor(unicorns$food,\n  levels = c(\"low\", \"medium\", \"high\")\n)\nfood_ord &lt;- unicorns[order(unicorns$food), ]\nrbind(head(food_ord, n = 10), tail(food_ord, n = 10))\n\n    p_care food block height weight mane_size fluffyness horn_rings\n33    care  low     1    8.0   6.88       9.3       16.1          4\n34    care  low     1    8.0  10.23      11.9       88.1          4\n35    care  low     1    6.4   5.97       8.7        7.3          2\n36    care  low     1    7.6  13.05       7.2       47.2          8\n37    care  low     1    9.7   6.49       8.1       18.0          3\n38    care  low     1   12.3  11.27      13.7       28.7          5\n39    care  low     1    9.1   8.96       9.7       23.8          3\n40    care  low     1    8.9  11.48      11.1       39.4          7\n41    care  low     2    7.4  10.89      13.3        9.5          5\n42    care  low     2    3.1   8.74      16.1       39.1          3\n71 no_care high     1    7.2  15.21      15.9      135.0         14\n72 no_care high     1    2.1  19.15      15.6      176.7          6\n73 no_care high     2    4.7  13.42      19.8      124.7          5\n74 no_care high     2    5.0  16.82      17.3      182.5         15\n75 no_care high     2    6.5  14.00      10.1      126.5          7\n76 no_care high     2    2.6  18.88      16.4      181.5         14\n77 no_care high     2    6.0  13.68      16.2      133.7          2\n78 no_care high     2    9.3  18.75      18.4      181.1         16\n79 no_care high     2    4.6  14.65      16.7       91.7         11\n80 no_care high     2    5.2  17.70      19.1      181.1          8\n\n\n\n3.4.4 Adding columns and rows\nSometimes it‚Äôs useful to be able to add extra rows and columns of data to our data frames. There are multiple ways to achieve this (as there always is in R!) depending on your circumstances. To simply append additional rows to an existing data frame we can use the rbind() function and to append columns the cbind() function. Let‚Äôs create a couple of test data frames to see this in action using our old friend the data.frame() function.\n\n# rbind for rows\ndf1 &lt;- data.frame(\n  id = 1:4, height = c(120, 150, 132, 122),\n  weight = c(44, 56, 49, 45)\n)\ndf1\n\n  id height weight\n1  1    120     44\n2  2    150     56\n3  3    132     49\n4  4    122     45\n\ndf2 &lt;- data.frame(\n  id = 5:6, height = c(119, 110),\n  weight = c(39, 35)\n)\ndf2\n\n  id height weight\n1  5    119     39\n2  6    110     35\n\ndf3 &lt;- data.frame(\n  id = 1:4, height = c(120, 150, 132, 122),\n  weight = c(44, 56, 49, 45)\n)\ndf3\n\n  id height weight\n1  1    120     44\n2  2    150     56\n3  3    132     49\n4  4    122     45\n\ndf4 &lt;- data.frame(location = c(\"UK\", \"CZ\", \"CZ\", \"UK\"))\ndf4\n\n  location\n1       UK\n2       CZ\n3       CZ\n4       UK\n\n\nWe can use the rbind() function to append the rows of data in df2 to the rows in df1 and assign the new data frame to df_rcomb.\n\ndf_rcomb &lt;- rbind(df1, df2)\ndf_rcomb\n\n  id height weight\n1  1    120     44\n2  2    150     56\n3  3    132     49\n4  4    122     45\n5  5    119     39\n6  6    110     35\n\n\nAnd cbind to append the column in df4 to the df3 data frame and assign to df_ccomb`.\n\ndf_ccomb &lt;- cbind(df3, df4)\ndf_ccomb\n\n  id height weight location\n1  1    120     44       UK\n2  2    150     56       CZ\n3  3    132     49       CZ\n4  4    122     45       UK\n\n\nAnother situation when adding a new column to a data frame is useful is when you want to perform some kind of transformation on an existing variable. For example, say we wanted to apply a log10 transformation on the height variable in the df_rcomb data frame we created above. We could just create a separate variable to contains these values but it‚Äôs good practice to create this variable as a new column inside our existing data frame so we keep all of our data together. Let‚Äôs call this new variable height_log10.\n\n# log10 transformation\ndf_rcomb$height_log10 &lt;- log10(df_rcomb$height)\ndf_rcomb\n\n  id height weight height_log10\n1  1    120     44     2.079181\n2  2    150     56     2.176091\n3  3    132     49     2.120574\n4  4    122     45     2.086360\n5  5    119     39     2.075547\n6  6    110     35     2.041393\n\n\nThis situation also crops up when we want to convert an existing variable in a data frame from one data class to another data class. For example, the id variable in the df_rcomb data frame is numeric type data (use the str() or class() functions to check for yourself). If we wanted to convert the id variable to a factor to use later in our analysis we can create a new variable called Fid in our data frame and use the factor() function to convert the id variable.\n\n# convert to a factor\ndf_rcomb$Fid &lt;- factor(df_rcomb$id)\ndf_rcomb\n\n  id height weight height_log10 Fid\n1  1    120     44     2.079181   1\n2  2    150     56     2.176091   2\n3  3    132     49     2.120574   3\n4  4    122     45     2.086360   4\n5  5    119     39     2.075547   5\n6  6    110     35     2.041393   6\n\nstr(df_rcomb)\n\n'data.frame':   6 obs. of  5 variables:\n $ id          : int  1 2 3 4 5 6\n $ height      : num  120 150 132 122 119 110\n $ weight      : num  44 56 49 45 39 35\n $ height_log10: num  2.08 2.18 2.12 2.09 2.08 ...\n $ Fid         : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6\n\n\n\n3.4.5 Merging data frames\nInstead of just appending either rows or columns to a data frame we can also merge two data frames together. Let‚Äôs say we have one data frame that contains taxonomic information on some common UK rocky shore invertebrates (called taxa) and another data frame that contains information on where they are usually found on the rocky shore (called zone). We can merge these two data frames together to produce a single data frame with both taxonomic and location information. Let‚Äôs first create both of these data frames (in reality you would probably just import your different datasets).\n\ntaxa &lt;- data.frame(\n  GENUS = c(\"Patella\", \"Littorina\", \"Halichondria\", \"Semibalanus\"),\n  species = c(\"vulgata\", \"littoria\", \"panacea\", \"balanoides\"),\n  family = c(\"patellidae\", \"Littorinidae\", \"Halichondriidae\", \"Archaeobalanidae\")\n)\ntaxa\n\n         GENUS    species           family\n1      Patella    vulgata       patellidae\n2    Littorina   littoria     Littorinidae\n3 Halichondria    panacea  Halichondriidae\n4  Semibalanus balanoides Archaeobalanidae\n\nzone &lt;- data.frame(\n  genus = c(\n    \"Laminaria\", \"Halichondria\", \"Xanthoria\", \"Littorina\",\n    \"Semibalanus\", \"Fucus\"\n  ),\n  species = c(\n    \"digitata\", \"panacea\", \"parietina\", \"littoria\",\n    \"balanoides\", \"serratus\"\n  ),\n  zone = c(\"v_low\", \"low\", \"v_high\", \"low_mid\", \"high\", \"low_mid\")\n)\nzone\n\n         genus    species    zone\n1    Laminaria   digitata   v_low\n2 Halichondria    panacea     low\n3    Xanthoria  parietina  v_high\n4    Littorina   littoria low_mid\n5  Semibalanus balanoides    high\n6        Fucus   serratus low_mid\n\n\nBecause both of our data frames contains at least one variable in common (species in our case) we can simply use the merge() function to create a new data frame called taxa_zone.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone)\ntaxa_zone\n\n     species        GENUS           family        genus    zone\n1 balanoides  Semibalanus Archaeobalanidae  Semibalanus    high\n2   littoria    Littorina     Littorinidae    Littorina low_mid\n3    panacea Halichondria  Halichondriidae Halichondria     low\n\n\nNotice that the merged data frame contains only the rows that have species information in both data frames. There are also two columns called GENUS and genus because the merge() function treats these as two different variables that originate from the two data frames.\nIf we want to include all data from both data frames then we will need to use the all = TRUE argument. The missing values will be included as NA.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone, all = TRUE)\ntaxa_zone\n\n     species        GENUS           family        genus    zone\n1 balanoides  Semibalanus Archaeobalanidae  Semibalanus    high\n2   digitata         &lt;NA&gt;             &lt;NA&gt;    Laminaria   v_low\n3   littoria    Littorina     Littorinidae    Littorina low_mid\n4    panacea Halichondria  Halichondriidae Halichondria     low\n5  parietina         &lt;NA&gt;             &lt;NA&gt;    Xanthoria  v_high\n6   serratus         &lt;NA&gt;             &lt;NA&gt;        Fucus low_mid\n7    vulgata      Patella       patellidae         &lt;NA&gt;    &lt;NA&gt;\n\n\nIf the variable names that you want to base the merge on are different in each data frame (for example GENUS and genus) you can specify the names in the first data frame (known as x) and the second data frame (known as y) using the by.x = and by.y = arguments.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone, by.x = \"GENUS\", by.y = \"genus\", all = TRUE)\ntaxa_zone\n\n         GENUS  species.x           family  species.y    zone\n1        Fucus       &lt;NA&gt;             &lt;NA&gt;   serratus low_mid\n2 Halichondria    panacea  Halichondriidae    panacea     low\n3    Laminaria       &lt;NA&gt;             &lt;NA&gt;   digitata   v_low\n4    Littorina   littoria     Littorinidae   littoria low_mid\n5      Patella    vulgata       patellidae       &lt;NA&gt;    &lt;NA&gt;\n6  Semibalanus balanoides Archaeobalanidae balanoides    high\n7    Xanthoria       &lt;NA&gt;             &lt;NA&gt;  parietina  v_high\n\n\nOr using multiple variable names.\n\ntaxa_zone &lt;- merge(\n  x = taxa, y = zone, by.x = c(\"species\", \"GENUS\"),\n  by.y = c(\"species\", \"genus\"), all = TRUE\n)\ntaxa_zone\n\n     species        GENUS           family    zone\n1 balanoides  Semibalanus Archaeobalanidae    high\n2   digitata    Laminaria             &lt;NA&gt;   v_low\n3   littoria    Littorina     Littorinidae low_mid\n4    panacea Halichondria  Halichondriidae     low\n5  parietina    Xanthoria             &lt;NA&gt;  v_high\n6   serratus        Fucus             &lt;NA&gt; low_mid\n7    vulgata      Patella       patellidae    &lt;NA&gt;\n\n\n\n3.4.6 Reshaping data frames\nReshaping data into different formats is a common task. With rectangular type data (data frames have the same number of rows in each column) there are two main data frame shapes that you will come across: the ‚Äòlong‚Äô format (sometimes called stacked) and the ‚Äòwide‚Äô format. An example of a long format data frame is given below. We can see that each row is a single observation from an individual subject and each subject can have multiple rows. This results in a single column of our measurement.\n\nlong_data &lt;- data.frame(\n  subject = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 3),\n  sex = rep(c(\"M\", \"F\", \"F\", \"M\"), each = 3),\n  condition = rep(c(\"control\", \"cond1\", \"cond2\"), times = 4),\n  measurement = c(\n    12.9, 14.2, 8.7, 5.2, 12.6, 10.1, 8.9,\n    12.1, 14.2, 10.5, 12.9, 11.9\n  )\n)\nlong_data\n\n   subject sex condition measurement\n1        A   M   control        12.9\n2        A   M     cond1        14.2\n3        A   M     cond2         8.7\n4        B   F   control         5.2\n5        B   F     cond1        12.6\n6        B   F     cond2        10.1\n7        C   F   control         8.9\n8        C   F     cond1        12.1\n9        C   F     cond2        14.2\n10       D   M   control        10.5\n11       D   M     cond1        12.9\n12       D   M     cond2        11.9\n\n\nWe can also format the same data in the wide format as shown below. In this format we have multiple observations from each subject in a single row with measurements in different columns (control, cond1 and cond2). This is a common format when you have repeated measurements from sampling units.\n\nwide_data &lt;- data.frame(\n  subject = c(\"A\", \"B\", \"C\", \"D\"),\n  sex = c(\"M\", \"F\", \"F\", \"M\"),\n  control = c(12.9, 5.2, 8.9, 10.5),\n  cond1 = c(14.2, 12.6, 12.1, 12.9),\n  cond2 = c(8.7, 10.1, 14.2, 11.9)\n)\nwide_data\n\n  subject sex control cond1 cond2\n1       A   M    12.9  14.2   8.7\n2       B   F     5.2  12.6  10.1\n3       C   F     8.9  12.1  14.2\n4       D   M    10.5  12.9  11.9\n\n\nWhilst there‚Äôs no inherent problem with either of these formats we will sometimes need to convert between the two because some functions will require a specific format for them to work. The most common format is the long format.\nThere are many ways to convert between these two formats but we‚Äôll use the melt() and dcast() functions from the reshape2 package (you will need to install this package first). The melt() function is used to convert from wide to long formats. The first argument for the melt() function is the data frame we want to melt (in our case wide_data). The id.vars = c(\"subject\", \"sex\") argument is a vector of the variables you want to stack, the measured.vars = c(\"control\", \"cond1\", \"cond2\") argument identifies the columns of the measurements in different conditions, the variable.name = \"condition\" argument specifies what you want to call the stacked column of your different conditions in your output data frame and value.name = \"measurement\" is the name of the column of your stacked measurements in your output data frame.\n\nlibrary(reshape2)\nwide_data # remind ourselves what the wide format looks like\n\n  subject sex control cond1 cond2\n1       A   M    12.9  14.2   8.7\n2       B   F     5.2  12.6  10.1\n3       C   F     8.9  12.1  14.2\n4       D   M    10.5  12.9  11.9\n\n# convert wide to long\nmy_long_df &lt;- melt(\n  data = wide_data, id.vars = c(\"subject\", \"sex\"),\n  measured.vars = c(\"control\", \"cond1\", \"cond2\"),\n  variable.name = \"condition\", value.name = \"measurement\"\n)\nmy_long_df\n\n   subject sex condition measurement\n1        A   M   control        12.9\n2        B   F   control         5.2\n3        C   F   control         8.9\n4        D   M   control        10.5\n5        A   M     cond1        14.2\n6        B   F     cond1        12.6\n7        C   F     cond1        12.1\n8        D   M     cond1        12.9\n9        A   M     cond2         8.7\n10       B   F     cond2        10.1\n11       C   F     cond2        14.2\n12       D   M     cond2        11.9\n\n\nThe dcast() function is used to convert from a long format data frame to a wide format data frame. The first argument is again is the data frame we want to cast (long_data for this example). The second argument is in formula syntax. The subject + sex bit of the formula means that we want to keep these columns separate, and the ~ condition part is the column that contains the labels that we want to split into new columns in our new data frame. The value.var = \"measurement\" argument is the column that contains the measured data.\n\nlong_data # remind ourselves what the long format look like\n\n   subject sex condition measurement\n1        A   M   control        12.9\n2        A   M     cond1        14.2\n3        A   M     cond2         8.7\n4        B   F   control         5.2\n5        B   F     cond1        12.6\n6        B   F     cond2        10.1\n7        C   F   control         8.9\n8        C   F     cond1        12.1\n9        C   F     cond2        14.2\n10       D   M   control        10.5\n11       D   M     cond1        12.9\n12       D   M     cond2        11.9\n\n# convert long to wide\nmy_wide_df &lt;- dcast(\n  data = long_data, subject + sex ~ condition,\n  value.var = \"measurement\"\n)\nmy_wide_df\n\n  subject sex cond1 cond2 control\n1       A   M  14.2   8.7    12.9\n2       B   F  12.6  10.1     5.2\n3       C   F  12.1  14.2     8.9\n4       D   M  12.9  11.9    10.5",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#introduction-to-the-tidyverse",
    "href": "03-data.html#introduction-to-the-tidyverse",
    "title": "3¬† Data",
    "section": "\n3.5 Introduction to the tidyverse\n",
    "text": "3.5 Introduction to the tidyverse\n\nit seems it is not super tidy in here and we need to improve that",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#summarising-data-frames",
    "href": "03-data.html#summarising-data-frames",
    "title": "3¬† Data",
    "section": "\n3.6 Summarising data frames",
    "text": "3.6 Summarising data frames\nNow that we‚Äôre able to manipulate and extract data from our data frames our next task is to start exploring and getting to know our data. In this section we‚Äôll start producing tables of useful summary statistics of the variables in our data frame and in the next two Chapters we‚Äôll cover visualising our data with base R graphics and using the ggplot2 package.\nA really useful starting point is to produce some simple summary statistics of all of the variables in our unicorns data frame using the summary() function.\n\nsummary(unicorns)\n\n    p_care              food        block         height           weight      \n Length:96          low   :32   Min.   :1.0   Min.   : 1.200   Min.   : 5.790  \n Class :character   medium:32   1st Qu.:1.0   1st Qu.: 4.475   1st Qu.: 9.027  \n Mode  :character   high  :32   Median :1.5   Median : 6.450   Median :11.395  \n                                Mean   :1.5   Mean   : 6.840   Mean   :12.155  \n                                3rd Qu.:2.0   3rd Qu.: 9.025   3rd Qu.:14.537  \n                                Max.   :2.0   Max.   :17.200   Max.   :23.890  \n   mane_size       fluffyness       horn_rings    \n Min.   : 5.80   Min.   :  5.80   Min.   : 1.000  \n 1st Qu.:11.07   1st Qu.: 39.05   1st Qu.: 4.000  \n Median :13.45   Median : 70.05   Median : 6.000  \n Mean   :14.05   Mean   : 79.78   Mean   : 7.062  \n 3rd Qu.:16.45   3rd Qu.:113.28   3rd Qu.: 9.000  \n Max.   :49.20   Max.   :189.60   Max.   :17.000  \n\n\nFor numeric variables (i.e. height, weight etc) the mean, minimum, maximum, median, first (lower) quartile and third (upper) quartile are presented. For factor variables (i.e. care and food) the number of observations in each of the factor levels is given. If a variable contains missing data then the number of NA values is also reported.\nIf we wanted to summarise a smaller subset of variables in our data frame we can use our indexing skills in combination with the summary() function. For example, to summarise only the height, weight, mane_size and fluffyness variables we can include the appropriate column indexes when using the [ ]. Notice we include all rows by not specifying a row index.\n\nsummary(unicorns[, 4:7])\n\n     height           weight         mane_size       fluffyness    \n Min.   : 1.200   Min.   : 5.790   Min.   : 5.80   Min.   :  5.80  \n 1st Qu.: 4.475   1st Qu.: 9.027   1st Qu.:11.07   1st Qu.: 39.05  \n Median : 6.450   Median :11.395   Median :13.45   Median : 70.05  \n Mean   : 6.840   Mean   :12.155   Mean   :14.05   Mean   : 79.78  \n 3rd Qu.: 9.025   3rd Qu.:14.537   3rd Qu.:16.45   3rd Qu.:113.28  \n Max.   :17.200   Max.   :23.890   Max.   :49.20   Max.   :189.60  \n\n# or equivalently\n# summary(unicorns[, c(\"height\", \"weight\", \"mane_size\", \"fluffyness\")])\n\nAnd to summarise a single variable.\n\nsummary(unicorns$mane_size)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.80   11.07   13.45   14.05   16.45   49.20 \n\n# or equivalently\n# summary(unicorns[, 6])\n\nAs you‚Äôve seen above, the summary() function reports the number of observations in each level of our factor variables. Another useful function for generating tables of counts is the table() function. The table() function can be used to build contingency tables of different combinations of factor levels. For example, to count the number of observations for each level of food\n\ntable(unicorns$food)\n\n\n   low medium   high \n    32     32     32 \n\n\nWe can extend this further by producing a table of counts for each combination of food and care factor levels.\n\ntable(unicorns$food, unicorns$p_care)\n\n        \n         care no_care\n  low      16      16\n  medium   16      16\n  high     16      16\n\n\nA more flexible version of the table() function is the xtabs() function. The xtabs() function uses a formula notation (~) to build contingency tables with the cross-classifying variables separated by a + symbol on the right hand side of the formula. xtabs() also has a useful data = argument so you don‚Äôt have to include the data frame name when specifying each variable.\n\nxtabs(~ food + p_care, data = unicorns)\n\n        p_care\nfood     care no_care\n  low      16      16\n  medium   16      16\n  high     16      16\n\n\nWe can even build more complicated contingency tables using more variables. Note, in the example below the xtabs() function has quietly coerced our block variable to a factor.\n\nxtabs(~ food + p_care + block, data = unicorns)\n\n, , block = 1\n\n        p_care\nfood     care no_care\n  low       8       8\n  medium    8       8\n  high      8       8\n\n, , block = 2\n\n        p_care\nfood     care no_care\n  low       8       8\n  medium    8       8\n  high      8       8\n\n\nAnd for a nicer formatted table we can nest the xtabs() function inside the ftable() function to ‚Äòflatten‚Äô the table.\n\nftable(xtabs(~ food + p_care + block, data = unicorns))\n\n               block 1 2\nfood   p_care           \nlow    care          8 8\n       no_care       8 8\nmedium care          8 8\n       no_care       8 8\nhigh   care          8 8\n       no_care       8 8\n\n\nWe can also summarise our data for each level of a factor variable. Let‚Äôs say we want to calculate the mean value of height for each of our low, meadium and high levels of food. To do this we will use the mean() function and apply this to the height variable for each level of food using the tapply() function.\n\ntapply(unicorns$height, unicorns$food, mean)\n\n     low   medium     high \n5.853125 7.012500 7.653125 \n\n\nThe tapply() function is not just restricted to calculating mean values, you can use it to apply many of the functions that come with R or even functions you‚Äôve written yourself (see Chapter 5 for more details). For example, we can apply the sd() function to calculate the standard deviation for each level of food or even the summary() function.\n\ntapply(unicorns$height, unicorns$food, sd)\n\n     low   medium     high \n2.828425 3.005345 3.483323 \n\ntapply(unicorns$height, unicorns$food, summary)\n\n$low\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.800   3.600   5.550   5.853   8.000  12.300 \n\n$medium\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.800   4.500   7.000   7.013   9.950  12.300 \n\n$high\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   5.800   7.450   7.653   9.475  17.200 \n\n\nNote, if the variable you want to summarise contains missing values (NA) you will also need to include an argument specifying how you want the function to deal with the NA values. We saw an example if this in Section 2.5.5 where the mean() function returned an NA when we had missing data. To include the na.rm = TRUE argument we simply add this as another argument when using tapply().\n\ntapply(unicorns$height, unicorns$food, mean, na.rm = TRUE)\n\n     low   medium     high \n5.853125 7.012500 7.653125 \n\n\nWe can also use tapply() to apply functions to more than one factor. The only thing to remember is that the factors need to be supplied to the tapply() function in the form of a list using the list() function. To calculate the mean height for each combination of food and care factor levels we can use the list(unicorns$food, unicorns$p_care) notation.\n\ntapply(unicorns$height, list(unicorns$food, unicorns$p_care), mean)\n\n         care no_care\nlow    8.0375 3.66875\nmedium 9.1875 4.83750\nhigh   9.6000 5.70625\n\n\nAnd if you get a little fed up with having to write unicorns$ for every variable you can nest the tapply() function inside the with() function. The with() function allows R to evaluate an R expression with respect to a named data object (in this case unicorns).\n\nwith(unicorns, tapply(height, list(food, p_care), mean))\n\n         care no_care\nlow    8.0375 3.66875\nmedium 9.1875 4.83750\nhigh   9.6000 5.70625\n\n\nThe with() function also works with many other functions and can save you alot of typing!\nAnother really useful function for summarising data is the aggregate() function. The aggregate() function works in a very similar way to tapply() but is a bit more flexible.\nFor example, to calculate the mean of the variables height, weight, mane_size and fluffyness for each level of food.\n\naggregate(unicorns[, 4:7], by = list(food = unicorns$food), FUN = mean)\n\n    food   height    weight mane_size fluffyness\n1    low 5.853125  8.652812  11.14375    45.1000\n2 medium 7.012500 11.164062  13.83125    67.5625\n3   high 7.653125 16.646875  17.18125   126.6875\n\n\nIn the code above we have indexed the columns we want to summarise in the unicorns data frame using unicorns[, 4:7]. The by = argument specifies a list of factors (list(food = unicorns$food)) and the FUN = argument names the function to apply (mean in this example).\nSimilar to the tapply() function we can include more than one factor to apply a function to. Here we calculate the mean values for each combination of food and care\n\naggregate(unicorns[, 4:7], by = list(\n  food = unicorns$food,\n  p_care = unicorns$p_care\n), FUN = mean)\n\n    food  p_care  height    weight mane_size fluffyness\n1    low    care 8.03750  9.016250   9.96250   30.30625\n2 medium    care 9.18750 11.011250  13.48750   40.59375\n3   high    care 9.60000 16.689375  15.54375   98.05625\n4    low no_care 3.66875  8.289375  12.32500   59.89375\n5 medium no_care 4.83750 11.316875  14.17500   94.53125\n6   high no_care 5.70625 16.604375  18.81875  155.31875\n\n\nWe can also use the aggregate() function in a different way by using the formula method (as we did with xtabs()). On the left hand side of the formula (~) we specify the variable we want to apply the mean function on and to the right hand side our factors separated by a + symbol. The formula method also allows you to use the data = argument for convenience.\n\naggregate(height ~ food + p_care, FUN = mean, data = unicorns)\n\n    food  p_care  height\n1    low    care 8.03750\n2 medium    care 9.18750\n3   high    care 9.60000\n4    low no_care 3.66875\n5 medium no_care 4.83750\n6   high no_care 5.70625\n\n\nOne advantage of using the formula method is that we can also use the subset = argument to apply the function to subsets of the original data. For example, to calculate the mean height for each combination of the food and care levels but only for those unicorns that have less than 7 horn_rings.\n\naggregate(height ~ food + p_care, FUN = mean, subset = horn_rings &lt; 7, data = unicorns)\n\n    food  p_care   height\n1    low    care 8.176923\n2 medium    care 8.570000\n3   high    care 7.900000\n4    low no_care 3.533333\n5 medium no_care 5.316667\n6   high no_care 3.850000\n\n\nOr for only those unicorns in block 1.\n\naggregate(height ~ food + p_care, FUN = mean, subset = block == \"1\", data = unicorns)\n\n    food  p_care  height\n1    low    care  8.7500\n2 medium    care  9.5375\n3   high    care 10.0375\n4    low no_care  3.3250\n5 medium no_care  5.2375\n6   high no_care  5.9250",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#exporting-data",
    "href": "03-data.html#exporting-data",
    "title": "3¬† Data",
    "section": "\n3.7 Exporting data",
    "text": "3.7 Exporting data\nBy now we hope you‚Äôre getting a feel for how powerful and useful R is for manipulating and summarising data (and we‚Äôve only really scratched the surface). One of the great benefits of doing all your data wrangling in R is that you have a permanent record of all the things you‚Äôve done to your data. Gone are the days of making undocumented changes in Excel or Calc! By treating your data as ‚Äòread only‚Äô and documenting all of your decisions in R you will have made great strides towards making your analysis more reproducible and transparent to others. It‚Äôs important to realise, however, that any changes you‚Äôve made to your data frame in R will not change the original data file you imported into R (and that‚Äôs a good thing). Happily it‚Äôs straightforward to export data frames to external files in a wide variety of formats.\n\n3.7.1 Export functions\nThe main workhorse function for exporting data frames is the write.table() function. As with the read.table() function, the write.table() function is very flexible with lots of arguments to help customise it‚Äôs behaviour. As an example, let‚Äôs take our original unicorns data frame, do some useful stuff to it and then export these changes to an external file.\nSimilarly to read.table(), write.table() has a series of function with format specific default values such as write.csv() and write.delim() which use ‚Äú,‚Äù and tabs as delimiters, respectively,and include column names by default.\nLet‚Äôs order the rows in the data frame in ascending order of height within each level food. We will also apply a square root transformation on the number of horn rings variable (horn_rings) and a log10 transformation on the height variable and save these as additional columns in our data frame (hopefully this will be somewhat familiar to you!).\n\nunicorns_df2 &lt;- unicorns[order(unicorns$food, unicorns$height), ]\nunicorns_df2$horn_rings_sqrt &lt;- sqrt(unicorns_df2$horn_rings)\nunicorns_df2$log10_height &lt;- log10(unicorns_df2$height)\nstr(unicorns_df2)\n\n'data.frame':   96 obs. of  10 variables:\n $ p_care         : chr  \"no_care\" \"no_care\" \"no_care\" \"no_care\" ...\n $ food           : Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ block          : int  1 1 1 2 1 2 2 2 1 2 ...\n $ height         : num  1.8 2.2 2.3 2.4 3 3.1 3.2 3.3 3.7 3.7 ...\n $ weight         : num  6.01 9.97 7.28 9.1 9.93 8.74 7.45 8.92 7.03 8.1 ...\n $ mane_size      : num  17.6 9.6 13.8 14.5 12 16.1 14.1 11.6 7.9 10.5 ...\n $ fluffyness     : num  46.2 63.1 32.8 78.7 56.6 39.1 38.1 55.2 36.7 60.5 ...\n $ horn_rings     : int  4 2 6 8 6 3 4 6 5 6 ...\n $ horn_rings_sqrt: num  2 1.41 2.45 2.83 2.45 ...\n $ log10_height   : num  0.255 0.342 0.362 0.38 0.477 ...\n\n\nNow we can export our new data frame unicorns_df2 using the write.table() function. The first argument is the data frame you want to export (unicorns_df2 in our example). We then give the filename (with file extension) and the file path in either single or double quotes using the file = argument. In this example we‚Äôre exporting the data frame to a file called unicorns_transformed.csv in the data directory. The row.names = FALSE argument stops R from including the row names in the first column of the file.\n\nwrite.csv(unicorns_df2,\n  file = \"data/unicorns_transformed.csv\", \n  row.names = FALSE\n)\n\nAs we saved the file as a comma delimited text file we could open this file in any text editor.\nWe can of course export our files in a variety of other formats.\n\n3.7.2 Other export functions\nAs with importing data files into R, there are also many alternative functions for exporting data to external files beyond the write.table() function. If you followed the ‚ÄòOther import functions‚Äô Section 3.3.4 of this Chapter you will already have the required packages installed.\nThe fwrite() function from the data.table üì¶ package is very efficient at exporting large data objects and is much faster than the write.table() function. It‚Äôs also quite simple to use as it has most of the same arguments as write.table(). To export a tab delimited text file we just need to specify the data frame name, the output file name and file path and the separator between columns.\n\nlibrary(data.table)\nfwrite(unicorns_df2, file = \"data/unicorns_04_12.txt\", sep = \"\\t\")\n\nTo export a csv delimited file it‚Äôs even easier as we don‚Äôt even need to include the sep = argument.\n\nlibrary(data.table)\nfwrite(unicorns_df2, file = \"data/unicorns_04_12.csv\")\n\nThe readr package also comes with two useful functions for quickly writing data to external files: the write_tsv() function for writing tab delimited files and the write_csv() function for saving comma separated values (csv) files.\n\nlibrary(readr)\nwrite_tsv(unicorns_df2, path = \"data/unicorns_04_12.txt\")\n\nwrite_csv(unicorns_df2, path = \"data/unicorns_04_12.csv\")",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#footnotes",
    "href": "03-data.html#footnotes",
    "title": "3¬† Data",
    "section": "",
    "text": "For space and simplicity we are just showing the first and last five rows‚Ü©Ô∏é",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html",
    "href": "04-graphics_short.html",
    "title": "4¬† Figures",
    "section": "",
    "text": "4.1 Simple base R plots\nThere are many functions in R to produce plots ranging from the very basic to the highly complex. It‚Äôs impossible to cover every aspect of producing graphics in R in this book so we‚Äôll introduce you to most of the common methods of graphing data and describe how to customise your graphs later on in Section 4.5.\nThe most common high level function used to produce plots in R is (rather unsurprisingly) the plot() function. For example, let‚Äôs plot the weight of unicorns from our unicorns data frame which we imported in Section 3.3.2.\nunicorns &lt;- read.csv(file = \"data/unicorns.csv\")\n\nplot(unicorns$weight)\nR has plotted the values of weight (on the y axis) against an index since we are only plotting one variable to plot. The index is just the order of the weight values in the data frame (1 first in the data frame and 97 last). The weight variable name has been automatically included as a y axis label and the axes scales have been automatically set.\nIf we‚Äôd only included the variable weight rather than unicorns$weight, the plot() function will display an error as the variable weight only exists in the unicorns data frame object.\nplot(weight)\n## Error in plot(weight) : object 'weight' not found\nAs many of the base R plotting functions don‚Äôt have a data = argument to specify the data frame name directly we can use the with() function in combination with plot() as a shortcut.\nwith(unicorns, plot(weight))\nTo plot a scatterplot of one numeric variable against another numeric variable we just need to include both variables as arguments when using the plot() function. For example to plot fluffyness on the y axis and weight of the x axis.\nplot(x = unicorns$weight, y = unicorns$fluffyness)\nThere is an equivalent approach for these types of plots which often causes some confusion at first. You can also use the formula notation when using the plot() function. However, in contrast to the previous method the formula method requires you to specify the y axis variable first, then a ~ and then our x axis variable.\nplot(fluffyness ~ weight, data = unicorns)\n\n\n\n\n\n\nFigure¬†4.2\nBoth of these two approaches are equivalent so we suggest that you just choose the one you prefer and go with it.\nYou can also specify the type of graph you wish to plot using the argument type =. You can plot just the points (type = \"p\", this is the default), just lines (type = \"l\"), both points and lines connected (type = \"b\"), both points and lines with the lines running through the points (type = \"o\") and empty points joined by lines (type = \"c\"). For example, let‚Äôs use our skills from Section 2.4 to generate two vectors of numbers (my_x and my_y) and then plot one against the other using different type = values to see what type of plots are produced. Don‚Äôt worry about the par(mfrow = c(2, 2)) line of code yet. We‚Äôre just using this to split the plotting device so we can fit all four plots on the same device to save some space. See Section 4.4 in the Chapter for more details about this. The top left plot is type = \"l\", the top right type = \"b\", bottom left type = \"o\" and bottom right is type = \"c\".\nmy_x &lt;- 1:10\nmy_y &lt;- seq(from = 1, to = 20, by = 2)\n\npar(mfrow = c(2, 2))\nplot(my_x, my_y, type = \"l\")\nplot(my_x, my_y, type = \"b\")\nplot(my_x, my_y, type = \"o\")\nplot(my_x, my_y, type = \"c\")\nAdmittedly the plots we‚Äôve produced so far don‚Äôt look anything particularly special. However, the plot() function is incredibly versatile and can generate a large range of plots which you can customise to your own taste. We‚Äôll cover how to customise ggplots in Section 4.5. As a quick aside, the plot() function is also what‚Äôs known as a generic function which means it can change its default behaviour depending on the type of object used as an argument. You will see an example of this in Section 9.6 where we use the plot() function to generate diagnostic plots of residuals from a linear model object (bet you can‚Äôt wait!).",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#ggplot2",
    "href": "04-graphics_short.html#ggplot2",
    "title": "4¬† Figures",
    "section": "\n4.2 ggplot2",
    "text": "4.2 ggplot2\nAs mentioned earlier ggplot grammar requires several elements to produce a graphic (Figure¬†4.1) and a minimum of 3 are required:\n\na data frame\na mapping system defining x and y\na geometry layer\n\nThe data and mapping are provided within the called to the ggplot() function with the data and mapping arguments. The geometry layer is added using specific functions.\nIn fact all layers are needed but default simple values of the other layers are automatically provided.\nTo redo the Figure¬†4.2, that contain only a scatterplot of point we can use the geom_point() function.\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point()\n\n\n\n\n\n\nFigure¬†4.3\n\n\n\n\nNow that we have basic understanding of ggplotwe can explore some graphics using both base R and ggplot code",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#simple-plots",
    "href": "04-graphics_short.html#simple-plots",
    "title": "4¬† Figures",
    "section": "\n4.3 Simple plots",
    "text": "4.3 Simple plots\n\n4.3.1 Scatterplots\nSimple type of plots really useful to have a look at the relation between 2 variables for example. Here are the code to do it using base R (Figure¬†4.2)\n\nplot(fluffyness ~ weight, data = unicorns)\n\nor ggplot (Figure¬†4.3)\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point()\n\nOne gig advantage of ggplot for simple scatterplot is the ease with which we can add a regression, smoother (loes or gam) line to the plot using stat_smooth()function to add a statistic layer to the plot.\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point() +\n  stat_smooth()\n\n\n\n\n\n\n\n\n4.3.2 Histograms\nFrequency histograms are useful when you want to get an idea about the distribution of values in a numeric variable. Using base R, the hist() function takes a numeric vector as its main argument. In ggplot, we need to use geom_histogram(). Let‚Äôs generate a histogram of the height values.\nWith base R\n\nhist(unicorns$height)\n\n\n\n\n\n\n\nwith ggplot2\n\nggplot(unicorns, aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThe hist() and geom_histogram() function automatically creates the breakpoints (or bins) in the histogram unless you specify otherwise by using the breaks = argument. For example, let‚Äôs say we want to plot our histogram with breakpoints every 1 cm unicorns height. We first generate a sequence from zero to the maximum value of height (18 rounded up) in steps of 1 using the seq() function. We can then use this sequence with the breaks = argument. While we‚Äôre at it, let‚Äôs also replace the ugly title for something a little better using the main = argument\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nhist(unicorns$height, breaks = brk, main = \"Unicorn height\")\n\n\n\n\n\n\n\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(breaks = brk) +\n  ggtitle(\"Unicorn height\")\n\n\n\n\n\n\n\nYou can also display the histogram as a proportion rather than a frequency by using the freq = FALSE argument to hist() or indicating aes(y = after_stat(density)) in geom_histogram().\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nhist(unicorns$height,\n  breaks = brk, main = \"Unicorn height\",\n  freq = FALSE\n)\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)), breaks = brk) +\n  ggtitle(\"Unicorn height\")\n\nAn alternative to plotting just a straight up histogram is to add a [kernel density][kernel-dens] curve to the plot. In base R, you first need to compute the kernel density estimates using the density() and then ad the estimates to plot as a line using the lines() function.\n\ndens &lt;- density(unicorns$height)\nhist(unicorns$height,\n  breaks = brk, main = \"Unicorn height\",\n  freq = FALSE\n)\nlines(dens)\n\n\n\n\n\n\n\nWith ggplot, you can simply add the geom_density() layer to the plot\n\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)), breaks = brk) +\n  geom_density() +\n  ggtitle(\"Unicorn height\")\n\n\n\n\n\n\n\n\n4.3.3 Box plots\nOK, we‚Äôll just come and out and say it, we love boxplots and their close relation the violin plot. Boxplots (or box-and-whisker plots to give them their full name) are very useful when you want to graphically summarise the distribution of a variable, identify potential unusual values and compare distributions between different groups. The reason we love them is their ease of interpretation, transparency and relatively high data-to-ink ratio (i.e. they convey lots of information efficiently). We suggest that you try to use boxplots as much as possible when exploring your data and avoid the temptation to use the more ubiquitous bar plot (even with standard error or 95% confidence intervals bars). The problem with bar plots (aka dynamite plots) is that they hide important information from the reader such as the distribution of the data and assume that the error bars (or confidence intervals) are symmetric around the mean. Of course, it‚Äôs up to you what you do but if you‚Äôre tempted to use bar plots just search for ‚Äòdynamite plots are evil‚Äô or see [here][dynamite-plot1] or [here][dynamite-plot2] for a fuller discussion.\nTo create a boxplot in R we use the boxplot() function. For example, let‚Äôs create a boxplot of the variable weight from our unicorns data frame. We can also include a y axis label using the ylab = argument.\n\nboxplot(unicorns$weight, ylab = \"weight (g)\")\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight)) +\n  geom_boxplot() +\n  labs(y = \"weight (g)\")\n\n\n\n\n\n\n\nThe thick horizontal line in the middle of the box is the median value of weight (around 11 g). The upper line of the box is the upper quartile (75th percentile) and the lower line is the lower quartile (25th percentile). The distance between the upper and lower quartiles is known as the inter quartile range and represents the values of weight for 50% of the data. The dotted vertical lines are called the whiskers and their length is determined as 1.5 x the inter quartile range. Data points that are plotted outside the the whiskers represent potential unusual observations. This doesn‚Äôt mean they are unusual, just that they warrant a closer look. We recommend using boxplots in combination with Cleveland dotplots to identify potential unusual observations (see the Section 4.3.5 for more details). The neat thing about boxplots is that they not only provide a measure of central tendency (the median value) they also give you an idea about the distribution of the data. If the median line is more or less in the middle of the box (between the upper and lower quartiles) and the whiskers are more or less the same length then you can be reasonably sure the distribution of your data is symmetrical.\nIf we want examine how the distribution of a variable changes between different levels of a factor we need to use the formula notation with the boxplot() function. For example, let‚Äôs plot our weight variable again, but this time see how this changes with each level of food. When we use the formula notation with boxplot() we can use the data = argument to save some typing. We‚Äôll also introduce an x axis label using the xlab = argument.\n\nboxplot(weight ~ food,\n  data = unicorns,\n  ylab = \"Weight (g)\", xlab = \"food level\"\n)\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\nThe factor levels are plotted in the same order defined by our factor variable food (often alphabetically). To change the order we need to change the order of our levels of the food factor in our data frame using the factor() function and then re-plot the graph. Let‚Äôs plot our boxplot with our factor levels going from low to high.\n\nunicorns$food &lt;- factor(unicorns$food,\n  levels = c(\"low\", \"medium\", \"high\")\n)\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\nWe can also group our variables by two factors in the same plot. Let‚Äôs plot our weight variable but this time plot a separate box for each food and parental care treatment (p_care) combination.\n\nboxplot(weight ~ food * p_care,\n  data = unicorns,\n  ylab = \"weight (g)\", xlab = \"food level\"\n)\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\") +\n  facet_grid(.\n  ~ p_care)\n\n\n\n\n\n\n\nThis plot looks much better in ggplot with the use of facet_grid allowing to make similar plots as a function of a third (or even fourth) variable.\n\n4.3.4 Violin plots\nViolin plots are like a combination of a boxplot and a kernel density plot (you saw an example of a kernel density plot in the histogram section above) all rolled into one figure. We can create a violin plot in R using the vioplot() function from the vioplot package. You‚Äôll need to first install this package using install.packages('vioplot') function as usual. The nice thing about the vioplot() function is that you use it in pretty much the same way you would use the boxplot() function. We‚Äôll also use the argument col = \"lightblue\" to change the fill colour to light blue.\n\nlibrary(vioplot)\nvioplot(weight ~ food,\n  data = unicorns,\n  ylab = \"weight (g)\", xlab = \"food Concentration\",\n  col = \"lightblue\"\n)\n\n\n\n\n\n\n\nIn the violin plot above we have our familiar boxplot for each food level but this time the median value is represented by a white circle. Plotted around each boxplot is the kernel density plot which represents the distribution of the data for each food level.\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\n\n4.3.5 Dot charts\nIdentifying unusual observations (aka outliers) in numeric variables is extremely important as they may influence parameter estimates in your statistical model or indicate an error in your data. A really useful (if undervalued) plot to help identify outliers is the Cleveland dotplot. You can produce a dotplot in R very simply by using the dotchart() function.\n\ndotchart(unicorns$height)\n\n\n\n\n\n\n\nIn the dotplot above the data from the height variable is plotted along the x axis and the data is plotted in the order it occurs in the unicorns data frame on the y axis (values near the top of the y axis occur later in the data frame with those lower down occurring at the beginning of the data frame). In this plot we have a single value extending to the right at about 17 cm but it doesn‚Äôt appear particularly large compared to the rest. An example of a dotplot with an unusual observation is given below.\n\n\n\n\n\n\n\n\nWe can also group the values in our height variable by a factor variable such as food using the groups = argument. This is useful for identifying unusual observations within a factor level that might be obscured when looking at all the data together.\n\ndotchart(unicorns$height, groups = unicorns$food)\n\n\n\n\n\n\n\n\n\nggdotchart(data = unicorns, x = \"height\", y = \"food\")\n\n\n\n\n\n\n\n\n4.3.6 Pairs plots\nPreviously in this Chapter we used the plot() function to create a scatterplot to explore the relationship between two numeric variables. With datasets that contain many numeric variables, it‚Äôs often handy to create multiple scatterplots to visualise relationships between all these variables. We could use the plot() function to create each of these plot individually, but a much easier way is to use the pairs() function. The pairs() function creates a multi-panel scatterplot (sometimes called a scatterplot matrix) which plots all combinations of variables. Let‚Äôs create a multi-panel scatterplot of all of the numeric variables in our unicorns data frame. Note, you may need to click on the ‚ÄòZoom‚Äô button in RStudio to display the plot clearly.\n\npairs(unicorns[, c(\n  \"height\", \"weight\", \"mane_size\",\n  \"fluffyness\", \"horn_rings\"\n)])\n\n\n\n\n\n\n# or we could use the equivalent\n# pairs(unicorns[, 4:8])\n\nInterpretation of the pairs plot takes a bit of getting used to. The panels on the diagonal give the variable names. The first row of plots displays the height variable on the y axis and the variables weight, mane_size, fluffyness and unicorns on the x axis for each of the four plots respectively. The next row of plots have weight on the y axis and height, mane_size, fluffyness and unicorns on the x axis. We interpret the rest of the rows in the same way with the last row displaying the unicorns variable on the y axis and the other variables on the x axis. Hopefully you‚Äôll notice that the plots below the diagonal are the same plots as those above the diagonal just with the axis reversed.\nTo do pairs plot with ggplot, you nee the ggpairs()function from GGallypackage. The output is quite similar but you have only the lower part of the matrix of plots, you get a density plot on the diagonal and the correlations on the upper part of the plot.\n\nggpairs(unicorns[, c(\n  \"height\", \"weight\", \"mane_size\",\n  \"fluffyness\", \"horn_rings\"\n)])\n\n\n\n\n\n\n\nThe pairs() function can be tweak to do similar things and more but is more involved. Have a lok at the great help file for the pairs() function (?pairs)which provide all the details to do something like the plot below.\n\n\n\n\n\n\n\n\n\n4.3.7 Coplots\nWhen examining the relationship between two numeric variables, it is often useful to be able to determine whether a third variable is obscuring or changing any relationship. A really handy plot to use in these situations is a conditioning plot (also known as conditional scatterplot plot) which we can create in R by using the coplot() function. The coplot() function plots two variables but each plot is conditioned (|) by a third variable. This third variable can be either numeric or a factor. As an example, let‚Äôs look at how the relationship between the number of unicorns (unicorns variable) and the weight of unicorns changes dependent on mane_size. Note the coplot() function has a data = argument so no need to use the $ notation.\n\ncoplot(horn_rings ~ weight | mane_size, data = unicorns)\n\n\n\n\n\n\n\n\ngg_coplot(unicorns,\n  x = weight, y = horn_rings,\n  faceting = mane_size\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIt takes a little practice to interpret coplots. The number of unicorns is plotted on the y axis and the weight of unicorns on the x axis. The six plots show the relationship between these two variables for different ranges of leaf area. The bar plot at the top indicates the range of leaf area values for each of the plots. The panels are read from bottom left to top right along each row. For example, the bottom left panel shows the relationship between number of unicorns and weight for unicorns with the lowest range of leaf area values (approximately 5 - 11 cm2). The top right plot shows the relationship between unicorns and weight for unicorns with a leaf area ranging from approximately 16 - 50 cm2. Notice that the range of values for leaf area differs between panels and that the ranges overlap from panel to panel. The coplot() function does it‚Äôs best to split the data up to ensure there are an adequate number of data points in each panel. If you don‚Äôt want to produce plots with overlapping data in the panel you can set the overlap = argument to overlap = 0\nYou can also use the coplot() function with factor conditioning variables. With gg_coplot() you need to first set the factor as numeric before plotting and specify overlap=0. For example, we can examine the relationship between unicorns and weight variables conditioned on the factor food. The bottom left plot is the relationship between unicorns and weight for those unicorns in the low food treatment. The top left plot shows the same relationship but for unicorns in the high food treatment.\n\ncoplot(horn_rings ~ weight | food, data = unicorns)\n\n\n\n\n\n\n\n\nunicorns &lt;- mutate(unicorns, food_num = as.numeric(food))\ngg_coplot(unicorns,\n  x = weight, y = horn_rings,\n  faceting = food_num, overlap = 0\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n4.3.8 Summary of plot function\n\n\n\n\n\n\n\nGraph type\nggplot2\nBase R function\n\n\n\nscatterplot\ngeom_point()\nplot()\n\n\nfrequency histogram\ngeom_histogram()\nhist()\n\n\nboxplot\ngeom_boxplot()\nboxplot()\n\n\nCleveland dotplot\nggdotchart()\ndotchart()\n\n\nscatterplot matrix\nggpairs()\npairs()\n\n\nconditioning plot\ngg_coplot()\ncoplot()\n\n\n\nHopefully, you‚Äôre getting the idea that we can create really informative exploratory plots quite easily using either base R or ggplot graphics. Which one you use is entirely up to you (that‚Äôs the beauty of using R, you get to choose) and we happily mix and match to suit our needs. In the next section we cover how to customise your base R plots to get them to look exactly how you want.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#sec-mult-graphs",
    "href": "04-graphics_short.html#sec-mult-graphs",
    "title": "4¬† Figures",
    "section": "\n4.4 Multiple graphs",
    "text": "4.4 Multiple graphs\n\n4.4.1 Base R\nIn base R, one of the most common methods to plot multiple graphs is to use the main graphical function par() to split the plotting device up into a number of defined sections using the mfrow = argument. With this method, you first need to specify the number of rows and columns of plots you would like and then run the code for each plot. For example, to plot two graphs side by side we would use par(mfrow = c(1, 2)) to split the device into 1 row and two columns.\n\npar(mfrow = c(1, 2))\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight\",\n  ylab = \"Fluffyness\"\n)\nboxplot(fluffyness ~ food, data = unicorns, cex.axis = 0.6)\n\n\n\n\n\n\n\nOnce you‚Äôve finished making your plots don‚Äôt forget to reset your plotting device back to normal with par(mfrow = c(1,1)).\n\n4.4.2 ggplot\nUsing ggplot in addition to the facet_grid() and facet_wrap functions allowing to easily repeat and organise multiple plots as a function of specific variables, there are multiple way of organising multiple ggplot together. The approach we recommend is using the package patchwork.\nFirst you will need to install (if you don‚Äôt have it yet) and make the patchwork üì¶ package available.\n\ninstall.packages(\"patchwork\")\nlibrary(patchwork)\n\nAn important note: For those who have used base R to produce their figures and are familiar with using par(mfrow = c(2,2)) (which allows plotting of four figures in two rows and two columns) be aware that this does not work for ggplot2 objects. Instead you will need to use either the patchwork package or alternative packages such as gridArrange or cowplot or covert the ggplot2 objects to grobs.\nTo plot both of the plots together we need to assign each figure to a separate object and then use these objects when we use patchwork.\nSo we can generate 2 figures and assign them to objects. As you can see, the figures do not appear in the plot window. They will appear only when you call the object.\n\nfirst_figure &lt;- ggplot(\n  aes(x = height, y = fluffyness, color = food),\n  data = unicorns\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(block ~ p_care)\nsecond_figure &lt;- ggplot(\n  aes(x = weight, y = fluffyness, color = food),\n  data = unicorns\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(block ~ p_care)\n\nWe have two immediate and simple options with patchwork; arrange figures on top of each other (specified with a /) or arrange figures side-by-side (specified with either a + or a |). Let‚Äôs try to plot both figures, one on top of the other.\n\nfirst_figure / second_figure\n\n\n\n\n\n\n\nPlay around: Try to create a side-by-side version of the above figure (hint: try the other operators).\nWe can take this one step further and assign nested patchwork figures to an object and use this in turn to create labels for individuals figures.\n\nnested_compare &lt;- first_figure / second_figure\n\nnested_compare +\n  plot_annotation(tag_levels = \"A\", tag_suffix = \")\")",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#sec-custom-plot",
    "href": "04-graphics_short.html#sec-custom-plot",
    "title": "4¬† Figures",
    "section": "\n4.5 Customising ggplots",
    "text": "4.5 Customising ggplots\nWent for a walk to be edited ü¶Ñ",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#sec-export-plots",
    "href": "04-graphics_short.html#sec-export-plots",
    "title": "4¬† Figures",
    "section": "\n4.6 Exporting plots",
    "text": "4.6 Exporting plots\nCreating plots in R is all well and good but what if you want to use these plots in your thesis, report or publication? One option is to click on the ‚ÄòExport‚Äô button in the ‚ÄòPlots‚Äô tab in RStudio. You can also export your plots from R to an external file by writing some code in your R script. The advantage of this approach is that you have a little more control over the output format and it also allows you to generate (or update) plots automatically whenever you run your script. You can export your plots in many different formats but the most common are, pdf, png, jpeg and tiff.\nBy default, R (and therefore RStudio) will direct any plot you create to the plot window. To save your plot to an external file you first need to redirect your plot to a different graphics device. You do this by using one of the many graphics device functions to start a new graphic device. For example, to save a plot in pdf format we will use the pdf() function. The first argument in the pdf() function is the filepath and filename of the file we want to save (don‚Äôt forget to include the .pdf extension). Once we‚Äôve used the pdf() function we can then write all of the code we used to create our plot including any graphical parameters such as setting the margins and splitting up the plotting device. Once the code has run we need to close the pdf plotting device using the dev.off() function.\n\npdf(file = \"output/my_plot.pdf\")\npar(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = \"i\", yaxs = \"i\")\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight (g)\",\n  ylab = expression(paste(\"shoot area (cm\"^\"2\", \")\")),\n  xlim = c(0, 30), ylim = c(0, 200), bty = \"l\",\n  las = 1, cex.axis = 0.8, tcl = -0.2,\n  pch = 16, col = \"dodgerblue1\", cex = 0.9\n)\ntext(x = 28, y = 190, label = \"A\", cex = 2)\ndev.off()\n\nIf we want to save this plot in png format we simply use the png() function in more or less the same way we used the pdf() function.\n\npng(\"output/my_plot.png\")\npar(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = \"i\", yaxs = \"i\")\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight (g)\",\n  ylab = expression(paste(\"shoot area (cm\"^\"2\", \")\")),\n  xlim = c(0, 30), ylim = c(0, 200), bty = \"l\",\n  las = 1, cex.axis = 0.8, tcl = -0.2,\n  pch = 16, col = \"dodgerblue1\", cex = 0.9\n)\ntext(x = 28, y = 190, label = \"A\", cex = 2)\ndev.off()\n\nOther useful functions are; jpeg(), tiff() and bmp(). Additional arguments to these functions allow you to change the size, resolution and background colour of your saved images. See ?png for more details.\nggplot2 üì¶ provide a really useful function ggsave() function which simplify saving plots a lot but works only for ggplots.\nAfter producing a plot and seeing it in your IDE, you can simply run ggsave() with the adequate argument to save the last ggplot produced. You can of course, also, specify which plot to save.\n\nggsave(\"file.png\")\n\n\n\n\n\nWilkinson, L. 2005. The Grammar of Graphics. Springer Science & Business Media.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "05-programming.html",
    "href": "05-programming.html",
    "title": "5¬† Programming",
    "section": "",
    "text": "5.1 Looking behind the curtain\nA good way to start learning to program in R is to see what others have done. We can start by briefly peeking behind the curtain. With many functions in R, if you want to have a quick glance at the machinery behind the scenes, we can simply write the function name but without the ().\nNote that to view the source code of base R packages (those that come with R) requires some additional steps which we won‚Äôt cover here (see this link if you‚Äôre interested), but for most other packages that you install yourself, generally entering the function name without () will show the source code of the function.\nWhat can have a look at the function to fit a linear model lm()\nlm\n\nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \n{\n    ret.x &lt;- x\n    ret.y &lt;- y\n    cl &lt;- match.call()\n    mf &lt;- match.call(expand.dots = FALSE)\n    m &lt;- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\", \n        \"offset\"), names(mf), 0L)\n    mf &lt;- mf[c(1L, m)]\n    mf$drop.unused.levels &lt;- TRUE\n    mf[[1L]] &lt;- quote(stats::model.frame)\n    mf &lt;- eval(mf, parent.frame())\n    if (method == \"model.frame\") \n        return(mf)\n    else if (method != \"qr\") \n        warning(gettextf(\"method = '%s' is not supported. Using 'qr'\", \n            method), domain = NA)\n    mt &lt;- attr(mf, \"terms\")\n    y &lt;- model.response(mf, \"numeric\")\n    w &lt;- as.vector(model.weights(mf))\n    if (!is.null(w) && !is.numeric(w)) \n        stop(\"'weights' must be a numeric vector\")\n    offset &lt;- model.offset(mf)\n    mlm &lt;- is.matrix(y)\n    ny &lt;- if (mlm) \n        nrow(y)\n    else length(y)\n    if (!is.null(offset)) {\n        if (!mlm) \n            offset &lt;- as.vector(offset)\n        if (NROW(offset) != ny) \n            stop(gettextf(\"number of offsets is %d, should equal %d (number of observations)\", \n                NROW(offset), ny), domain = NA)\n    }\n    if (is.empty.model(mt)) {\n        x &lt;- NULL\n        z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, \n            ncol(y)) else numeric(), residuals = y, fitted.values = 0 * \n            y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != \n            0) else ny)\n        if (!is.null(offset)) {\n            z$fitted.values &lt;- offset\n            z$residuals &lt;- y - offset\n        }\n    }\n    else {\n        x &lt;- model.matrix(mt, mf, contrasts)\n        z &lt;- if (is.null(w)) \n            lm.fit(x, y, offset = offset, singular.ok = singular.ok, \n                ...)\n        else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n            ...)\n    }\n    class(z) &lt;- c(if (mlm) \"mlm\", \"lm\")\n    z$na.action &lt;- attr(mf, \"na.action\")\n    z$offset &lt;- offset\n    z$contrasts &lt;- attr(x, \"contrasts\")\n    z$xlevels &lt;- .getXlevels(mt, mf)\n    z$call &lt;- cl\n    z$terms &lt;- mt\n    if (model) \n        z$model &lt;- mf\n    if (ret.x) \n        z$x &lt;- x\n    if (ret.y) \n        z$y &lt;- y\n    if (!qr) \n        z$qr &lt;- NULL\n    z\n}\n&lt;bytecode: 0x6392c6069a40&gt;\n&lt;environment: namespace:stats&gt;\nWhat we see above is the underlying code for this particular function. We could copy and paste this into our own script and make any changes we deemed necessary, although tread carefully and test the changes you‚Äôve made.\nDon‚Äôt worry overly if most of the code contained in functions doesn‚Äôt make sense immediately. This will be especially true if you are new to R, in which case it seems incredibly intimidating. Honestly, it can be intimidating even after years of R experience. To help with that, we‚Äôll begin by making our own functions in R in the next section.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#functions-in-r",
    "href": "05-programming.html#functions-in-r",
    "title": "5¬† Programming",
    "section": "\n5.2 Functions in R",
    "text": "5.2 Functions in R\nFunctions are the bread and butter of R, the essential sustaining elements allowing you to work with R. They‚Äôre made (most of the time) with the utmost care and attention but may end up being something of a Frankenstein‚Äôs monster - with weirdly attached limbs. But no matter how convoluted they may be they will always faithfully do the same thing.\nThis means that functions can also be very stupid.\nIf we asked you to go to the supermarket to get us some ingredients to make Balmoral chicken, even if you don‚Äôt know what the heck that is, you‚Äôd be able to guess and bring at least something back. Or you could decide to make something else. Or you could ask a chef for help. Or you could pull out your phone and search online for what Balmoral chicken is. The point is, even if we didn‚Äôt give you enough information to do the task, you‚Äôre intelligent enough to, at the very least, try to find a work around.\nIf instead, we asked a function to do the same, it would listen intently to our request, and then will simply return an error. It would then repeat this every single time we asked it to do the job when the task is not clear. The point here, is that code and functions cannot find workarounds to poorly provided information, which is great. It‚Äôs totally reliant on you, to tell it very explicitly what it needs to do step by step.\nRemember two things: the intelligence of code comes from the coder, not the computer and functions need exact instructions to work.\nTo prevent functions from being too stupid you must provide the information the function needs in order for it to work. As with the Balmoral chicken example, if we‚Äôd supplied a recipe list to the function, it would have managed just fine. We call this ‚Äúfulfilling an argument‚Äù. The vast majority of functions require the user to fulfill at least one argument.\nThis can be illustrated in the pseudocode below. When we make a function we can:\n\nspecify what arguments the user must fulfill (e.g. arg1 and arg2)\nprovide default values to arguments (e.g. arg2 = TRUE)\ndefine what to do with the arguments (expression):\n\n\nmy_function &lt;- function(arg1, arg2, ...) {\n  expression\n}\n\nThe first thing to note is that we‚Äôve used the function function() to create a new function called my_function. To walk through the above code; we‚Äôre creating a function called my_function. Within the round brackets we specify what information (i.e. arguments) the function requires to run (as many or as few as needed). These arguments are then passed to the expression part of the function. The expression can be any valid R command or set of R commands and is usually contained between a pair of braces { }. Once you run the above code, you can then use your new function by typing:\n\nmy_function(arg1, arg2)\n\nLet‚Äôs work through an example to help clear things up.\nFirst we are going to create a data frame called dishes, where columns lasagna, stovies, poutine, and tartiflette are filled with 10 random values drawn from a bag (using the rnorm() function to draw random values from a Normal distribution with mean 0 and standard deviation of 1). We also include a ‚Äúproblem‚Äù, for us to solve later, by including 3 NA values within the poutine column (using rep(NA, 3)).\n\ndishes &lt;- data.frame(\n  lasagna = rnorm(10),\n  stovies = rnorm(10),\n  poutine = c(rep(NA, 3), rnorm(7)),\n  tartiflette = rnorm(10)\n)\n\nLet‚Äôs say that you want to multiply the values in the variables stovies and lasagna and create a new object called stovies_lasagna. We can do this ‚Äúby hand‚Äù using:\n\nstovies_lasagna &lt;- dishes$stovies * dishes$lasagna\n\nIf this was all we needed to do, we can stop here. R works with vectors, so doing these kinds of operations in R is actually much simpler than other programming languages, where this type of code might require loops (we say that R is a vectorised language). Something to keep in mind for later is that doing these kinds of operations with loops can be much slower compared to vectorisation.\nBut what if we want to repeat this multiplication many times? Let‚Äôs say we wanted to multiply columns lasagna and stovies, stovies and tartiflette, and poutine and tartiflette. In this case we could copy and paste the code, replacing the relevant information.\n\nlasagna_stovies &lt;- dishes$lasagna * dishes$stovies\nstovies_tartiflette &lt;- dishes$stovies * dishes$stovies\npoutine_tartiflette &lt;- dishes$poutine * dishes$tartiflette\n\nWhile this approach works, it‚Äôs easy to make mistakes. In fact, here we‚Äôve ‚Äúforgotten‚Äù to change stovies to tartiflette in the second line of code when copying and pasting. This is where writing a function comes in handy. If we were to write this as a function, there is only one source of potential error (within the function itself) instead of many copy-pasted lines of code (which we also cut down on by using a function).\n\n\n\n\n\n\nTip\n\n\n\nAs a rule of thumb if we have to do the same thing (by copy/paste & modify) 3 times or more, we just make a function for it.\n\n\nIn this case, we‚Äôre using some fairly trivial code where it‚Äôs maybe hard to make a genuine mistake. But what if we increased the complexity?\n\ndishes$lasagna * dishes$stovies / dishes$lasagna + (dishes$lasagna * 10^(dishes$stovies))\n-dishes$stovies - (dishes$lasagna * sqrt(dishes$stovies + 10))\n\nNow imagine having to copy and paste this three times, and in each case having to change the lasagna and stovies variables (especially if we had to do it more than three times).\nWhat we could do instead is generalize our code for x and y columns instead of naming specific dishes. If we did this, we could recycle the x * y code. Whenever we wanted to multiple columns together, we assign a dishes to either x or y. We‚Äôll assign the multiplication to the objects lasagna_stovies and stovies_poutine so we can come back to them later.\n\n# Assign x and y values\nx &lt;- dishes$lasagna\ny &lt;- dishes$stovies\n\n# Use multiplication code\nlasagna_stovies &lt;- x * y\n\n# Assign new x and y values\nx &lt;- dishes$stovies\ny &lt;- dishes$poutine\n\n# Reuse multiplication code\nstovies_poutine &lt;- x * y\n\nThis is essentially what a function does. Let‚Äôs call our new function multiply_cols() and define it with two arguments, x and y. A function in R will simply return its last value. However, it is possible to force the function to return an earlier value if wanted/needed. Using the return() function is not strictly necessary in this example as R will automatically return the value of the last line of code in our function. We include it here to make this explicit.\n\nmultiply_cols &lt;- function(x, y) {\n  return(x * y)\n}\n\nNow that we‚Äôve defined our function we can use it. Let‚Äôs use the function to multiple the columns lasagna and stovies and assign the result to a new object called lasagna_stovies_func\n\nlasagna_stovies_func &lt;- multiply_cols(x = dishes$lasagna, y = dishes$stovies)\nlasagna_stovies_func\n\n [1]  0.15888066  0.07950564  0.10198302  0.17930855 -0.89100492 -2.07422678\n [7]  0.18301854  1.36082011 -0.26397192  0.36560595\n\n\nIf we‚Äôre only interested in multiplying dishes$lasagna and dishes$stovies, it would be overkill to create a function to do something once. However, the benefit of creating a function is that we now have that function added to our environment which we can use as often as we like. We also have the code to create the function, meaning we can use it in completely new projects, reducing the amount of code that has to be written (and retested) from scratch each time.\nTo satisfy ourselves that the function has worked properly, we can compare the lasagna_stovies variable with our new variable lasagna_stovies_func using the identical() function. The identical() function tests whether two objects are exactly identical and returns either a TRUE or FALSE value. Use ?identical if you want to know more about this function.\n\nidentical(lasagna_stovies, lasagna_stovies_func)\n\n[1] TRUE\n\n\nAnd we confirm that the function has produced the same result as when we do the calculation manually. We recommend getting into a habit of checking that the function you‚Äôve created works the way you think it has.\nNow let‚Äôs use our multiply_cols() function to multiply columns stovies and poutine. Notice now that argument x is given the value dishes$stoviesand y the value dishes$poutine.\n\nstovies_poutine_func &lt;- multiply_cols(x = dishes$stovies, y = dishes$poutine)\nstovies_poutine_func\n\n [1]           NA           NA           NA  0.134984249  0.991411525\n [6] -1.138108599 -1.153075159  1.636003714  0.008278524 -0.076287115\n\n\nSo far so good. All we‚Äôve really done is wrapped the code x * y into a function, where we ask the user to specify what their x and y variables are.\nUsing the function is a bit long since we have to retype the name of the data frame for each variable. For a bit of fun we can modify the function so that, we can specify the data frame as an argument and the column names without quoting them (as in a tidyverse style).\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = prod(.)) %&gt;%\n    pull(xy)\n}\n\nFor this new version of the function, we added we added a data argument on line 1. On lines 3, we select the x and y variables provided as arguments. On line 4., we create the product of the 2 selected columns and on line 5. we extract the column we juste created. We also remove the return() function since it was not needed\nOur function is now compatible with the pipe (either native |&gt; or magrittr %&gt;%) function. However, since the function now uses the pipe from magrittr üì¶ and dplyr üì¶ functions, we need to load the tidyverse üì¶ package for it to work.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\nlasagna_stovies_func &lt;- dishes |&gt; multiply_cols(lasagna, stovies)\n\nNow let‚Äôs add a little bit more complexity. If you look at the output of poutine_tartiflette some of the calculations have produced NA values. This is because of those NA values we included in poutine when we created the dishes data frame. Despite these NA values, the function appeared to have worked but it gave us no indication that there might be a problem. In such cases we may prefer if it had warned us that something was wrong. How can we get the function to let us know when NA values are produced? Here‚Äôs one way.\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\n\nThe core of our function is still the same, but we‚Äôve now got an extra six lines of code (lines 6-11). We‚Äôve included some conditional statements, if (lines 6-8) and else (lines 9-11), to test whether any NAs have been produced and if they have we display a warning message to the user. The next section of this Chapter will explain how these work and how to use them.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#conditional-statements",
    "href": "05-programming.html#conditional-statements",
    "title": "5¬† Programming",
    "section": "\n5.3 Conditional statements",
    "text": "5.3 Conditional statements\nx * y does not apply any logic. It merely takes the value of x and multiplies it by the value of y. Conditional statements are how you inject some logic into your code. The most commonly used conditional statement is if. Whenever you see an if statement, read it as ‚ÄòIf X is TRUE, do a thing‚Äô. Including an else statement simply extends the logic to ‚ÄòIf X is TRUE, do a thing, or else do something different‚Äô.\nBoth the if and else statements allow you to run sections of code, depending on a condition is either TRUE or FALSE. The pseudo-code below shows you the general form.\n  if (condition) {\n  Code executed when condition is TRUE\n  } else {\n  Code executed when condition is FALSE\n  }\nTo delve into this a bit more, we can use an old programmer joke to set up a problem.\n\nA programmer‚Äôs partner says: ‚ÄòPlease go to the store and buy a carton of milk and if they have eggs, get six.‚Äô\nThe programmer returned with 6 cartons of milk.\nWhen the partner sees this, and exclaims ‚ÄòWhy the heck did you buy six cartons of milk?‚Äô\nThe programmer replied ‚ÄòThey had eggs‚Äô\n\nAt the risk of explaining a joke, the conditional statement here is whether or not the store had eggs. If coded as per the original request, the programmer should bring 6 cartons of milk if the store had eggs (condition = TRUE), or else bring 1 carton of milk if there weren‚Äôt any eggs (condition = FALSE). In R this is coded as:\n\neggs &lt;- TRUE # Whether there were eggs in the store\n\nif (eggs == TRUE) { # If there are eggs\n  n.milk &lt;- 6 # Get 6 cartons of milk\n} else { # If there are not eggs\n  n.milk &lt;- 1 # Get 1 carton of milk\n}\n\nWe can then check n.milk to see how many milk cartons they returned with.\n\nn.milk\n\n[1] 6\n\n\nAnd just like the joke, our R code has missed that the condition was to determine whether or not to buy eggs, not more milk (this is actually a loose example of the Winograd Scheme, designed to test the intelligence of artificial intelligence by whether it can reason what the intended referent of a sentence is).\nWe could code the exact same egg-milk joke conditional statement using an ifelse() function.\n\neggs &lt;- TRUE\nn.milk &lt;- ifelse(eggs == TRUE, yes = 6, no = 1)\n\nThis ifelse() function is doing exactly the same as the more fleshed out version from earlier, but is now condensed down into a single line of code. It has the added benefit of working on vectors as opposed to single values (more on this later when we introduce loops). The logic is read in the same way; ‚ÄúIf there are eggs, assign a value of 6 to n.milk, if there isn‚Äôt any eggs, assign the value 1 to n.milk‚Äù.\nWe can check again to make sure the logic is still returning 6 cartons of milk:\n\nn.milk\n\n[1] 6\n\n\nCurrently we‚Äôd have to copy and paste code if we wanted to change if eggs were in the store or not. We learned above how to avoid lots of copy and pasting by creating a function. Just as with the simple x * y expression in our previous multiply_cols() function, the logical statements above are straightforward to code and well suited to be turned into a function. How about we do just that and wrap this logical statement up in a function?\n\nmilk &lt;- function(eggs) {\n  if (eggs == TRUE) {\n    6\n  } else {\n    1\n  }\n}\n\nWe‚Äôve now created a function called milk() where the only argument is eggs. The user of the function specifies if eggs is either TRUE or FALSE, and the function will then use a conditional statement to determine how many cartons of milk are returned.\nLet‚Äôs quickly try:\n\nmilk(eggs = TRUE)\n\n[1] 6\n\n\nAnd the joke is maintained. Notice in this case we have actually specified that we are fulfilling the eggs argument (eggs = TRUE). In some functions, as with ours here, when a function only has a single argument we can be lazy and not name which argument we are fulfilling. In reality, it‚Äôs generally viewed as better practice to explicitly state which arguments you are fulfilling to avoid potential mistakes.\nOK, lets go back to the multiply_cols() function we created above and explain how we‚Äôve used conditional statements to warn the user if NA values are produced when we multiple any two columns together.\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\nIn this new version of the function we still use x * y as before but this time we‚Äôve assigned the values from this calculation to a temporary vector called temp_var so we can use it in our conditional statements. Note, this temp_var variable is local to our function and will not exist outside of the function due something called R‚Äôs scoping rules. We then use an if statement to determine whether our temp_var variable contains any NA values. The way this works is that we first use the is.na() function to test whether each value in our temp_var variable is an NA. The is.na() function returns TRUE if the value is an NA and FALSE if the value isn‚Äôt an NA. We then nest the is.na(temp_var) function inside the function any() to test whether any of the values returned by is.na(temp_var) are TRUE. If at least one value is TRUE the any() function will return a TRUE. So, if there are any NA values in our temp_var variable the condition for the if() function will be TRUE whereas if there are no NA values present then the condition will be FALSE. If the condition is TRUE the warning() function generates a warning message for the user and then returns the temp_var variable. If the condition is FALSE the code below the else statement is executed which just returns the temp_var variable.\nSo if we run our modified multiple_columns() function on the columns dishes$stovies and dishes$poutine (which contains NAs) we will receive an warning message.\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\n\nWhereas if we multiple two columns that don‚Äôt contain NA values we don‚Äôt receive a warning message\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#combining-logical-operators",
    "href": "05-programming.html#combining-logical-operators",
    "title": "5¬† Programming",
    "section": "\n5.4 Combining logical operators",
    "text": "5.4 Combining logical operators\nThe functions that we‚Äôve created so far have been perfectly suited for what we need, though they have been fairly simplistic. Let‚Äôs try creating a function that has a little more complexity to it. We‚Äôll make a function to determine if today is going to be a good day or not based on two criteria. The first criteria will depend on the day of the week (Friday or not) and the second will be whether or not your code is working (TRUE or FALSE). To accomplish this, we‚Äôll be using if and else statements. The complexity will come from if statements immediately following the relevant else statement. We‚Äôll use such conditional statements four times to achieve all combinations of it being a Friday or not, and if your code is working or not.\nWe also used the cat() function to output text formatted correctly.\n\ngood.day &lt;- function(code.working, day) {\n  if (code.working == TRUE && day == \"Friday\") {\n    cat(\n  \"BEST.\n  DAY.\n    EVER.\n      Stop while you are ahead and go to the pub!\"\n    )\n  } else if (code.working == FALSE && day == \"Friday\") {\n    cat(\"Oh well, but at least it's Friday! Pub time!\")\n  } else if (code.working == TRUE && day != \"Friday\") {\n    cat(\"\n  So close to a good day...\n  shame it's not a Friday\"\n    )\n  } else if (code.working == FALSE && day != \"Friday\") {\n    cat(\"Hello darkness.\")\n  }\n}\n\n\ngood.day(code.working = TRUE, day = \"Friday\")\n\nBEST.\n  DAY.\n    EVER.\n      Stop while you are ahead and go to the pub!\n\ngood.day(FALSE, \"Tuesday\")\n\nHello darkness.\n\n\nNotice that we never specified what to do if the day was not a Friday? That‚Äôs because, for this function, the only thing that matters is whether or not it‚Äôs Friday.\nWe‚Äôve also been using logical operators whenever we‚Äôve used if statements. Logical operators are the final piece of the logical conditions jigsaw. Below is a table which summarises operators. The first two are logical operators and the final six are relational operators. You can use any of these when you make your own functions (or loops).\n\n\n\n\n\n\n\n\nOperator\nTechnical Description\nWhat it means\nExample\n\n\n\n&&\nLogical AND\nBoth conditions must be met\nif(cond1 == test && cond2 == test)\n\n\n||\nLogical OR\nEither condition must be met\nif(cond1 == test || cond2 == test)\n\n\n&lt;\nLess than\nX is less than Y\nif(X &lt; Y)\n\n\n&gt;\nGreater than\nX is greater than Y\nif(X &gt; Y)\n\n\n&lt;=\nLess than or equal to\nX is less/equal to Y\nif(X &lt;= Y)\n\n\n&gt;=\nGreater than or equal to\nX is greater/equal to Y\nif(X &gt;= Y)\n\n\n==\nEqual to\nX is equal to Y\nif(X == Y)\n\n\n!=\nNot equal to\nX is not equal to Y\nif(X != Y)",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#loops",
    "href": "05-programming.html#loops",
    "title": "5¬† Programming",
    "section": "\n5.5 Loops",
    "text": "5.5 Loops\nR is very good at performing repetitive tasks. If we want a set of operations to be repeated several times we use what‚Äôs known as a loop. When you create a loop, R will execute the instructions in the loop a specified number of times or until a specified condition is met. There are three main types of loop in R: the for loop, the while loop and the repeat loop.\nLoops are one of the staples of all programming languages, not just R, and can be a powerful tool (although in our opinion, used far too frequently when writing R code).\n\n5.5.1 For loop\nThe most commonly used loop structure when you want to repeat a task a defined number of times is the for loop. The most basic example of a for loop is:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nBut what‚Äôs the code actually doing? This is a dynamic bit of code were an index i is iteratively replaced by each value in the vector 1:5. Let‚Äôs break it down. Because the first value in our sequence (1:5) is 1, the loop starts by replacing i with 1 and runs everything between the { }. Loops conventionally use i as the counter, short for iteration, but you are free to use whatever you like, even your pet‚Äôs name, it really does not matter (except when using nested loops, in which case the counters must be called different things, like SenorWhiskers and HerrFlufferkins).\nSo, if we were to do the first iteration of the loop manually\n\ni &lt;- 1\nprint(i)\n\n[1] 1\n\n\nOnce this first iteration is complete, the for loop loops back to the beginning and replaces i with the next value in our 1:5 sequence (2 in this case):\n\ni &lt;- 2\nprint(i)\n\n[1] 2\n\n\nThis process is then repeated until the loop reaches the final value in the sequence (5 in this example) after which point it stops.\nTo reinforce how for loops work and introduce you to a valuable feature of loops, we‚Äôll alter our counter within the loop. This can be used, for example, if we‚Äôre using a loop to iterate through a vector but want to select the next row (or any other value). To show this we‚Äôll simply add 1 to the value of our index every time we iterate our loop.\n\nfor (i in 1:5) {\n  print(i + 1)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n\n\nAs in the previous loop, the first value in our sequence is 1. The loop begins by replacing i with 1, but this time we‚Äôve specified that a value of 1 must be added to i in the expression resulting in a value of 1 + 1.\n\ni &lt;- 1\ni + 1\n\n[1] 2\n\n\nAs before, once the iteration is complete, the loop moves onto the next value in the sequence and replaces i with the next value (2 in this case) so that i + 1 becomes 2 + 1.\n\ni &lt;- 2\ni + 1\n\n[1] 3\n\n\nAnd so on. We think you get the idea! In essence this is all a for loop is doing and nothing more.\nWhilst above we have been using simple addition in the body of the loop, you can also combine loops with functions.\nLet‚Äôs go back to our data frame dishes. Previously in the Chapter we created a function to multiply two columns and used it to create our lasagna_stovies, stovies_poutine, and poutine_tartiflette objects. We could have used a loop for this. Let‚Äôs remind ourselves what our data look like and the code for the multiple_columns() function.\n\ndishes &lt;- data.frame(\n  lasagna = rnorm(10),\n  stovies = rnorm(10),\n  poutine = c(rep(NA, 3), rnorm(7)),\n  tartiflette = rnorm(10)\n)\n\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\nTo use a list to iterate over these columns we need to first create an empty list (remember Section 3.2.3?) which we call temp (short for temporary) which will be used to store the output of the for loop.\n\ntemp &lt;- list()\nfor (i in 1:(ncol(dishes) - 1)) {\n  temp[[i]] &lt;- multiply_cols(dishes, x = colnames(dishes)[i], y = colnames(dishes)[i + 1])\n}\n\nWarning in multiply_cols(dishes, x = colnames(dishes)[i], y =\ncolnames(dishes)[i + : The function has produced NAs\nWarning in multiply_cols(dishes, x = colnames(dishes)[i], y =\ncolnames(dishes)[i + : The function has produced NAs\n\n\nWhen we specify our for loop notice how we subtracted 1 from ncol(dishes). The ncol() function returns the number of columns in our dishes data frame which is 4 and so our loop runs from i = 1 to i = 4 - 1 which is i = 3.\nSo in the first iteration of the loop i takes on the value 1. The multiply_cols() function multiplies the dishes[, 1] (lasagna) and dishes[, 1 + 1] (stovies) columns and stores it in the temp[[1]] which is the first element of the temp list.\nThe second iteration of the loop i takes on the value 2. The multiply_cols() function multiplies the dishes[, 2] (stovies) and dishes[, 2 + 1] (poutine) columns and stores it in the temp[[2]] which is the second element of the temp list.\nThe third and final iteration of the loop i takes on the value 3. The multiply_cols() function multiplies the dishes[, 3] (poutine) and dishes[, 3 + 1] (tartiflette) columns and stores it in the temp[[3]] which is the third element of the temp list.\nAgain, it‚Äôs a good idea to test that we are getting something sensible from our loop (remember, check, check and check again!). To do this we can use the identical() function to compare the variables we created by hand with each iteration of the loop manually.\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\ni &lt;- 1\nidentical(\n  multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + 1]),\n  lasagna_stovies_func\n)\n\n[1] TRUE\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\ni &lt;- 2\nidentical(\n  multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + 1]),\n  stovies_poutine_func\n)\n\nWarning in multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + :\nThe function has produced NAs\n\n\n[1] TRUE\n\n\nIf you can follow the examples above, you‚Äôll be in a good spot to begin writing some of your own for loops. That said there are other types of loops available to you.\n\n5.5.2 While loop\nAnother type of loop that you may use (albeit less frequently) is the while loop. The while loop is used when you want to keep looping until a specific logical condition is satisfied (contrast this with the for loop which will always iterate through an entire sequence).\nThe basic structure of the while loop is:\n\nwhile (logical_condition) {\n  expression\n}\n\nA simple example of a while loop is:\n\ni &lt;- 0\nwhile (i &lt;= 4) {\n  i &lt;- i + 1\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere the loop will only continue to pass values to the main body of the loop (the expression body) when i is less than or equal to 4 (specified using the &lt;= operator in this example). Once i is greater than 4 the loop will stop.\nThere is another, very rarely used type of loop; the repeat loop. The repeat loop has no conditional check so can keep iterating indefinitely (meaning a break, or ‚Äústop here‚Äù, has to be coded into it). It‚Äôs worthwhile being aware of it‚Äôs existence, but for now we don‚Äôt think you need to worry about it; the for and while loops will see you through the vast majority of your looping needs.\n\n5.5.3 When to use a loop?\nLoops are fairly commonly used, though sometimes a little overused in our opinion. Equivalent tasks can be performed with functions, which are often more efficient than loops. Though this raises the question when should you use a loop?\nIn general loops are implemented inefficiently in R and should be avoided when better alternatives exist, especially when you‚Äôre working with large datasets. However, loop are sometimes the only way to achieve the result we want.\nSome examples of when using loops can be appropriate:\n\nSome simulations (e.g. the Ricker model can, in part, be built using loops)\nRecursive relationships (a relationship which depends on the value of the previous relationship [‚Äúto understand recursion, you must understand recursion‚Äù])\nMore complex problems (e.g., how long since the last badger was seen at site \\(j\\), given a pine marten was seen at time \\(t\\), at the same location \\(j\\) as the badger, where the pine marten was detected in a specific 6 hour period, but exclude badgers seen 30 minutes before the pine marten arrival, repeated for all pine marten detections)\nWhile loops (keep jumping until you‚Äôve reached the moon)\n\n5.5.4 If not loops, then what?\nIn short, use the apply family of functions; apply(), lapply(), tapply(), sapply(), vapply(), and mapply(). The apply functions can often do the tasks of most ‚Äúhome-brewed‚Äù loops, sometimes faster (though that won‚Äôt really be an issue for most people) but more importantly with a much lower risk of error. A strategy to have in the back of your mind which may be useful is; for every loop you make, try to remake it using an apply function (often lapply or sapply will work). If you can, use the apply version. There‚Äôs nothing worse than realizing there was a small, tiny, seemingly meaningless mistake in a loop which weeks, months or years down the line has propagated into a huge mess. We strongly recommend trying to use the apply functions whenever possible.\nlapply\nYour go to apply function will often be lapply() at least in the beginning. The way that lapply() works, and the reason it is often a good alternative to for loops, is that it will go through each element in a list and perform a task (i.e. run a function). It has the added benefit that it will output the results as a list - something you‚Äôd have to otherwise code yourself into a loop.\nAn lapply() has the following structure:\nlapply(X, FUN)\nHere X is the vector which we want to do something to. FUN stands for how much fun this is (just kidding!). It‚Äôs also short for ‚Äúfunction‚Äù.\nLet‚Äôs start with a simple demonstration first. Let‚Äôs use the lapply() function create a sequence from 1 to 5 and add 1 to each observation (just like we did when we used a for loop):\n\nlapply(0:4, function(a) {\n  a + 1\n})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nNotice that we need to specify our sequence as 0:4 to get the output 1 ,2 ,3 ,4 , 5 as we are adding 1 to each element of the sequence. See what happens if you use 1:5 instead.\nEquivalently, we could have defined the function first and then used the function in lapply()\n\nadd_fun &lt;- function(a) {\n  a + 1\n}\nlapply(0:4, add_fun)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nThe sapply() function does the same thing as lapply() but instead of storing the results as a list, it stores them as a vector.\n\nsapply(0:4, function(a) {\n  a + 1\n})\n\n[1] 1 2 3 4 5\n\n\nAs you can see, in both cases, we get exactly the same results as when we used the for loop.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06-quarto.html",
    "href": "06-quarto.html",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "",
    "text": "6.1 What is R markdown / Quarto?",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#what-is-r-markdown-quarto",
    "href": "06-quarto.html#what-is-r-markdown-quarto",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "",
    "text": "6.1.1 R Markdown\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).\nTechnically, R markdown is a combination of three languages, R, Markdown and YAML (yet another markup language). Both Markdown and YAML are a type of ‚Äòmarkup‚Äô language. A markup language simply provides a way of creating an easy to read plain text file which can incorporate formatted text, images, headers and links to other documents. If you‚Äôre interested you can find more information about markup languages here. Actually, you are exposed to a markup language on a daily basis, as most of the internet content you digest every day is underpinned by a markup language called HTML (Hypertext Markup Language). Anyway, the main point is that R markdown is very easy to learn (much, much easier than HTML) and when used with a good IDE (RStudio or VS Code) it‚Äôs ridiculously easy to integrate into your workflow to produce feature rich content (so why wouldn‚Äôt you?!).\n\n6.1.2 Quarto?\nQuarto is a multi-language, next generation version of R Markdown from Posit, with many new features and capabilities and is compatible not only with R but also with other language like Python and Julia. Like R Markdown, Quarto uses knitr üì¶ package to execute R code, and is therefore able to render most existing .Rmd files without modification. However, it also comes with a plethora of new functionalities. More importantly, it makes it much easier to create different type of output since the coding is homogenize for specific format without having to rely on different r packages each with there own specificity (e.g bookdown, hugodown, blogdown, thesisdown, rticles, xaringan, ‚Ä¶).\nIn the rest of this chapter, we will talk about Quarto but a lot can be done with R markdown. Quarto uses .qmd files while R markdown works with .Rmd but Quarto can render .Rmd files too.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#why-use-quarto",
    "href": "06-quarto.html#why-use-quarto",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "\n6.2 Why use Quarto?",
    "text": "6.2 Why use Quarto?\nDuring the previous Chapters we talked a lot about conducting your research in a robust and reproducible manner to facilitate open science. In a nutshell, open science is about doing all we can to make our data, methods, results and inferences transparent and available to everyone. Some of the main tenets of open science are described here and include:\n\nTransparency in experimental methodology, observation, collection of data and analytical methods.\nPublic availability and re-usability of scientific data\nPublic accessibility and transparency of scientific communication\nUsing web-based tools to facilitate scientific collaboration\n\nBy now all of you will (hopefully) be using R to explore and analyse your interesting data. As such, you‚Äôre already well along the road to making your analysis more reproducible, transparent and shareable. However, perhaps your current workflow looks something like this:\n\n\n\n\n\n\n\nFigure¬†6.1: Non-reproducible workflow\n\n\n\n\nYour data is imported from your favourite spreadsheet software into R, you write your R code to explore and analyse your data, you save plots as external files, copy tables of analysis output and then manually combine all of this and your written prose into a single MS Word document (maybe for a paper or thesis chapter). Whilst there is nothing particularly wrong with this approach (and it‚Äôs certainly better than using point and click software to analyse your data) there are some limitations:\n\nIt‚Äôs not particularly reproducible. Because this workflow separates your R code from the final document there are multiple opportunities for undocumented decisions to be made (which plots did you use? what analysis did/didn‚Äôt you include? etc).\nIt‚Äôs inefficient. If you need to go back and change something (create a new plot or update your analysis etc) you will need to create or amend multiple documents increasing the risk of mistakes creeping into your workflow.\nIt‚Äôs difficult to maintain. If your analysis changes you again need to update multiple files and documents.\nIt can be difficult to decide what to share with others. Do you share all of your code (initial data exploration, model validation etc) or just the code specific to your final document? It‚Äôs quite a common (and bad!) practice for researchers to maintain two R scripts, one used for the actual analysis and one to share with the final paper or thesis chapter. This can be both time consuming and confusing and should be avoided.\n\nPerhaps a more efficient and robust workflow would look something like this:\n\n\n\n\n\n\n\nFigure¬†6.2: A-reproducible (and more fficient) workflow\n\n\n\n\nYour data is imported into R as before but this time all of the R code you used to analyse your data, produce your plots and your written text (Introduction, Materials and Methods, Discussion etc) is contained within a single Quarto document which is then used (along with your data) to automatically create your final document. This is exactly what Quarto allows you to do.\nSome of the advantages of using Quarto include:\n\nExplicitly links your data with your R code and output creating a fully reproducible workflow. ALL of the R code used to explore, summarise and analyse your data can be included in a single easy to read document. You can decide what to include in your final document (as you will learn below) but all of your R code can be included in the Quarto document.\nYou can create a wide variety of output formats (pdf, html web pages, MS Word and many others) from a single Quarto document which enhances both collaboration and communication.\nEnhances transparency of your research. Your data and Quarto file can be included with your publication or thesis chapter as supplementary material or hosted on a GitHub repository (see Chapter 7).\nIncreases the efficiency of your workflow. If you need to modify or extend your current analysis you just need to update your Quarto document and these changes will automatically be included in your final document.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#get-started-with-quarto",
    "href": "06-quarto.html#get-started-with-quarto",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "\n6.3 Get started with Quarto",
    "text": "6.3 Get started with Quarto\nQuarto integrates really well with R Studio and VS Code, and provide both a source editor as well as a visual editor providing an experience close to your classic WYSIWYG (what you see is what you write) writing software (e.g.¬†Microsoft Word or LibreOffice writer)\n\n6.3.1 Installation\nTo use Quarto you will first need to install the Quarto software and the quarto üì¶ package (with its dependencies). You can find instructions on how to do this in Section 1.1.1 and on the Quarto website. If you would like to create pdf documents (or MS Word documents) from your Quarto file you will also need to install a version of \\(\\LaTeX\\) on your computer. If you‚Äôve not installed \\(\\LaTeX\\) before, we recommend that you install TinyTeX. Again, instructions on how to do this can be found at Section 1.1.1.\n\n6.3.2 Create a Quarto document, .qmd\n\nRight, time to create your first Quarto document. Within RStudio, click on the menu File -&gt; New File -&gt; Quarto.... In the pop up window, give the document a ‚ÄòTitle‚Äô and enter the ‚ÄòAuthor‚Äô information (your name) and select HTML as the default output. We can change all of this later so don‚Äôt worry about it for the moment.\n\n\n\n\n\n\n\nFigure¬†6.3: Creating a Quarto document\n\n\n\n\nYou will notice that when your new Quarto document is created it includes some example Quarto code. Normally you would just highlight and delete everything in the document except the information at the top between the --- delimiters (this is called the YAML header which we will discuss in a bit) and then start writing your own code. However, just for now we will use this document to practice converting Quarto to both html and pdf formats and check everything is working.\n\n\n\n\n\n\n\nFigure¬†6.4: A new Quarto document\n\n\n\n\nOnce you‚Äôve created your Quarto document it‚Äôs good practice to save this file somewhere convenient (Section 1.4 and Figure¬†1.11). You can do this by selecting File -&gt; Save from RStudio menu (or use the keyboard shortcut ctrl + s on Windows or cmd + s on a Mac) and enter an appropriate file name (maybe call it my_first_quarto). Notice the file extension of your new Quarto file is .qmd.\nNow, to convert your .qmd file to a HTML document click on the little black triangle next to the Knit icon at the top of the source window and select knit to HTML\n\n\n\n\n\n\n\nFigure¬†6.5: Knitting a Qmd file\n\n\n\n\nRStudio will now ‚Äòknit‚Äô (or render) your .qmd file into a HTML file. Notice that there is a new Quarto tab in your console window which provides you with information on the rendering process and will also display any errors if something goes wrong.\nIf everything went smoothly a new HTML file will have been created and saved in the same directory as your .qmd file (ours will be called my_first_quarto.html). To view this document simply double click on the file to open in a browser (like Chrome or Firefox) to display the rendered content. RStudio will also display a preview of the rendered file in a new window for you to check out (your window might look slightly different if you‚Äôre using a Windows computer).\n\n\n\n\n\n\n\nFigure¬†6.6: A my first rendered html\n\n\n\n\nGreat, you‚Äôve just rendered your first Quarto document. If you want to knit your .qmd file to a pdf document then all you need to do is choose knit to PDF instead of knit to HTML when you click on the knit icon. This will create a file called my_first_quarto.pdf which you can double click to open. Give it a go!\nYou can also knit an .qmd file using the command line in the console rather than by clicking on the knit icon. To do this, just use the quarto_render() function from the quarto üì¶ package as shown below. Again, you can change the output format using the output_format = argument as well as many other options.\nlibrary(quarto)\n\nquarto_render('my_first_quarto.qmd', output_format = 'html_document')\n\n# alternatively if you don't want to load the quarto package\n\nquarto::quarto_render('my_first_quarto.Rmd', output_format = 'html_document')",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#quarto-document-.qmd-anatomy",
    "href": "06-quarto.html#quarto-document-.qmd-anatomy",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "\n6.4 Quarto document (.qmd) anatomy",
    "text": "6.4 Quarto document (.qmd) anatomy\nOK, now that you can render a Quarto file in RStudio into both HTML and pdf formats let‚Äôs take a closer look at the different components of a typical Quarto document. Normally each Quarto document is composed of 3 main components:\n\na YAML header\nformatted text\ncode chunks.\n\n\n\n\n\n\n\n\nFigure¬†6.7: Structure of a qmd file\n\n\n\n\n\n6.4.1 YAML header\nYAML stands for ‚ÄòYAML Ain‚Äôt Markup Language‚Äô (it‚Äôs an ‚Äòin‚Äô joke!) and this optional component contains the metadata and options for the entire document such as the author name, date, output format, etc. The YAML header is surrounded before and after by a --- on its own line. In RStudio a minimal YAML header is automatically created for you when you create a new Quarto document as we did above (Section 6.3.2) but you can change this any time. A simple YAML header may look something like this:\n---\ntitle: My first Quarto document\nauthor: Jane Doe\ndate: March 01, 2020\nformat: html\n---\nIn the YAML header above the output format is set to HTML. If you would like to change the output to pdf format then you can change it from format: html to format: pdf (you can also set more than one output format if you like). You can also change the default font and font size for the whole document and even include fancy options such as a table of contents and inline references and a bibliography. If you want to explore the plethora of other options see here. Just a note of caution, many of the options you can specify in the YAML header will work with both HTML and pdf formatted documents, but not all. If you need multiple output formats for your Quarto document check whether your YAML options are compatible between these formats. Also, indentation in the YAML header has a meaning, so be careful when aligning text. For example, if you want to include a table of contents you would modify the output: field in the YAML header as follows\n---\ntitle: My first Quarto document\nauthor: Bob Hette\ndate: March 01, 2020\nformat:\n  html:\n    toc: true\n---\n\n6.4.2 Formatted text\nAs mentioned above, one of the great things about Quarto is that you don‚Äôt need to rely on your word processor to bring your R code, analysis and writing together. Quarto is able to render (almost) all of the text formatting that you are likely to need such as italics, bold, strike-through, super and subscript as well as bulleted and numbered lists, headers and footers, images, links to other documents or web pages and also equations. However, in contrast to your familiar What-You-See-Is-What-You-Get (WYSIWYG) word processing software you don‚Äôt see the final formatted text in your Quarto document (as you would in MS Word), rather you need to ‚Äòmarkup‚Äô the formatting in your text ready to be rendered in your output document. At first, this might seem like a right pain in the proverbial but it‚Äôs actually very easy to do and also has many advantages (do you find yourself spending more time on making your text look pretty in MS Word rather than writing good content?!).\nHere is an example of marking up text formatting in an Quarto document\n#### Tadpole sediment experiment\n\nThese data were obtained from a mesocosm experiment which aimed to examine the\neffect of bullfrog tadpoles (*Lithobates catesbeianus*) biomass on sediment\nnutrient (NH~4~, NO~3~ and PO~3~) release.\nAt the start of the experiment 15 replicate mesocosms were filled with\n20 cm^2^ of **homogenised** marine sediment and assigned to one of five \ntadpole biomass treatments.\nwhich would look like this in the final rendered document (can you spot the markups?)\n\nTadpole sediment experiment\n\n\nThese data were obtained from a mesocosm experiment which aimed to examine the effect of bullfrog tadpoles (Lithobates catesbeianus) biomass on sediment nutrient (NH4, NO3 and PO3) release. At the start of the experiment 15 replicate mesocosms were filled with 20 cm2 of homogenised marine sediment and assigned to one of five tadpole biomass treatments.\n\nEmphasis\nSome of the most common markdown syntax for providing emphasis and formatting text is given below.\n\n\nGoal\nQuarto\noutput\n\n\n\nbold text\n**mytext**\nmytext\n\n\nitalic text\n*mytext*\nmytext\n\n\nstrikethrough\n~~mytext~~\nmytext\n\n\nsuperscript\nmytext^2^\nmytext2\n\n\n\nsubscript\nmytext~2~\nmytext2\n\n\n\n\nInterestingly there is no underline in R markdown syntax by default, for more or less esoteric reasons (e.g. an underline is considered a stylistic element (there may well be other reasons)). Quarto fixed that problem, you can simply do [text to underline]{.underline} to underline your text.\nWhite space and line breaks\nOne of the things that can be confusing for new users of markdown is the use of spaces and carriage returns (the enter key on your keyboard). In markdown, multiple spaces within the text are generally ignored as are carriage returns. For example this markdown text\nThese      data were      obtained from a\nmesocosm experiment which    aimed to examine the\neffect\nof          bullfrog tadpoles (*Lithobates catesbeianus*) biomass.\nwill be rendered as\n\nThese data were obtained from a mesocosm experiment which aimed to examine the effect of bullfrog tadpoles (Lithobates catesbeianus) biomass.\n\nThis is generally a good thing (no more random multiple spaces in your text). If you want your text to start on a new line then you can simply add two blank spaces at the end of the preceding line\n\nThese data were obtained from a\nmesocosm experiment which aimed to examine the\neffect bullfrog tadpoles (Lithobates catesbeianus) biomass.\n\nIf you really want multiple spaces within your text then you can use the Non breaking space tag &nbsp;\nThese &nbsp; &nbsp; &nbsp; data were &nbsp; &nbsp; &nbsp; &nbsp; obtained from a  \nmesocosm experiment which &nbsp; &nbsp; aimed to examine the    \neffect &nbsp; &nbsp; &nbsp; &nbsp; bullfrog tadpoles (*Lithobates catesbeianus*) biomass.\n\nThese ¬† ¬† ¬† data were ¬† ¬† ¬† ¬† obtained from a\nmesocosm experiment which ¬† ¬† aimed to examine the\neffect ¬† ¬† ¬† ¬† bullfrog tadpoles (Lithobates catesbeianus) biomass.\n\nHeadings\nYou can add headings and subheadings to your Quarto document by using the # symbol at the beginning of the line. You can decrease the size of the headings by simply adding more # symbols. For example\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n###### Header 6\nresults in headings in decreasing size order\n\nHeader 1\nHeader 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6\n\nComments\nAs you can see above the meaning of the # symbol is different when formatting text in an Quarto document compared to a standard R script (which is used to included a comment - remember?!). You can, however, use a # symbol to comment code inside a code chunk (Section 6.4.3) as usual (more about this in a bit). If you want to include a comment in your Quarto document outside a code chunk which won‚Äôt be included in the final rendered document then enclose your comment between &lt;!-- and --&gt;.\n&lt;!--\nthis is an example of how to format a comment using Quarto.\n--&gt;\nLists\nIf you want to create a bullet point list of text you can format an unordered list with sub items. Notice that the sub-items need to be indented.\n- item 1\n- item 2\n   + sub-item 2\n   + sub-item 3\n- item 3\n- item 4\n\n\nitem 1\nitem 2\n\nsub-item 2\nsub-item 3\n\n\nitem 3\nitem 4\n\n\nIf you need an ordered list\n1. item 1\n1. item 2\n    + sub-item 2\n    + sub-item 3\n1. item 3\n1. item 4\n\n\nitem 1\nitem 2\n\nsub-item 2\nsub-item 3\n\n\nitem 3\nitem 4\n\n\nLinks\nIn addition to images you can also include links to webpages or other links in your document. Use the following syntax to create a clickable link to an existing webpage. The link text goes between the square brackets and the URL for the webpage between the round brackets immediately after.\nYou can include a text for your clickable [link](https://www.worldwildlife.org)\nwhich gives you:\n\nYou can include a text for your clickable link\n\n\n6.4.3 Code chunks\nNow to the heart of the matter. To include R code into your Quarto document you simply place your code into a ‚Äòcode chunk‚Äô. All code chunks start and end with three backticks ```. Note, these are also known as ‚Äògrave accents‚Äô or ‚Äòback quotes‚Äô and are not the same as an apostrophe! On most keyboards you can find the backtick on the same key as tilde (~).\n```{r}\nAny valid R code goes here\n```\nYou can insert a code chunk by either typing the chunk delimiters ```{r} and ``` manually or use your IDE option (RStudio toolbar (the Insert button) or by clicking on the menu Code -&gt; Insert Chunk. In VS Code you can use code snippets) Perhaps an even better way is to get familiar with the keyboard shortcuts for you IDE or code snippets.\nThere are a many things you can do with code chunks: you can produce text output from your analysis, create tables and figures and insert images amongst other things. Within the code chunk you can place rules and arguments between the curly brackets {} that give you control over how your code is interpreted and output is rendered. These are known as chunk options. The only mandatory chunk option is the first argument which specifies which language you‚Äôre using (r in our case but other languages are supported). Note, chunk options can be written in two ways:\n\neither all of your chunk options must be written between the curly brackets on one line with no line breaks\nor they can be written using a YAML notation within the code chunk using #| notation at the beginning of the line.\n\nWe are using the YAML notation for code chunk options since we find it much easier to read when you have multiple options of long captions.\nYou can also specify an optional code chunk name (or label) which can be useful when trying to debug problems and when performing advanced document rendering. In the following block, we name the code chunk summary-stats, load the package ggplot2 üì¶, create a dataframe (dataf) with two variables x and y, use the summary() function to display some summary statistics and plot a scatterplot of the data with ggplot(). When we run the code chunk both the R code and the resulting output are displayed in the final document.\n```{r, summary-stats, echo = TRUE, fig.cap = \"Caption for a simple figure but making the chunk options long and hard to read\"}\nlibrary(ggplot)\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\nggplot(dataf, aes(x = x, y = y)) + geom_point()\n```\n```{r}\n#| label: summary-stats\n#| echo: true\n#| fig-cap = \"Caption for a simple figure but making the chunk options long and hard to read\"\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\nggplot(dataf, aes(x = x, y = y)) + geom_point()\n```\nBoth will output\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 3.25   1st Qu.: 3.25  \n Median : 5.50   Median : 5.50  \n Mean   : 5.50   Mean   : 5.50  \n 3rd Qu.: 7.75   3rd Qu.: 7.75  \n Max.   :10.00   Max.   :10.00  \n\nggplot(dataf, aes(x = x, y = y)) + geom_point()\n\n\n\n\n\n\nFigure¬†6.8: Caption for a simple figure but making the chunk options long and hard to read\n\n\n\n\nWhen using chunk names make sure that you don‚Äôt have duplicate chunk names in your Quarto document and avoid spaces and full stops as this will cause problems when you come to knit your document (We use a - to separate words in our chunk names).\nIf we wanted to only display the output of our R code (just the summary statistics for example) and not the code itself in our final document we can use the chunk option echo=FALSE\n```{r}\n#| label: summary-stats2\n#| echo: false\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\n\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 3.25   1st Qu.: 3.25  \n Median : 5.50   Median : 5.50  \n Mean   : 5.50   Mean   : 5.50  \n 3rd Qu.: 7.75   3rd Qu.: 7.75  \n Max.   :10.00   Max.   :10.00  \n\n\nTo display the R code but not the output use the results='hide' chunk option.\n```{r}\n#| label: summary-stats\n#| results: 'hide'\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n\nSometimes you may want to execute a code chunk without showing any output at all. You can suppress the entire output using the chunk option include: false.\n```{r}\n#| label: summary-stats4\n#| include: false\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\nThere are a large number of chunk options documented here with a more condensed version here. Perhaps the most commonly used are summarised below with the default values shown.\n\n\n\n\n\n\n\nChunk option\ndefault value\nFunction\n\n\n\necho\necho: true\nIf false, will not display the code in the final document\n\n\nresults\nresults: 'markup'\nIf ‚Äòhide‚Äô, will not display the code‚Äôs results in the final document.\n\n\n\nIf ‚Äòhold‚Äô, will delay displaying all output pieces until the end of the chunk. If ‚Äòasis‚Äô, will pass through results without reformatting them. | | include | include: true | If false, will run the chunk but not include the chunk in the final document. | | eval | eval: true | If false, will not run the code in the code chunk. | | message | message: true | If false, will not display any messages generated by the code. | | warning | warning: true | If false, will not display any warning messages generated by the code. |\n\n6.4.4 Inline R code\nUp till now we‚Äôve been writing and executing our R code in code chunks. Another great reason to use Quarto is that we can also include our R code directly within our text. This is known as ‚Äòinline code‚Äô. To include your code in your Quarto text you simply write `r write your code here`. This can come in really useful when you want to include summary statistics within your text. For example, we could describe the iris dataset as follows:\nMorphological characteristics (variable names: \n`r names(iris)[1:4]`) were measured from \n`r nrow(iris)` *Iris sp.* plants from \n`r length(levels(iris$Species))` different species.\nThe mean Sepal length was\n`r round(mean(iris$Sepal.Length), digits = 2)` mm.\n  \nwhich will be rendered as\n\nMorphological characteristics (variable names: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) were measured from 150 iris plants from 3 different species. The mean Sepal length was 5.84 mm.\n\nThe great thing about including inline R code in your text is that these values will automatically be updated if your data changes.\n\n6.4.5 Images and photos\nA useful feature is the ability to embed images and links to web pages (or other documents) into your Quarto document. You can include images into your Quarto document in a number of different ways. Perhaps the simplest method is to use the Quarto markdown format:\n![Image caption](path/to/you/image){options}\nHere is an example with an image taking 75% of the width and centered.\n![Waiting for the eclipse](images/markdown/eclipse_ready.jpg){fig-align=\"center\" width=\"75%\"}\nresulting in:\n\n\n\n\n\nFigure¬†6.9: Waiting for the eclipse\n\n\nAn alternative way of including images in your document is to use the include_graphics() function from the knitr package. The following code will produce similar output.\n```{r}\n#| label: fig-knitr\n#| fig-align: center\n#| out-width: 75%\n#| fig-cap: Waiting for the eclipse\nknitr::include_graphics(\"images/markdown/eclipse_ready.jpg\")\n```\nThe code above will only work if the image file (eclipse_ready.jpg) is in the right place relative to where you saved your .qmd file. In the example the image file is in a sub directory (folder) called images/markdown in the directory where we saved our my_first_quarto.qmd file. You can embed images saved in many different file types but perhaps the most common are .jpg and .png.\n\n6.4.6 Figures\nBy default, figures produced by R code will be placed immediately after the code chunk they were generated from. For example:\n\n```{r}\n#| label: fig-simple-plot\n#| fig-cap: A simple plot\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure¬†6.10: A simple plot\n\n\n\n\nThe fig-cap: chunk option allow to provide a figure caption recognized by Quarto and using in figure numbering and cross referencing (Section 6.4.8).\nIf you want to change the plot dimensions in the final document you can use the fig-width: and fig-height: chunk options (in inches!). You can also change the alignment of the figure using the fig-align: chunk option.\n\n```{r}\n#| label: fig-simple-plot2\n#| fig-cap: A shrinked figure\n#| fig-width: 4\n#| fig-height: 3\n#| fig-align: center\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure¬†6.11: A shrinked figure\n\n\n\n\nYou can add a figure caption using the fig-cap: option.\n\n```{r}\n#| label: fig-simple-plot-cap\n#| class-source: fold-show\n#| fig-cap: A simple plot\n#| fig-align: center\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure¬†6.12: A simple plot\n\n\n\n\nIf you want to suppress the figure in the final document use the fig-show: 'hide' option.\n\n```{r}\n#| label: fig-simple-plot5\n#| fig-show: hide\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\nIf you‚Äôre using a package like ggplot2 üì¶ to create your plots then don‚Äôt forget you will need to make the package available with the library() function in the code chunk (or in a preceding code chunk).\n\n```{r}\n#| label: fig-simple-ggplot\n#| fig-cap: A simple ggplot\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nlibrary(ggplot2)\nggplot(dataf, aes(x = x, y = y)) +\n  geom_point()\n```\n\n\n\n\n\n\nFigure¬†6.13: A simple ggplot\n\n\n\n\nAgain, there are a large number of chunk options specific to producing plots and figures. See here for more details.\n\n6.4.7 Tables\nIn Quarto, you can create tables using native markdown syntax (this doesn‚Äôt need to be in a code chunk).\n|  x  |  y  |\n|:---:|:---:|\n|  1  |  5  | \n|  2  |  4  |\n|  3  |  3  |\n|  4  |  2  |\n|  5  |  1  |\n\n: Caption for a simple markdown table\n\n\nTable¬†6.1: Caption for a simple markdown table\n\n\n\nx\ny\n\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n2\n\n\n5\n1\n\n\n\n\n\n\nThe :-------: lets markdown know that the line above should be treated as a header and the lines below as the body of the table. Alignment within the table is set by the position of the :. To center align use :------:, to left align :------ and right align ------:. Whilst it can be fun(!) to create tables with raw markup it‚Äôs only practical for very small and simple tables.\nThe easiest way we know to include tables in an Quarto document is by using the kable() function from the knitr üì¶ package. The kable() function can create tables for HTML, PDF and Word outputs.\nTo create a table of the first 2 rows per species of the iris data frame using the kable() function simply write\nlibrary(knitr)\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\nkable()\nor without loading knitr üì¶ but indicating where to find the kable() function.\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  knitr::kable()\n\n\n\nTable¬†6.2: A simple kable table\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n\n\n\n\n\n\nThe kable() function offers plenty of options to change the formatting of the table. For example, if we want to round numeric values to one decimal place use the digits = argument. To center justify the table contents use align = 'c' and to provide custom column headings use the col.names = argument. See ?knitr::kable for more information.\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  knitr::kable(\n    digits=0,\n    align = 'c',\n    col.names = c(\n      'Sepal length', 'Sepal width',\n      'Petal length', 'Petal width', 'Species'\n    )\n)\n\n\n\nTable¬†6.3: A nicer kable table\n\n\n\n\nSepal length\nSepal width\nPetal length\nPetal width\nSpecies\n\n\n\n5\n4\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n7\n3\n5\n1\nversicolor\n\n\n6\n3\n4\n2\nversicolor\n\n\n6\n3\n6\n2\nvirginica\n\n\n6\n3\n5\n2\nvirginica\n\n\n\n\n\n\n\n\nYou can further enhance the look of your kable tables using the kableExtra üì¶ package (don‚Äôt forget to install the package first!). See here for more details and a helpful tutorial.\nIf you want even more control and customisation options for your tables take a look at the gt üì¶ [package][gt]. gt is an acronym for grammar of tables and is based on similar principle for tables that are used for plots in ggplot.\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  rename_with(~ gsub(\"([._])\", \" \", .x)) %&gt;%\n  gt()\n\n\nTable¬†6.4: A nice gt table\n\n\n\n\n\n\nSepal Length\nSepal Width\nPetal Length\nPetal Width\n\n\n\nsetosa\n\n\n5.1\n3.5\n1.4\n0.2\n\n\n4.9\n3.0\n1.4\n0.2\n\n\nversicolor\n\n\n7.0\n3.2\n4.7\n1.4\n\n\n6.4\n3.2\n4.5\n1.5\n\n\nvirginica\n\n\n6.3\n3.3\n6.0\n2.5\n\n\n5.8\n2.7\n5.1\n1.9\n\n\n\n\n\n\n\n\n\nWithin most R packages developped to produce tables, there are options to include table captions. However, if you want to add a table caption we recommend to do using the code chunk option in Quarto tbl-cap: since it will allow for cross-referencing (Section 6.4.8) and better integration in the document.\n```{r}\n#| label: tbl-gt-table\n#| tbl-cap: A nice gt table\n#| echo: true\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n=2) %&gt;%\n  rename_with(~gsub(\"([._])\", \" \", .x)) %&gt;%\n  gt()\n```\n\n6.4.8 Cross-referencing\nCross-references make it easier for readers to navigate your document by providing numbered references and hyperlinks to various entities like figures and tables. Once set up, tables and figures numbering happens automatically, so you don‚Äôt need to re-number all the figures when you add or delete one.\nEvery cross-referenceable entity requires a label (a unique identifier) prefixed with a cross-reference type e.g.¬†#fig-element\nFor more details see the cross-referencing section on Quarto website.\n\n6.4.8.1 Document sections\nYou can make cross-references to other sections of the document. To do so you need to:\n\nset up a identifier for the section you want to link to. The identifier should:\n\nstart with #sec-\n\nbe in lower case (Figure 6.3)\ndoe not have any space, using - instead\n\n\nuse the @ symbol and the identifier to refer to the section\n\n## Cross-referencing sections {#sec-cross-ref-sections}\n\n[...]\n\nAs seen before(@sec-cross-ref-sections)\n\n6.4.8.2 Images, figures and tables\nFor tables, images and figures, in addition to the identifier the element also needs a caption for cross-referencing to work.\nThe prefix for tables is #tbl- and #fig- for images and figures.\nHere is an example for an image included with markdown:\n![Rocking the eclipse](images/markdown/eclipse_ready.jpg){#fig-cute-dog}\n\nSee @fig-cute-dog for an illustration.\n\n\n\n\n\nFigure¬†6.14: Rocking the eclipse\n\n\nSee Figure¬†6.14 for an illustration.\nFor figures and tables produced with R code chunks, simply provide the identifier in the label chunk option and the caption also as a chunk option.\nHere is the code for a figure and a table.\n\n```{r}\n#| label: fig-cr-plot\n#| fig-cap: A nice figure\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nlibrary(ggplot2)\nggplot(dataf, aes(x = x, y = y)) +\n  geom_point()\n```\n\n\n\n\n\n\nFigure¬†6.15: A nice figure\n\n\n\n\n\n```{r}\n#| label: tbl-cr-table\n#| tbl-cap: A nice table\n#| warning: false\nlibrary(knitr)\nkable(iris[1:5,], digits=0, align = 'c', col.names = c('sepal length', 'sepal width', 'petal length', 'petal width', 'species'))\n```\n\n\nTable¬†6.5: A nice table\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\nspecies\n\n\n\n5\n4\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n5\n3\n2\n0\nsetosa\n\n\n5\n4\n1\n0\nsetosa\n\n\n\n\n\n\n\n\nUsing cross-references, we can write:\nAs seen on @fig-cr-plot and @tbl-cr-table ‚Ä¶\nTo get:\nAs seen on Figure¬†6.15 and Table¬†6.5 ‚Ä¶\n\n6.4.9 Citations and bibliography\nTo generate citations and a bibliography, Quarto requires:\n\na properly formatted .qmd document\na bibliographic source file including all the information for the citations. It works with awide variatey of format but we suggest using BibTEX format.\n(optional) a CSL file which specifies the formatting to use when generating the citations and bibliography.\n\nThe bibliographic source and the (optional) csl file are specified in the yaml header as :\n---\ntitle: \"My Document\"\nbibliography: references.bib\ncsl: ecology.csl\n---\n\n6.4.9.1 Citations\nQuarto uses the standard Pandoc markdown representation for citations (e.g.¬†[@citation]) ‚Äî citations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of ‚Äò@‚Äô + the citation identifier from the database, and may optionally have a prefix, a locator, and a suffix. The citation key must begin with a letter, digit, or , and may contain alphanumerics, , and internal punctuation characters.\n\n\n\n\n\n\nMarkdown Format\nOutput (default)\n\n\n\nUnicorns are the best [see @martin1219, pp.¬†33-35; also @martin2200, chap.¬†1]\nUnicorns are the best (see Martin 1219 pp. 33‚Äì35, also Martin 2200 chap. 1)\n\n\n\nUnicorns are the best [@martin2200; @martin1219]\nUnicorns are the best (Martin 1219, 2200)\n\n\n\nMartin says unicorns are the best [-@martin2200]\nMartin says unicorns are the best (2200)\n\n\n\n\n@martin1219 says unicorns are the best.\n\nMartin (1219) says unicorns are the best.\n\n\n\n@martin1219 [p.¬†33] says unicorns are the best.\n\nMartin (1219 p. 33) says unicorns are the best.\n\n\n\n6.4.9.2 Create the bibliography\nBy default, the list of works cited will automatically be generated and placed at the end of document if the style calls for it. It will be placed in a div with the id refs if one exists like\n### Bibliography\n\n::: {#refs}\n:::\nFor more details see the Citation page on Quarto website.\n\n6.4.9.3 Integration with Zotero\nQuarto integrates really well with Zotero if you are using the visual editor in either RStudio or VS Code.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#sec-tips-tricks",
    "href": "06-quarto.html#sec-tips-tricks",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "\n6.5 Some tips and tricks",
    "text": "6.5 Some tips and tricks\nProblem :\nWhen rendering my Quarto document to pdf my code runs off the edge of the page.\nSolution:\nAdd a global_options argument at the start of your .qmd file in a code chunk:\n```{r}\n#| label: global_options\n#| include: false \nknitr::opts_chunk$set(message=FALSE, tidy.opts=list(width.cutoff=60), tidy=TRUE) \n```\nThis code chunk won‚Äôt be displayed in the final document due to the include: false argument and you should place the code chunk immediately after the YAML header to affect everything below that.\ntidy.opts = list(width.cutoff = 60), tidy=TRUE defines the margin cutoff point and wraps text to the next line. Play around with this value to get it right (60-80 should be OK for most documents).\nWith quarto you can also put the global knitr options in a knitrblock in the YAML header (see Quarto website for details).\n---\ntitle: \"My Document\"\nformat: html\nknitr:\n  opts_chunk: \n    message: false\n    tidy.opts: !expr 'list(width.cutoff=60)'\n    tidy: true \n---\nProblem:\nWhen I load a package in my Quarto document my rendered output contains all of the startup messages and/or warnings.\nSolution:\nYou can load all of your packages at the start of your Quarto document in a code chunk along with setting your global options.\n```{r}\n#| label: global_options\n#| include: false\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning=FALSE,\n  tidy.opts=list(width.cutoff=60)\n) \nsuppressPackageStartupMessages(library(ggplot2))\n```\nThe message = FALSE and warning = FALSE arguments suppress messages and warnings. The suppressPackageStartupMessages(library(ggplot2)) will load the ggplot2 üì¶ package but suppress startup messages.\nProblem:\nWhen rendering my Quarto document to pdf my tables and/or figures are split over two pages.\nSolution:\nAdd a page break using the \\(\\LaTeX\\) \\pagebreak notation before your offending table or figure\nProblem:\nThe code in my rendered document looks ugly!\nSolution:\nAdd the argument tidy: true to your global arguments. Sometimes, however, this can cause problems especially with correct code indentation. The best solution is to write code that looks nice (insert space and use multiple lines)",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#further-information",
    "href": "06-quarto.html#further-information",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "\n6.6 Further Information",
    "text": "6.6 Further Information\nAlthough we‚Äôve covered more than enough to get you quite far using Quarto, as with most things R related, we‚Äôve really only had time to scratch the surface. Happily, there‚Äôs a wealth of information available to you should you need to expand your knowledge and experience. A good place to start is the excellent quarto website here.\nAnother useful and concise Quarto reference guide can be found here\nA quick and easy R Markdown cheatsheet",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#practical",
    "href": "06-quarto.html#practical",
    "title": "6¬† Reproducible reports with Quarto",
    "section": "\n6.7 Practical",
    "text": "6.7 Practical\nWe will create a new Rmarkdown document and edit it using basic R and Rmarkdown functions.\n\n6.7.1 Context\nWe will use the awesome palmerpenguins dataset üêß to explore and visualize data.\nThese data have been collected and shared by Dr.¬†Kristen Gorman and Palmer Station, Antarctica LTER.\nThe package was built by Drs Allison Horst and Alison Hill, check out the official website.\nThe package palmerpenguins has two datasets:\n\n\npenguins_raw has the raw data of penguins observations (see ?penguins_raw for more info)\n\npenguins is a simplified version of the raw data (see ?penguins for more info)\n\nFor this exercise, we‚Äôre gonna use the penguins dataset.\n\nlibrary(palmerpenguins)\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n6.7.2 Questions\n1) Install the package palmerpenguins.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ninstall.packages(\"palmerpenguins\")\n\n\n\n\n2)\n\nCreate a new Quarto document, name it and save it.\nDelete everything after line 12.\nAdd a new section title, simple text and text in bold font.\nCompile (‚ÄúKnit‚Äù).\n\n3)\n\nAdd a chunk in which you load the palmerpenguins. The corresponding line of code should be hidden in the output.\nLoad also the tidyverse suite of packages. Modify the defaults to suppress all messages.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n```{r}\n#| echo: false\n#| message:false\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n```\n\n\n\n4) Add another chunk in which you build a table with the 10 first rows of the dataset.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n```{r}\npenguins %&gt;%\n  slice(1:10) %&gt;%\n  knitr::kable()\n```\n\n\n\n5) In a new section, display how many individuals, penguins species and islands we have in the dataset. This info should appear directly in the text, you need to use inline code üòÑ. Calculate the mean of the (numeric) traits measured on the penguins.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n## Numerical exploration\n\nThere are `r nrow(penguins)` penguins in the dataset,\nand `r length(unique(penguins$species))` different species.\nThe data were collected in `r length(unique(penguins$island))`\nislands of the Palmer archipelago in Antarctica.\n\nThe mean of all traits that were measured on the penguins are:\n```{r}\n#| echo: false\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(across(where(is.numeric), mean, na.rm = TRUE))\n```\n\n\n\n6) In another section, entitled ‚ÄòGraphical exploration‚Äô, build a figure with 3 superimposed histograms, each one corresponding to the body mass of a species.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n## Graphical exploration\n\nA histogram of body mass per species:\n```{r}\n#| fig-cap: Distribution of body mass by species of penguins\n  ggplot(data = penguins) +\n  aes(x = body_mass_g) +\n  geom_histogram(aes(fill = species),\n                 alpha = 0.5,\n                 position = \"identity\") +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  theme_minimal() +\n  labs(x = \"Body mass (g)\",\n       y = \"Frequency\",\n       title = \"Penguin body mass\")\n```\n\n\n\n7) In another section, entitled Linear regression, fit a model of bill length as a function of body size (flipper length), body mass and sex. Obtain the output and graphically evaluate the assumptions of the model. As reminder here is how you fit a linear regression.\n```{r}\nmodel &lt;- lm(Y ~  X1 + X2, data = data)\nsummary(model)\nplot(model)\n```\n\n\n\n\n\n\nSolution\n\n\n\n\n\n## Linear regression\n\nAnd here is a nice model with graphical output\n```{r}\n#| fig-cap: \"Checking assumptions of the model\"\nm1 &lt;- lm(bill_length_mm ~  flipper_length_mm + body_mass_g + sex, data = penguins)\nsummary(m1)\npar(mfrow= c(2,2))\nplot(m1)\n```\n\n\n\n8) Add references manually or using citr in RStudio.\n\nPick a recent publication from the researcher who shared the data, Dr Kristen Gorman. Import this publication in your favorite references manager (we use Zotero, no hard feeling), and create a bibtex reference that you will add to to the file mabiblio.bib.\nAdd bibliography: mabiblio.bib at the beginning of your R Markdown document (YAML).\nCite the reference iin the text using either typing the reference manually or using citr. To use citr, instal it first; if everything goes well, you should see it in the pulldown menu Addins üí™. Then simply use Insert citations in the pull-down menu Addins.\nCompile.\n\n9) Change the default citation format (Chicago style) into the The American Naturalist format. It can be found here https://www.zotero.org/styles. To do soo, add csl: the-american-naturalist.csl in the YAML.\n10) Build your report in html, pdf and docx format. üéâ\nExample of output\nYou can see an example of the Rmarkdown source file and pdf output\n\n\n\n\nHappy coding\n\n\n\n\n\n\n\nA. C. Davison, and D. V. Hinkley. 1997. Bootstrap methods and their\napplications. Cambridge University Press, Cambridge.\n\n\nAdler, D., S. T. Kelly, T. Elliott, and J. Adamson. 2024. vioplot: Violin plot.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey,\nA. Atkins, H. Wickham, J. Cheng, W. Chang, and R. Iannone. 2024. rmarkdown: Dynamic documents for r.\n\n\nAngelo Canty, and B. D. Ripley. 2024. boot:\nBootstrap r (s-plus) functions.\n\n\nBanta, J. A., M. H. H. Stevens, and M. Pigliucci. 2010. A comprehensive\ntest of the ‚Äúlimiting resources‚Äù framework applied to plant\ntolerance to apical meristem damage. Oikos 119:359‚Äì369.\n\n\nBarto≈Ñ, K. 2024. MuMIn:\nMulti-model inference.\n\n\nBates, D., M. M√§chler, B. Bolker, and S. Walker. 2015. Fitting linear\nmixed-effects models using lme4. Journal\nof Statistical Software 67:1‚Äì48.\n\n\nBolker, B. M., M. E. Brooks, C. J. Clark, S. W. Geange, J. R. Poulsen,\nM. H. H. Stevens, and J.-S. S. White. 2009. Generalized linear mixed\nmodels: A practical guide for ecology and evolution. Trends in Ecology\nand Evolution 24:127‚Äì135.\n\n\nBolker, B., and D. Robinson. 2024. broom.mixed: Tidying methods for mixed models.\n\n\nB√ºrkner, P.-C. 2017. brms: An R package for\nBayesian multilevel models using Stan.\nJournal of Statistical Software 80:1‚Äì28.\n\n\nB√ºrkner, P.-C. 2018. Advanced\nBayesian multilevel modeling with the R\npackage brms. The R Journal 10:395‚Äì411.\n\n\nB√ºrkner, P.-C. 2021. Bayesian item response\nmodeling in R with brms and\nStan. Journal of Statistical Software 100:1‚Äì54.\n\n\nCarr, D., ported by Nicholas Lewin-Koh, M. Maechler, and contains copies\nof lattice functions written by Deepayan Sarkar. 2023. hexbin: Hexagonal binning routines.\n\n\nChampely, S. 2020. pwr: Basic functions for power analysis.\n\n\nChang, W., J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J.\nAllen, J. McPherson, A. Dipert, and B. Borges. 2024. shiny: Web application framework for r.\n\n\nDouglas, A. 2023. An introduction to\nr.\n\n\nEddelbuettel, D. 2024. digest: Create compact hash digests of r\nobjects.\n\n\nElston, D. A., R. Moss, T. Boulinier, C. Arrowsmith, and X. Lambin.\n2001. Analysis of aggregation, a worked example: Numbers of ticks on red\ngrouse chicks. Parasitology 122:563‚Äì569.\n\n\nFox, J. 2003. Effect\ndisplays in R for generalised linear models. Journal of\nStatistical Software 8:1‚Äì27.\n\n\nFox, J., and J. Hong. 2009. Effect displays in\nR for multinomial and proportional-odds logit models:\nExtensions to the effects package.\nJournal of Statistical Software 32:1‚Äì24.\n\n\nFox, J., and S. Weisberg. 2018. Visualizing fit and lack of\nfit in complex regression models with predictor effect plots and partial\nresiduals. Journal of Statistical Software 87:1‚Äì27.\n\n\nFox, J., and S. Weisberg. 2019a. An\nR companion to applied regression. Third. Sage,\nThousand Oaks CA.\n\n\nFox, J., and S. Weisberg. 2019b. An\nr companion to applied regression. 3rd edition. Sage, Thousand Oaks\nCA.\n\n\nFriendly, M. 2023. vcdExtra: ‚Äúvcd‚Äù extensions and additions.\n\n\nGabry, J., and T. Mahr. 2024. bayesplot: Plotting for bayesian models.\n\n\nGabry, J., D. Simpson, A. Vehtari, M. Betancourt, and A. Gelman. 2019.\nVisualization in bayesian\nworkflow. J. R. Stat. Soc. A 182:389‚Äì402.\n\n\nGarbett, S. P., J. Stephens, K. Simonov, Y. Xie, Z. Dong, H. Wickham, J.\nHorner, reikoch, W. Beasley, B. O‚ÄôConnor, G. R. Warnes, M. Quinn, Z. N.\nKamvar, and C. Gao. 2024. yaml: Methods to convert r data to YAML and\nback.\n\n\nGenz, A., and F. Bretz. 2009. Computation of multivariate normal and t\nprobabilities. Springer-Verlag, Heidelberg.\n\n\nHadfield, J. D. 2010. MCMC\nmethods for multi-response generalized linear mixed models: The\nMCMCglmm R package. Journal of Statistical\nSoftware 33:1‚Äì22.\n\n\nHadfield, J. D., A. J. Wilson, D. Garant, B. C. Sheldon, and L. E.\nKruuk. 2010. The Misuse of BLUP in\nEcology and Evolution. American Naturalist\n175:116‚Äì125.\n\n\nHartig, F. 2022. DHARMa:\nResidual diagnostics for hierarchical (multi-level / mixed) regression\nmodels.\n\n\nHester, J., and G. Cs√°rdi. 2024. brio: Basic r input output.\n\n\nHester, J., L. Henry, K. M√ºller, K. Ushey, H. Wickham, and W. Chang.\n2024. withr: Run code ‚ÄúWith‚Äù\ntemporarily modified global state.\n\n\nHorikoshi, M., and Y. Tang. 2018. ggfortify: Data visualization tools for\nstatistical analysis results.\n\n\nHorst, A. M., A. P. Hill, and K. B. Gorman. 2020. palmerpenguins: Palmer archipelago (antarctica)\npenguin data.\n\n\nHothorn, T., F. Bretz, and P. Westfall. 2008. Simultaneous inference in\ngeneral parametric models. Biometrical Journal 50:346‚Äì363.\n\n\nHouslay, T. M., and A. J. Wilson. 2017. Avoiding the misuse of\nBLUP in behavioural ecology. Behavioral Ecology\n28:948‚Äì952.\n\n\nHvitfeldt, E. 2022. emoji: Data and function to work with emojis.\n\n\nIannone, R., J. Cheng, B. Schloerke, E. Hughes, A. Lauer, J. Seo, K.\nBrevoort, and O. Roy. 2024. gt: Easily create presentation-ready display\ntables.\n\n\nKassambara, A. 2023. ggpubr: ‚Äúggplot2‚Äù based publication ready plots.\n\n\nKassambara, A., and F. Mundt. 2020. factoextra: Extract and visualize the results of\nmultivariate data analyses.\n\n\nKoenker, R. 2024. quantreg: Quantile regression.\n\n\nKuznetsova, A., P. B. Brockhoff, and R. H. B. Christensen. 2017. lmerTest package: Tests in linear mixed effects\nmodels. Journal of Statistical Software 82:1‚Äì26.\n\n\nL√™, S., J. Josse, and F. Husson. 2008. FactoMineR: A\npackage for multivariate analysis. Journal of Statistical Software\n25:1‚Äì18.\n\n\nL√ºdecke, D., M. S. Ben-Shachar, I. Patil, P. Waggoner, and D. Makowski.\n2021. performance: An R package for\nassessment, comparison and testing of statistical models. Journal of\nOpen Source Software 6:3139.\n\n\nMartin, J. 1219. Another lasagna recipe from medieval times. Journal of\nLasagna 4:1686.\n\n\nMartin, J. 2200. A silly example. Chapman; Hall/CRC, Boca Raton,\nFlorida.\n\n\nMeyer, D., A. Zeileis, and K. Hornik. 2006. The strucplot framework:\nVisualizing multi-way contingency tables with vcd. Journal of\nStatistical Software 17:1‚Äì48.\n\n\nMeyer, D., A. Zeileis, K. Hornik, and M. Friendly. 2023. vcd: Visualizing categorical data.\n\n\nPedersen, T. L. 2024. patchwork: The composer of plots.\n\n\nPeng, R. D. 2024. simpleboot: Simple bootstrap routines.\n\n\nPlummer, M., N. Best, K. Cowles, and K. Vines. 2006. CODA:\nConvergence diagnosis and output analysis for MCMC. R News 6:7‚Äì11.\n\n\nPrunello, M., and G. Mari. 2021. ggcleveland: Implementation of plots from\ncleveland‚Äôs visualizing data book.\n\n\nR Core Team. 2024. R:\nA language and environment for statistical computing. R Foundation\nfor Statistical Computing, Vienna, Austria.\n\n\nRodriguez-Sanchez, F., and C. P. Jackson. 2023. grateful: Facilitate citation of r packages.\n\n\nSalmon, M. 2024. babeldown: Helpers for automatic translation of\nmarkdown-based content.\n\n\nSalmon, M., Z. N. Kamvar, and J. Ooms. 2024. tinkr: Cast\n‚Äú(R)Markdown‚Äù files to\n‚ÄúXML‚Äù and back again.\n\n\nSarkar, D. 2008. Lattice:\nMultivariate data visualization with r. Springer, New York.\n\n\nSchloerke, B., D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen,\nA. Elberg, and J. Crowley. 2024. GGally:\nExtension to ‚Äúggplot2‚Äù.\n\n\nStan Development Team. 2021. Stan modeling\nlanguage users guide and reference manual, 2.26.\n\n\nStan Development Team. 2024. RStan: The R\ninterface to Stan.\n\n\nStoffel, M. A., S. Nakagawa, and H. Schielzeth. 2017. rptR: Repeatability estimation and variance\ndecomposition by generalized linear mixed-effects models. Methods in\nEcology and Evolution 8:1639???1644.\n\n\nTang, Y., M. Horikoshi, and W. Li. 2016. ggfortify: Unified interface to visualize\nstatistical result of popular r packages. The R Journal 8:474‚Äì485.\n\n\nThe VSNi Team. 2023. asreml: Fits linear mixed models using REML.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern applied statistics\nwith s. Fourth. Springer, New York.\n\n\nWheeler, B., and M. Torchiano. 2016. lmPerm: Permutation tests for linear models.\n\n\nWickham, H. 2007. Reshaping\ndata with the reshape package. Journal\nof Statistical Software 21:1‚Äì20.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. Fran√ßois,\nG. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E.\nMiller, S. M. Bache, K. M√ºller, J. Ooms, D. Robinson, D. P. Seidel, V.\nSpinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, and H. Yutani. 2019.\nWelcome to the tidyverse. Journal of Open Source Software\n4:1686.\n\n\nWickham, H., J. Hester, W. Chang, K. M√ºller, and D. Cook. 2021. memoise: ‚ÄúMemoisation‚Äù\nof functions.\n\n\nWilkinson, L. 2005. The Grammar of Graphics.\nSpringer Science & Business Media.\n\n\nWolak, M. E. 2012. nadiv: An R package to create\nrelatedness matrices for estimating non-additive genetic variances in\nanimal models. Methods in Ecology and Evolution 3:792‚Äì796.\n\n\nXie, Y. 2014. knitr: A comprehensive tool\nfor reproducible research in R. in V. Stodden, F.\nLeisch, and R. D. Peng, editors. Implementing reproducible computational\nresearch. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic documents with\nR and knitr. 2nd edition. Chapman; Hall/CRC, Boca\nRaton, Florida.\n\n\nXie, Y. 2024. knitr: A general-purpose package for dynamic\nreport generation in r.\n\n\nXie, Y., J. J. Allaire, and G. Grolemund. 2018. R markdown: The definitive\nguide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, and E. Riederer. 2020. R markdown\ncookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZeileis, A., and T. Hothorn. 2002. Diagnostic checking in\nregression relationships. R News 2:7‚Äì10.\n\n\nZeileis, A., D. Meyer, and K. Hornik. 2007. Residual-based shadings\nfor visualizing (conditional) independence. Journal of Computational\nand Graphical Statistics 16:507‚Äì525.\n\n\nZhu, H. 2024. kableExtra: Construct complex table with\n‚Äúkable‚Äù and pipe syntax.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "07-github.html",
    "href": "07-github.html",
    "title": "7¬† Version control with Git and GitHub",
    "section": "",
    "text": "7.1 What is version control?\nA Version Control System (VCS) keeps a record of all the changes you make to your files that make up a particular project and allows you to revert to previous versions of files if you need to. To put it another way, if you muck things up or accidentally lose important files you can easily roll back to a previous stage in your project to sort things out. Version control was originally designed for collaborative software development, but it‚Äôs equally useful for scientific research and collaborations (although admittedly a lot of the terms, jargon and functionality are focused on the software development side). There are many different version control systems currently available, but we‚Äô we‚Äôll focus on using Git, because it‚Äôs free and open source and it integrates nicely with RStudio. This means that its can easily become part of your usual workflow with minimal additional overhead.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#why-use-version-control",
    "href": "07-github.html#why-use-version-control",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.2 Why use version control?",
    "text": "7.2 Why use version control?\nSo why should you worry about version control? Well, first of all it helps avoid this (familiar?) situation when you‚Äôre working on a project usually arising from this (familiar?) scenario.\n\n\n\n\n\n\n\nFigure¬†7.1: Why you need version control (source: PhDComics)\n\n\n\n\nVersion control automatically takes care of keeping a record of all the versions of a particular file and allows you to revert back to previous versions if you need to. Version control also helps you (especially the future you) keep track of all your files in a single place and it helps others (especially collaborators) review, contribute to and reuse your work through the GitHub website. Lastly, your files are always available from anywhere and on any computer, all you need is an internet connection.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#what-is-git-and-github",
    "href": "07-github.html#what-is-git-and-github",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.3 What is Git and GitHub?",
    "text": "7.3 What is Git and GitHub?\nGit is a version control system originally developed by Linus Torvalds that lets you track changes to a set of files. These files can be any type of file including the menagerie of files that typically make up a data orientated project (.pdf, .Rmd, .docx, .txt, .jpg etc) although plain text files work the best. All the files that make up a project is called a repository (or just repo).\nGitHub is a web-based hosting service for Git repositories which allows you to create a remote copy of your local version-controlled project. This can be used as a backup or archive of your project or make it accessible to you and to your colleagues so you can work collaboratively.\nAt the start of a project we typically (but not always) create a remote repository on GitHub, then clone (think of this as copying) this repository to our local computer (the one in front of you). This cloning is usually a one time event and you shouldn‚Äôt need to clone this repository again unless you really muck things up. Once you have cloned your repository you can then work locally on your project as usual, creating and saving files for your data analysis (scripts, R markdown documents, figures etc). Along the way you can take snapshots (called commits) of these files after you‚Äôve made important changes. We can then push these changes to the remote GitHub repository to make a backup or make available to our collaborators. If other people are working on the same project (repository), or maybe you‚Äôre working on a different computer, you can pull any changes back to your local repository so everything is synchronised.\n\n\n\n\n\n\n\nFigure¬†7.2: How git works",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-setup-git",
    "href": "07-github.html#sec-setup-git",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.4 Getting started",
    "text": "7.4 Getting started\nThis Chapter assumes that you have already installed the latest versions of R and an IDE (RStudio or VSCode). If you haven‚Äôt done this yet you can find instructions in Section 1.1.1.\n\n7.4.1 Install Git\nTo get started, you first need to install Git. If you‚Äôre lucky you may already have Git installed (especially if you have a Mac or Linux computer). You can check if you already have Git installed by clicking on the Terminal tab in RStudio and typing git --version. If you see something that looks like git version 2.25.0 (the version number may be different on your computer) then you already have Git installed (happy days). If you get an error (something like git: command not found) this means you don‚Äôt have Git installed (yet!).\nYou can also do this check outside RStudio by opening up a separate Terminal if you want. On Windows go to the ‚ÄòStart menu‚Äô and in the search bar (or run box) type cmd and press enter. On a Mac go to ‚ÄòApplications‚Äô in Finder, click on the ‚ÄòUtilities‚Äô folder and then on the ‚ÄòTerminal‚Äô program. On a Linux machine simply open the Terminal (Ctrl+Alt+T often does it).\nTo install Git on a Windows computer we recommend you download and install Git for Windows (also known as ‚ÄòGit Bash‚Äô). You can find the download file and installation instructions here.\nFor those of you using a Mac computer we recommend you download Git from here and install in the usual way (double click on the installer package once downloaded). If you‚Äôve previously installed Xcode on your Mac and want to use a more up to date version of Git then you will need to follow a few more steps documented here. If you‚Äôve never heard of Xcode then don‚Äôt worry about it!\nFor those of you lucky enough to be working on a Linux machine you can simply use your OS package manager to install Git from the official repository. For Ubuntu Linux (or variants of) open your Terminal and type\nsudo apt update\nsudo apt install git\nYou will need administrative privileges to do this. For other versions of Linux see here for further installation instructions.\nWhatever version of Git you‚Äôre installing, once the installation has finished verify that the installation process has been successful by running the command git --version in the Terminal tab in RStudio (as described above). On some installations of Git (yes we‚Äôre looking at you MS Windows) this may still produce an error as you will also need to setup RStudio so it can find the Git executable (described in Section 7.4.3).\n\n7.4.2 Configure Git\nAfter installing Git, you need to configure it so you can use it. Click on the Terminal tab in the Console window again and type the following:\ngit config --global user.email 'you@youremail.com'\n\ngit config --global user.name 'Your Name'\nsubstituting 'Your Name' for your actual name and 'you@youremail.com' with your email address. We recommend you use your University email address (if you have one) as you will also use this address when you register for your GitHub account (coming up in a bit).\nIf this was successful, you should see no error messages from these commands. To verify that you have successfully configured Git type the following into the Terminal\ngit config --global --list\nYou should see both your user.name and user.email configured.\n\n7.4.3 Configure RStudio\nAs you can see above, Git can be used from the command line, but it also integrates well with RStudio, providing a friendly graphical user interface. If you want to use RStudio‚Äôs Git integration (we recommend you do - at least at the start), you need to check that the path to the Git executable is specified correctly. In RStudio, go to the menu Tools -&gt; Global Options -&gt; Git/SVN and make sure that ‚ÄòEnable version control interface for RStudio projects‚Äô is ticked and that the ‚ÄòGit executable:‚Äô path is correct for your installation. If it‚Äôs not correct hit the Browse... button and navigate to where you installed git and click on the executable file. You will need to restart RStudio after doing this.\n\n\n\n\n\n\n\nFigure¬†7.3: Providing path to git software in RStudio\n\n\n\n\n\n7.4.4 Configure VSCode\n\nto develop\n\n\n7.4.5 Register a GitHub account\nIf all you want to do is to keep track of files and file versions on your local computer then Git is sufficient. If however, you would like to make an off-site copy of your project or make it available to your collaborators then you will need a web-based hosting service for your Git repositories. This is where GitHub comes into play (there are also other services like GitLab, Bitbucket and Savannah). You can sign up for a free account on GitHub here. You will need to specify a username, an email address and a strong password. We suggest that you use your University email address (if you have one) as this will also allow you to apply for a free educator or researcher account later on which gives you some useful benefits (don‚Äôt worry about this now though). When it comes to choosing a username we suggest you give this some thought. Choose a short(ish) rather than a long username, use all lowercase and hyphenate if you want to include multiple words, find a way of incorporating your actual name and lastly, choose a username that you will feel comfortable revealing to your future employer!\nNext click on the ‚ÄòSelect a plan‚Äô (you may have to solve a simple puzzle first to verify you‚Äôre human) and choose the ‚ÄòFree Plan‚Äô option. Github will send an email to the email address you supplied for you to verify.\nOnce you‚Äôve completed all those steps you should have both Git and GitHub setup up ready for you to use (Finally!).",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#setting-up-a-project",
    "href": "07-github.html#setting-up-a-project",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.5 Setting up a project",
    "text": "7.5 Setting up a project\n\n7.5.1 in RStudio\nNow that you‚Äôre all set up, let‚Äôs create your first version controlled RStudio project. There are a couple of different approaches you can use to do this. You can either setup a remote GitHub repository first then connect an RStudio project to this repository (we‚Äôll call this Option 1). Another option is to setup a local repository first and then link a remote GitHub repository to this repository (Option 2). You can also connect an existing project to a GitHub repository but we won‚Äôt cover this here. We suggest that if you‚Äôre completely new to Git and GitHub then use Option 1 as this approach sets up your local Git repository nicely and you can push and pull immediately. Option 2 requires a little more work and therefore there are more opportunities to go wrong. We will cover both of these options below.\n\n7.5.2 Option 1 - GitHub first\nTo use the GitHub first approach you will first need to create a repository (repo) on GitHub. Go to your GitHub page and sign in if necessary. Click on the ‚ÄòRepositories‚Äô tab at the top and then on the green ‚ÄòNew‚Äô button on the right\n\n\n\n\n\n\n\nFigure¬†7.4: Creating a new repository on Github\n\n\n\n\nGive your new repo a name (let‚Äôs call it first_repo for this Chapter), select ‚ÄòPublic‚Äô, tick on the ‚ÄòInitialize this repository with a README‚Äô (this is important) and then click on ‚ÄòCreate repository‚Äô (ignore the other options for now).\n\n\n\n\n\n\n\nFigure¬†7.5: Configuring a new repository on Github\n\n\n\n\nYour new GitHub repository will now be created. Notice the README has been rendered in GitHub and is in markdown (.md) format (see Chapter 6 on R markdown if this doesn‚Äôt mean anything to you). Next click on the green ‚ÄòClone or Download‚Äô button and copy the https//... URL that pops up for later (either highlight it all and copy or click on the copy to clipboard icon to the right).\n\n\n\n\n\n\n\nFigure¬†7.6: Getting the cloning path for a directory on github\n\n\n\n\nOk, we now switch our attention to RStudio. In RStudio click on the File -&gt; New Project menu. In the pop up window select Version Control.\n\n\n\n\n\n\n\nFigure¬†7.7: Setting a new Github project in RStudio\n\n\n\n\nNow paste the the URL you previously copied from GitHub into the Repository URL: box. This should then automatically fill out the Project Directory Name: section with the correct repository name (it‚Äôs important that the name of this directory has the same name as the repository you created in GitHub). You can then select where you want to create this directory by clicking on the Browse button opposite the Create project as a subdirectory of: option. Navigate to where you want the directory created and click OK. We also tick the Open in new session option.\n\n\n\n\n\n\n\n\nFigure¬†7.8\n\n\n\n\nRStudio will now create a new directory with the same name as your repository on your local computer and will then clone your remote repository to this directory. The directory will contain three new files; first_repo.Rproj (or whatever you called your repository), README.md and .gitignore. You can check this out in the Files tab usually in the bottom right pane in RStudio. You will also have a Git tab in the top right pane with two files listed (we will come to this later on in the Chapter). That‚Äôs it for Option 1, you now have a remote GitHub repository set up and this is linked to your local repository managed by RStudio. Any changes you make to files in this directory will be version controlled by Git.\n\n\n\n\n\n\n\nFigure¬†7.9\n\n\n\n\n\n7.5.3 Option 2 - RStudio first\nAn alternative approach is to create a local RStudio project first and then link to a remote Github repository. As we mentioned before, this option is more involved than Option 1 so feel free to skip this now and come back later to it if you‚Äôre interested. This option is also useful if you just want to create a local RStudio project linked to a local Git repository (i.e. no GitHub involved). If you want to do this then just follow the instructions below omitting the GitHub bit.\nIn RStudio click on the File -&gt; New Project menu and select the New Directory option.\n\n\n\n\n\n\n\nFigure¬†7.10\n\n\n\n\nIn the pop up window select the New Project option\n\n\n\n\n\n\n\nFigure¬†7.11\n\n\n\n\nIn the New Project window specify a Directory name (choose second_repo for this Chapter) and select where you would like to create this directory on you computer (click the Browse button). Make sure the Create a git repository option is ticked\n\n\n\n\n\n\n\nFigure¬†7.12\n\n\n\n\nThis will create a version controlled directory called second_repo on your computer that contains two files, second_repo.Rproj and .gitignore (there might also be a .Rhistory file but ignore this). You can check this by looking in the Files tab in RStudio (usually in the bottom right pane).\n\n\n\n\n\n\n\nFigure¬†7.13\n\n\n\n\nOK, before we go on to create a repository on GitHub we need to do one more thing - we need to place our second_repo.Rproj and .gitignorefiles under version control. Unfortunately we haven‚Äôt covered this in detail yet so just follow the next few instructions (blindly!) and we‚Äôll revisit them in Section 7.6 of this Chapter.\nTo get our two files under version control click on the ‚ÄòGit‚Äô tab which is usually in the top tight pane in RStudio\n\n\n\n\n\n\n\nFigure¬†7.14\n\n\n\n\nYou can see that both files are listed. Next, tick the boxes under the ‚ÄòStaged‚Äô column for both files and then click on the ‚ÄòCommit‚Äô button.\n\n\n\n\n\n\n\nFigure¬†7.15\n\n\n\n\nThis will take you to the ‚ÄòReview Changes‚Äô window. Type in the commit message ‚ÄòFirst commit‚Äô in the ‚ÄòCommit message‚Äô window and click on the ‚ÄòCommit‚Äô button. A new window will appear with some messages which you can ignore for now. Click ‚ÄòClose‚Äô to close this window and also close the ‚ÄòReview Changes‚Äô window. The two files should now have disappeared from the Git pane in RStudio indicating a successful commit.\n\n\n\n\n\n\n\nFigure¬†7.16\n\n\n\n\nOK, that‚Äôs those two files now under version control. Now we need to create a new repository on GitHub. In your browser go to your GitHub page and sign in if necessary. Click on the ‚ÄòRepositories‚Äô tab and then click on the green ‚ÄòNew‚Äô button on the right. Give your new repo the name second_repo (the same as your version controlled directory name) and select ‚ÄòPublic‚Äô. This time do not tick the ‚ÄòInitialize this repository with a README‚Äô (this is important) and then click on ‚ÄòCreate repository‚Äô.\n\n\n\n\n\n\n\nFigure¬†7.17\n\n\n\n\nThis will take you to a Quick setup page which provides you with some code for various situations. The code we are interested in is the code under ...or push an existing repository from the command line heading.\n\n\n\n\n\n\n\nFigure¬†7.18\n\n\n\n\nHighlight and copy the first line of code (note: yours will be slightly different as it will include your GitHub username not mine)\ngit remote add origin https://github.com/alexd106/second_repo.git\nSwitch to RStudio, click on the ‚ÄòTerminal‚Äô tab and paste the command into the Terminal. Now go back to GitHub and copy the second line of code\ngit push -u origin master\nand paste this into the Terminal in RStudio. You should see something like this\n\n\n\n\n\n\n\nFigure¬†7.19\n\n\n\n\nIf you take a look at your repo back on GitHub (click on the /second_repo link at the top) you will see the second_repo.Rproj and .gitignore files have now been pushed to GitHub from your local repository.\n\n\n\n\n\n\n\nFigure¬†7.20\n\n\n\n\nThe last thing we need to do is create and add a README file to your repository. A README file describes your project and is written using the same Markdown language you learned in Chapter 6. A good README file makes it easy for others (or the future you!) to use your code and reproduce your project. You can create a README file in RStudio or in GitHub. Let‚Äôs use the second option.\nIn your repository on GitHub click on the green Add a README button.\n\n\n\n\n\n\n\nFigure¬†7.21\n\n\n\n\nNow write a short description of your project in the &lt;&gt; Edit new file section and then click on the green Commit new file button.\n\n\n\n\n\n\n\nFigure¬†7.22\n\n\n\n\nYou should now see the README.md file listed in your repository. It won‚Äôt actually exist on your computer yet as you will need to pull these changes back to your local repository, but more about that in the next section.\nWhether you followed Option 1 or Option 2 (or both) you have now successfully setup a version controlled RStudio project (and associated directory) and linked this to a GitHub repository. Git will now monitor this directory for any changes you make to files and also if you add or delete files. If the steps above seem like a bit of an ordeal, just remember, you only need to do this once for each project and it gets much easier over time.\n\n7.5.4 in VSCode\n\nto develop",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-use-git",
    "href": "07-github.html#sec-use-git",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.6 Using Git with RStudio",
    "text": "7.6 Using Git with RStudio\nNow that we have our project and repositories (both local and remote) set up, it‚Äôs finally time to learn how to use Git in your IDE!\nTypically, when using Git your workflow will go something like this:\n\nYou create/delete and edit files in your project directory on your computer as usual (saving these changes as you go)\nOnce you‚Äôve reached a natural ‚Äòbreak point‚Äô in your progress (i.e. you‚Äôd be sad if you lost this progress) you stage these files\nYou then commit the changes you made to these staged files (along with a useful commit message) which creates a permanent snapshot of these changes\nYou keep on with this cycle until you get to a point when you would like to push these changes to GitHub\nIf you‚Äôre working with other people on the same project you may also need to pull their changes to your local computer\n\n\n\n\n\n\n\n\nFigure¬†7.23\n\n\n\n\nOK, let‚Äôs go through an example to help clarify this workflow.\nOpen up the first_repo.Rproj you created previously during Option 1. Either use the File -&gt; Open Project menu or click on the top right project icon and select the appropriate project.\n\n\n\n\n\n\n\nFigure¬†7.24\n\n\n\n\nCreate an R markdown document inside this project by clicking on the File -&gt; New File -&gt; R markdown menu (remember Chapter 6?).\nOnce created, we can delete all the example R markdown code (except the YAML header) as usual and write some interesting R markdown text and include a plot. We‚Äôll use the inbuilt cars dataset to do this. Save this file (cmd + s for Mac or ctrl + s in Windows). Your R markdown document should look something like the following (it doesn‚Äôt matter if it‚Äôs not exactly the same).\n\n\n\n\n\n\n\nFigure¬†7.25\n\n\n\n\nTake a look at the ‚ÄòGit‚Äô tab which should list your new R markdown document (first_doc.Rmd in this example) along with first_repo.Rproj, and .gitignore (you created these files previously when following Option 1).\n\n\n\n\n\n\n\nFigure¬†7.26\n\n\n\n\nFollowing our workflow, we now need to stage these files. To do this tick the boxes under the ‚ÄòStaged‚Äô column for all files. Notice that there is a status icon next to the box which gives you an indication of how the files were changed. In our case all of the files are to be added (capital A) as we have just created them.\n\n\n\n\n\n\n\nFigure¬†7.27\n\n\n\n\nAfter you have staged the files the next step is to commit the files. This is done by clicking on the ‚ÄòCommit‚Äô button.\n\n\n\n\n\n\n\nFigure¬†7.28\n\n\n\n\nAfter clicking on the ‚ÄòCommit‚Äô button you will be taken to the ‚ÄòReview Changes‚Äô window. You should see the three files you staged from the previous step in the left pane. If you click on the file name first_doc.Rmd you will see the changes you have made to this file highlighted in the bottom pane. Any content that you have added is highlighted in green and deleted content is highlighted in red. As you have only just created this file, all the content is highlighted in green. To commit these files (take a snapshot) first enter a mandatory commit message in the ‚ÄòCommit message‚Äô box. This message should be relatively short and informative (to you and your collaborators) and indicate why you made the changes, not what you changed. This makes sense as Git keeps track of what has changed and so it is best not to use commit messages for this purpose. It‚Äôs traditional to enter the message ‚ÄòFirst commit‚Äô (or ‚ÄòInitial commit‚Äô) when you commit files for the first time. Now click on the ‚ÄòCommit‚Äô button to commit these changes.\n\n\n\n\n\n\n\nFigure¬†7.29\n\n\n\n\nA summary of the commit you just performed will be shown. Now click on the ‚ÄòClose‚Äô button to return to the ‚ÄòReview Changes‚Äô window. Note that the staged files have now been removed.\n\n\n\n\n\n\n\nFigure¬†7.30\n\n\n\n\nNow that you have committed your changes the next step is to push these changes to GitHub. Before you push your changes it‚Äôs good practice to first pull any changes from GitHub. This is especially important if both you and your collaborators are working on the same files as it keeps you local copy up to date and avoids any potential conflicts. In this case your repository will already be up to date but it‚Äôs a good habit to get into. To do this, click on the ‚ÄòPull‚Äô button on the top right of the ‚ÄòReview Changes‚Äô window. Once you have pulled any changes click on the green ‚ÄòPush‚Äô button to push your changes. You will see a summary of the push you just performed. Hit the ‚ÄòClose‚Äô button and then close the ‚ÄòReview Changes‚Äô window.\n\n\n\n\n\n\n\nFigure¬†7.31\n\n\n\n\nTo confirm the changes you made to the project have been pushed to GitHub, open your GitHub page, click on the Repositories link and then click on the first_repo repository. You should see four files listed including the first_doc.Rmd you just pushed. Along side the file name you will see your last commit message (‚ÄòFirst commit‚Äô in this case) and when you made the last commit.\n\n\n\n\n\n\n\nFigure¬†7.32\n\n\n\n\nTo see the contents of the file click on the first_doc.Rmd file name.\n\n\n\n\n\n\n\nFigure¬†7.33\n\n\n\n\n\n7.6.1 Tracking changes\nAfter following the steps outlined above, you will have successfully modified an RStudio project by creating a new R markdown document, staged and then committed these changes and finally pushed the changes to your GitHub repository. Now let‚Äôs make some further changes to your R markdown file and follow the workflow once again but this time we ‚Äôll take a look at how to identify changes made to files, examine the commit history and how to restore to a previous version of the document.\nIn RStudio open up the first_repo.Rproj file you created previously (if not already open) then open the first_doc.Rmd file (click on the file name in the Files tab in RStudio).\nLet‚Äôs make some changes to this document. Delete the line beginning with ‚ÄòMy first version controlled ‚Ä¶‚Äô and replace it with something more informative (see figure below). We will also change the plotted symbols to red and give the plot axes labels. Lastly, let‚Äôs add a summary table of the dataframe using the kable() and summary() functions (you may need to install the knitr package if you haven‚Äôt done so previously to use the kable() function) and finally render this document to pdf by changing the YAML option to output: pdf_document.\n\n\n\n\n\n\n\nFigure¬†7.34\n\n\n\n\nNow save these changes and then click the knit button to render to pdf. A new pdf file named first_doc.pdf will be created which you can view by clicking on the file name in the Files tab in RStudio.\nNotice that these two files have been added to the Git tab in RStudio. The status icons indicate that the first_doc.Rmd file has been modified (capital M) and the first_doc.pdf file is currently untracked (question mark).\n\n\n\n\n\n\n\nFigure¬†7.35\n\n\n\n\nTo stage these files tick the ‚ÄòStaged‚Äô box for each file and click on the ‚ÄòCommit‚Äô button to take you to the ‚ÄòReview Changes‚Äô window\n\n\n\n\n\n\n\nFigure¬†7.36\n\n\n\n\nBefore you commit your changes notice the status of first_doc.pdf has changed from untracked to added (A). You can view the changes you have made to the first_doc.Rmd by clicking on the file name in the top left pane which will provide you with a useful summary of the changes in the bottom pane (technically called diffs). Lines that have been deleted are highlighted in red and lines that have been added are highlighted in green (note that from Git‚Äôs point of view, a modification to a line is actually two operations: the removal of the original line followed by the creation of a new line). Once you‚Äôre happy, commit these changes by writing a suitable commit message and click on the ‚ÄòCommit‚Äô button.\n\n\n\n\n\n\n\nFigure¬†7.37\n\n\n\n\nTo push the changes to GitHub, click on the ‚ÄòPull‚Äô button first (remember this is good practice even though you are only collaborating with yourself at the moment) and then click on the ‚ÄòPush‚Äô button. Go to your online GitHub repository and you will see your new commits, including the first_doc.pdf file you created when you rendered your R markdown document.\n\n\n\n\n\n\n\nFigure¬†7.38\n\n\n\n\nTo view the changes in first_doc.Rmd click on the file name for this file.\n\n\n\n\n\n\n\nFigure¬†7.39\n\n\n\n\n\n7.6.2 Commit history\nOne of the great things about Git and GitHub is that you can view the history of all the commits you have made along with the associated commit messages. You can do this locally using RStudio (or the Git command line) or if you have pushed your commits to GitHub you can check them out on the GitHub website.\nTo view your commit history in RStudio click on the ‚ÄòHistory‚Äô button (the one that looks like a clock) in the Git pane to bring up the history view in the ‚ÄòReview Changes‚Äô window. You can also click on the ‚ÄòCommit‚Äô or ‚ÄòDiff‚Äô buttons which takes you to the same window (you just need to additionally click on the ‚ÄòHistory‚Äô button in the ‚ÄòReview Changes‚Äô window).\n\n\n\n\n\n\n\nFigure¬†7.40\n\n\n\n\nThe history window is split into two parts. The top pane lists every commit you have made in this repository (with associated commit messages) starting with the most recent one at the top and oldest at the bottom. You can click on each of these commits and the bottom pane shows you the changes you have made along with a summary of the Date the commit was made, Author of the commit and the commit message (Subject). There is also a unique identifier for the commit (SHA - Secure Hash Algorithm) and a Parent SHA which identifies the previous commit. These SHA identifiers are really important as you can use them to view and revert to previous versions of files (details below Section 7.6.3). You can also view the contents of each file by clicking on the ‚ÄòView file @ SHA key` link (in our case ‚ÄôView file @ 2b4693d1‚Äô).\n\n\n\n\n\n\n\nFigure¬†7.41\n\n\n\n\nYou can also view your commit history on GitHub website but this will be limited to only those commits you have already pushed to GitHub. To view the commit history navigate to the repository and click on the ‚Äòcommits‚Äô link (in our case the link will be labelled ‚Äò3 commits‚Äô as we have made 3 commits).\n\n\n\n\n\n\n\nFigure¬†7.42\n\n\n\n\nYou will see a list of all the commits you have made, along with commit messages, date of commit and the SHA identifier (these are the same SHA identifiers you saw in the RStudio history). You can even browse the repository at a particular point in time by clicking on the &lt;&gt; link. To view the changes in files associated with the commit simply click on the relevant commit link in the list.\n\n\n\n\n\n\n\nFigure¬†7.43\n\n\n\n\nWhich will display changes using the usual format of green for additions and red for deletions.\n\n\n\n\n\n\n\nFigure¬†7.44\n\n\n\n\n\n7.6.3 Reverting changes\nOne the great things about using Git is that you are able to revert to previous versions of files if you‚Äôve made a mistake, broke something or just prefer and earlier approach. How you do this will depend on whether the changes you want to discard have been staged, committed or pushed to GitHub. We‚Äôll go through some common scenarios below mostly using RStudio but occasionally we will need to resort to using the Terminal (still in RStudio though).\nChanges saved but not staged, committed or pushed\nIf you have saved changes to your file(s) but not staged, committed or pushed these files to GitHub you can right click on the offending file in the Git pane and select ‚ÄòRevert ‚Ä¶‚Äô. This will roll back all of the changes you have made to the same state as your last commit. Just be aware that you cannot undo this operation so use with caution.\n\n\n\n\n\n\n\nFigure¬†7.45\n\n\n\n\nYou can also undo changes to just part of a file by opening up the ‚ÄòDiff‚Äô window (click on the ‚ÄòDiff‚Äô button in the Git pane). Select the line you wish to discard by double clicking on the line and then click on the ‚ÄòDiscard line‚Äô button. In a similar fashion you can discard chunks of code by clicking on the ‚ÄòDiscard chunk‚Äô button.\n\n\n\n\n\n\n\nFigure¬†7.46\n\n\n\n\nStaged but not committed and not pushed\nIf you have staged your files, but not committed them then simply unstage them by clicking on the ‚ÄòStaged‚Äô check box in the Git pane (or in the ‚ÄòReview Changes‚Äô window) to remove the tick. You can then revert all or parts of the file as described in the section above.\nStaged and committed but not pushed\nIf you have made a mistake or have forgotten to include a file in your last commit which you have not yet pushed to GitHub, you can just fix your mistake, save your changes, and then amend your previous commit. You can do this by staging your file and then tick the ‚ÄòAmend previous commit` box in the ‚ÄôReview Changes‚Äô window before committing.\n\n\n\n\n\n\n\nFigure¬†7.47\n\n\n\n\nIf we check out our commit history you can see that our latest commit contains both changes to the file rather than having two separate commits. We use the amend commit approach alot but it‚Äôs important to understand that you should not do this if you have already pushed your last commit to GitHub as you are effectively rewriting history and all sorts bad things may happen!\n\n\n\n\n\n\n\nFigure¬†7.48\n\n\n\n\nIf you spot a mistake that has happened multiple commits back or you just want to revert to a previous version of a document you have a number of options.\nOption 1 - (probably the easiest but very unGit - but like, whatever!) is to look in your commit history in RStudio, find the commit that you would like to go back to and click on the ‚ÄòView file @‚Äô button to show the file contents.\n\n\n\n\n\n\n\nFigure¬†7.49\n\n\n\n\nYou can then copy the contents of the file to the clipboard and paste it into your current file to replace your duff code or text. Alternatively, you can click on the ‚ÄòSave As‚Äô button and save the file with a different file name. Once you have saved your new file you can delete your current unwanted file and then carry on working on your new file. Don‚Äôt forget to stage and commit this new file.\n\n\n\n\n\n\n\nFigure¬†7.50\n\n\n\n\nOption 2 - (Git like) Go to your Git history, find the commit you would like to roll back to and write down (or copy) its SHA identifier.\n\n\n\n\n\n\n\nFigure¬†7.51\n\n\n\n\nNow go to the Terminal in RStudio and type git checkout &lt;SHA&gt; &lt;filename&gt;. In our case the SHA key is 2b4693d1 and the filename is first_doc.Rmd so our command would look like this:\ngit checkout 2b4693d1 first_doc.Rmd\nThe command above will copy the selected file version from the past and place it into the present. RStudio may ask you whether you want to reload the file as it now changed - select yes. You will also need to stage and commit the file as usual.\nIf you want to revert all your files to the same state as a previous commit rather than just one file you can use (the single ‚Äòdot‚Äô . is important otherwise your HEAD will detach!):\ngit rm -r .\ngit checkout 2b4693d1 .\n\nNote that this will delete all files that you have created since you made this commit so be careful!\nStaged, committed and pushed\nIf you have already pushed your commits to GitHub you can use the git checkout strategy described above and then commit and push to update GitHub (although this is not really considered ‚Äòbest‚Äô practice). Another approach would be to use git revert (Note: as far as we can tell git revert is not the same as the ‚ÄòRevert‚Äô option in RStudio). The revert command in Git essentially creates a new commit based on a previous commit and therefore preserves all of your commit history. To rollback to a previous state (commit) you first need to identify the SHA for the commit you wish to go back to (as we did above) and then use the revert command in the Terminal. Let‚Äôs say we want to revert back to our ‚ÄòFirst commit‚Äô which has a SHA identifier d27e79f1.\n\n\n\n\n\n\n\nFigure¬†7.52\n\n\n\n\nWe can use the revert command as shown below in the Terminal. The --no-commit option is used to prevent us from having to deal with each intermediate commit.\ngit revert --no-commit d27e79f1..HEAD\nYour first_doc.Rmd file will now revert back to the same state as it was when you did your ‚ÄòFirst commit‚Äô. Notice also that the first_doc.pdf file has been deleted as this wasn‚Äôt present when we made our first commit. You can now stage and commit these files with a new commit message and finally push them to GitHub. Notice that if we look at our commit history all of the commits we have made are still present.\n\n\n\n\n\n\n\nFigure¬†7.53\n\n\n\n\nand our repo on GitHub also reflects these changes\n\n\n\n\n\n\n\nFigure¬†7.54",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-use-git_vsc",
    "href": "07-github.html#sec-use-git_vsc",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.7 Using Git with VSCode",
    "text": "7.7 Using Git with VSCode\nNow that we have our project and repositories (both local and remote) set up, it‚Äôs finally time to learn how to use Git in VSCode!\nTypically, when using Git your workflow will go something like this:\n\nYou create/delete and edit files in your project directory on your computer as usual (saving these changes as you go)\nOnce you‚Äôve reached a natural ‚Äòbreak point‚Äô in your progress (i.e. you‚Äôd be sad if you lost this progress) you stage these files\nYou then commit the changes you made to these staged files (along with a useful commit message) which creates a permanent snapshot of these changes\nYou keep on with this cycle until you get to a point when you would like to push these changes to GitHub\nIf you‚Äôre working with other people on the same project you may also need to pull their changes to your local computer\n\n\n\n\n\n\n\n\nFigure¬†7.55\n\n\n\n\nOK, let‚Äôs go through an example to help clarify this workflow.\nTracking changes\nCommit History\nReverting changes",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-collab",
    "href": "07-github.html#sec-collab",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.8 Collaborate with Git",
    "text": "7.8 Collaborate with Git\nGitHub is a great tool for collaboration, it can seem scary and complicated at first, but it is worth investing some time to learn how it works. What makes GitHub so good for collaboration is that it is a distributed system, which means that every collaborator works on their own copy of the project and changes are then merged together in the remote repository. There are two main ways you can set up a collaborative project on GitHub. One is the workflow we went through above, where everybody connects their local repository to the same remote one; this system works well with small projects where different people mainly work on different aspects of the project but can quickly become unwieldy if many people are collaborating and are working on the same files (merge misery!). The second approach consists of every collaborator creating a copy (or fork) of the main repository, which becomes their remote repository. Every collaborator then needs to send a request (a pull request) to the owner of the main repository to incorporate any changes into the main repository and this includes a review process before the changes are integrated. More detail of these topics can be found in Section 7.10.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#git-tips",
    "href": "07-github.html#git-tips",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.9 Git tips",
    "text": "7.9 Git tips\nGenerally speaking you should commit often (including amended commits) but push much less often. This makes collaboration easier and also makes the process of reverting to previous versions of documents much more straight forward. We generally only push changes to GitHub when we‚Äôre happy for our collaborators (or the rest of the world) to see our work. However, this is entirely up to you and depends on the project (and who you are working with) and what your priorities are when using Git.\nIf you don‚Äôt want to track a file in your repository (maybe they are too large or transient files) you can get Git to ignore the file by adding it to the .gitignore file. On RStudio, in the git pane, you can right clicking on the filename to exclude and selecting ‚ÄòIgnore‚Ä¶‚Äô\n\n\n\n\n\n\n\nFigure¬†7.56\n\n\n\n\nThis will add the filename to the .gitignore file. If you want to ignore multiple files or a particular type of file you can also include wildcards in the .gitignore file. For example to ignore all png files you can include the expression *.png in your .gitignore file and save.\nIf it all goes pear shaped and you end up completely trashing your Git repository don‚Äôt despair (we‚Äôve all been there!). As long as your GitHub repository is good, all you need to do is delete the offending project directory on your computer, create a new RStudio project and link this with your remote GitHub repository using Option 2 (-Section 7.5.3). Once you have cloned the remote repository you should be good to go.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-resources",
    "href": "07-github.html#sec-resources",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.10 Further resources",
    "text": "7.10 Further resources\nThere are many good online guides to learn more about git and GitHub and as with any open source software there is a huge community that can be a great resource:\n\nThe British Ecological Society guide to Reproducible Code\nThe GitHub guides\nThe Mozilla Science Lab GitHub for Collaboration on Open Projects guide\nJenny Bryan‚Äôs Happy Git and GitHub. We borrowed the idea (but with different content) of RStudio first, RStudio second in the ‚ÄòSetting up a version controlled Project in RStudio‚Äô section.\nMelanie Frazier‚Äôs GitHub: A beginner‚Äôs guide to going back in time (aka fixing mistakes). We followed this structure (with modifications and different content) in the ‚ÄòReverting changes‚Äô section.\nIf you have done something terribly wrong and don‚Äôt know how to fix it try Oh Shit, Git or if you‚Äôre easily offended Dangit, Git\n\nThese are only a couple of examples, all you need to do is search for ‚Äúversion control with git and GitHub‚Äù to see how huge the community around these open source projects is and how many free resources are available for you to become a version control expert.",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#git_practical",
    "href": "07-github.html#git_practical",
    "title": "7¬† Version control with Git and GitHub",
    "section": "\n7.11 Practical",
    "text": "7.11 Practical\n\n7.11.1 Context\nWe will configure Rstudio to work with our github account, then create a new project and start using github. To have some data I suggest to use the awesome palmerpenguins dataset üêß.\n\n7.11.2 Information of the data\nThese data have been collected and shared by Dr.¬†Kristen Gorman and Palmer Station, Antarctica LTER.\nThe package was built by Drs Allison Horst and Alison Hill, check out the official website.\nThe package palmerpenguins has two datasets.\n\nlibrary(palmerpenguins)\n\nThe dataset penguins is a simplified version of the raw data; see ?penguins for more info:\n\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe other dataset penguins_raw has the raw data; see ?penguins_raw for more info:\n\nhead(penguins_raw)\n\n# A tibble: 6 √ó 17\n  studyName `Sample Number` Species          Region Island Stage `Individual ID`\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          \n1 PAL0708                 1 Adelie Penguin ‚Ä¶ Anvers Torge‚Ä¶ Adul‚Ä¶ N1A1           \n2 PAL0708                 2 Adelie Penguin ‚Ä¶ Anvers Torge‚Ä¶ Adul‚Ä¶ N1A2           \n3 PAL0708                 3 Adelie Penguin ‚Ä¶ Anvers Torge‚Ä¶ Adul‚Ä¶ N2A1           \n4 PAL0708                 4 Adelie Penguin ‚Ä¶ Anvers Torge‚Ä¶ Adul‚Ä¶ N2A2           \n5 PAL0708                 5 Adelie Penguin ‚Ä¶ Anvers Torge‚Ä¶ Adul‚Ä¶ N3A1           \n6 PAL0708                 6 Adelie Penguin ‚Ä¶ Anvers Torge‚Ä¶ Adul‚Ä¶ N3A2           \n# ‚Ñπ 10 more variables: `Clutch Completion` &lt;chr&gt;, `Date Egg` &lt;date&gt;,\n#   `Culmen Length (mm)` &lt;dbl&gt;, `Culmen Depth (mm)` &lt;dbl&gt;,\n#   `Flipper Length (mm)` &lt;dbl&gt;, `Body Mass (g)` &lt;dbl&gt;, Sex &lt;chr&gt;,\n#   `Delta 15 N (o/oo)` &lt;dbl&gt;, `Delta 13 C (o/oo)` &lt;dbl&gt;, Comments &lt;chr&gt;\n\n\nFor this exercise, we‚Äôre gonna use the penguins dataset.\n\n7.11.3 Questions\n1) Create a github account if not done yet.\n2) Configure Rstudio with your github account using the usethis package.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nusethis::git_sitrep()\nusethis::use_git_config(\n  user.name = \"your_username\",\n  user.email = \"your_email@address.com\"\n)\n\n\n\n\n3) Create and Store your GITHUB Personal Authorisation Token\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nusethis::create_github_token()\ngitcreds::gitcreds_set()\n\n\n\n\n4) Create a new R Markdown project, initialize it for git, and create a new git repository\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#create R project\nusethis::use_git()\n\n#restart R\nusethis::use_github()\nusethis::git_vaccinate()\n\n\n\n\n5) Create a new Rmarkdown document, in your project. Then save the file and stage it.\n6) Create a new commit including the new file and push it to github (Check on github that it works).\n7) Edit the file. Delete everything after line 12. Add a new section title, simple text and text in bold font. Then knit and compile.\n8) Make a new commit (with a meaningful message), and push to github.\n9) Create a new branch, and add a new section to the rmarkdown file in this branch. Whatever you want. I would suggest a graph of the data.\n10) Create a commit and push it to the branch.\n11) On github, create a pull request to merge the 2 different branches.\n12) Check and accept the pull request to merge the 2 branches.\nYou have successfully used all the essential tools of git üéâ . You are really to explore üïµÔ∏è and discover its power üí™\n\n\n\n\nHappy git(hub)-ing\n\n\n\n\n7.11.4 Solution\n2)\n\nusethis::git_sitrep()\nusethis::use_git_config(\n  user.name = \"your_username\",\n  user.email = \"your_email@address.com\"\n)\n\n3)\n\nusethis::create_github_token()\ngitcreds::gitcreds_set()\n\n4)\n\n#create R project\nusethis::use_git()\n\n#restart R\nusethis::use_github()\nusethis::git_vaccinate()",
    "crumbs": [
      "Data & Code",
      "Using R",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "25-power.html",
    "href": "25-power.html",
    "title": "\n8¬† Power Analysis\n",
    "section": "",
    "text": "8.1 The theory",
    "crumbs": [
      "Data & Code",
      "Fundamentals of stats",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "25-power.html#the-theory",
    "href": "25-power.html#the-theory",
    "title": "\n8¬† Power Analysis\n",
    "section": "",
    "text": "8.1.1 What is power?\nPower is the probability of rejecting the null hypothesis when it is false\n\n8.1.2 Why do a power analysis?\nAssess the strength of evidence\nPower analysis, performed after accepting a null hypothesis, can help assess the probability of rejecting the null if it were false, and if the magnitude of the effect was equal to that observed (or to any other given magnitude). This type of a posteriori analysis is very common.\nDesign better experiments\nPower analysis, performed prior to conducting an experiment (but most often after a preliminary experiment), can be used to determine the number of observations required to detect an effect of a given magnitude with some probability (the power). This type of a priori experiment should be more common.\nEstimate minimum detectable effect\nSampling effort is often predetermined (when you are handed data of an experiment already completed), or extremely constrained (when logistics dictates what can be done). Whether it is a priori or a posteriori, power analysis can help you estimate, for a fixed sample size and a given power, what is the minimum effect size that can be detected.\n\n8.1.3 Factors affecting power\nFor a given statistical test, there are 3 factors that affect power.\nDecision criteria\nPower is related to \\(\\alpha\\), the probability level at which one rejects the null hypothesis. If this decision criteria is made very strict (i.e.¬†if critical \\(\\alpha\\) is set to a very low value, like 0.1% or \\(p = 0.001\\)), then power will be lower than if the critical \\(\\alpha\\) was less strict.\nSample size\nThe larger the sample size, the larger the power. As sample size increases, one‚Äôs ability to detect small effect sizes as being statistically significant gets better.\nEffect size\nThe larger the effect size, the larger the power. For a given sample size, the ability to detect an effect as being significant is higher for large effects than for small ones. Effect size measures how false the null hypothesis is.\n\n8.1.4 Types of power analyses\nFirst, \\(\\alpha\\) is define as the probability level at which one rejects the null hypothesis, and \\(\\beta\\) is \\(1 - power\\).\nA priori\nComputes the sample size required given \\(\\beta\\), \\(\\alpha\\), and the effect size. This type of analysis is useful when planning experiments.\nCompromise\nComputes \\(\\alpha\\) and \\(\\beta\\) for a given \\(\\alpha\\)/\\(\\beta\\) ratio, sample size, and effect size. Less commonly used (I have never used it myself) although it can be useful when the \\(\\alpha\\)/\\(\\beta\\) ratio has meaning, for example when the cost of type I and type II errors can be quantified.\nCriterion\nComputes \\(\\alpha\\) for a given \\(\\beta\\), sample size, and effect size. In practice, I see little interest in this. Let me know if you see something I don‚Äôt!\nPost-hoc\nComputes the power for a given \\(\\alpha\\), effect size, and sample size. Used frequently to help in the interpretation of a test that is not statistically significant, but only if an effect size that is biologically significant is used (and not the observed effect size). Not relevant when the test is significant.\nSensitivity\nComputes the detectable effect size for a given \\(\\beta\\) ,\\(\\alpha\\), and sample size. Very useful at the planning stage of an experiment.\n\n8.1.5 How to calculate effect size\nThe metric for effect size depends on the test. Note that other software packages often use different effect size metrics and that it is important to use the correct one for each package. G*Power, a software to easily perform power analysis, has an effect size calculator for many tests that only requires you to enter the relevant values. The following table lists the effect size metrics used by G*Power for the various tests.\n\n\n\n\n\n\n\nTest\nEffect size\nFormula\n\n\n\nt-test on means\nd\n\\(d = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{({s_1}^2 + {s_2}^2)/2}}\\)\n\n\nt-test on correlations\nr\n\n\n\nother t-tests\nd\n\\(d = \\frac{\\mu}{\\sigma}\\)\n\n\nF-test (ANOVA)\nf\n\\(f = \\frac{\\frac{\\sqrt{\\sum_{i=1}^k (\\mu_i - \\mu)^2}}{k}}{\\sigma}\\)\n\n\nother F-tests\n\\(f^2\\)\n\\(f^2 = \\frac{{R_p}^2}{1-{R_p}^2}\\)\n\n\n\n\n\n\\({R_p}\\) is the squared partial correlation coefficient\n\n\nChi-square test\nw\n\\(w = \\sqrt{ \\sum_{i=1}^m \\frac{(p_{0i} - p_{1i})^2}{ p_{0i}}}\\)\n\n\n\n\n\n\\(p_{0i}\\) and \\(p_{1i}\\) are the proportion in category \\(i\\) predicted by the null, \\(_0\\), and alternative, \\(_1\\), hypothesis",
    "crumbs": [
      "Data & Code",
      "Fundamentals of stats",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "25-power.html#practical",
    "href": "25-power.html#practical",
    "title": "\n8¬† Power Analysis\n",
    "section": "\n8.2 Practical",
    "text": "8.2 Practical\nAfter completing this laboratory, you should :\n\nbe able to compute the power of a t-test with G*Power and R\nbe able to calculate the required sample size to achieve a desired power level with a t-test\nbe able to calculate the detectable effect size by a t-test given the sample size, the power and \\(\\alpha\\)\n\nunderstand how power changes when sample size increases, the effect size changes, or when \\(\\alpha\\) decreases\nunderstand how power is affected when you change from a two-tailed to a one-tailed test.\n\n\n8.2.1 What is G*Power?\nG*Power is free software developed by quantitative psychologists from the University of Dusseldorf in Germany. It is available in MacOS and Windows versions. It can be run under Linux using Wine or a virtual machine.\nG*Power will allow you to do power analyses for the majority of statistical tests we will cover during the term without making lengthy calculations and looking up long tables and figures of power curves. It is a really useful tool that you need to master.\nIt is possible to perform all analysis made by G*Power in R, but it requires a bit more code, and a better understanding of the process since everything should be coded by hand. In simple cases, R code is also provided.\n\n\n\n\n\n\nCaution\n\n\n\nDownload the software here and install it on your computer and your workstation (if it is not there already).\n\n\n\n8.2.2 How to use G*Power\n\n8.2.2.1 General Principle\nUsing G*Power generally involves 3 steps:\n\nChoosing the appropriate test\nChoosing one of the 5 types of available power analyses\nEnter parameter values and press the Calculate button\n\n8.2.3 Power analysis for a t-test on two independent means\n\n\n\n\n\n\nImportant\n\n\n\nAll the power analysis presented in this chapter can be done using 2 functions in R.\n\n\npwr.t.test() when the two samples have a similar size\n\npwr.t2n.test() when samples have different numbers of observations\n\n\n\nThe objective of this lab is to learn to use G*Power and understand how the 4 parameters of power analyses (\\(\\alpha\\), \\(\\beta\\), sample size and effect size) are related to each other. For this, you will only use the standard t-test to compare two independent means. This is the test most used by biologists, you have all used it, and it will serve admirably for this lab. What you will learn today will be applicable to all other power analyses.\nJaynie Stephenson studied the productivity of streams in the Ottawa region. She has measured fish biomass in 36 streams, 18 on the Shield and 18 in the Ottawa Valley. She found that fish biomass was lower in streams from the valley (2.64 \\(g/m^2\\) , standard deviation = 3.28) than from the Shield (3.31 \\(g/m^2\\) , standard deviation = 2.79.).\nWhen she tested the null hypothesis that fish biomass is the same in the two regions by a t-test, she obtained:\nPooled-Variance Two-Sample t-Test\nt = -0.5746, df = 34, p-value = 0.5693\nShe therefore accepted the null hypothesis (since p is much larger than 0.05) and concluded that fish biomass is the same in the two regions.\n\n8.2.4 Post-hoc analysis\nUsing the observed means and standard deviations, we can use G*Power to calculate the power of the two-tailed t-test for two independent means, using the observed effect size (the difference between the two means, weighted by the standard deviations) for \\(\\alpha\\) = 0.05.\nStart G*Power.\n\nIn *Test family , choose: t tests\nFor Statistical test , choose: Means: Difference between two independent means (two groups)\nFor Type of power analysis , choose: Post hoc: Compute achieved power - given \\(\\alpha\\), sample size, and effect size\nAt Input Parameters ,\n\n\nin the box Tail(s) , chose: Two,\ncheck that \\(\\alpha\\) err prob is equal to 0.05\nEnter 18 for the Sample size of group 1 and of group 2\nthen, to calculate effect size (d), click on Determine =&gt;\n\n\n\nIn the window that opens,\n\n\nselect n1 = n2 , then\nenter the two means (Mean group 1 et 2)\nthe two standard deviations(SD group 1 et 2)\nclick on Calculate and transfer to main window\n\n\n\nAfter you click on the Calculate button in the main window, you should get the following:\n\n\n\n\n\n\n\n\nFigure¬†8.1: Post-hoc analysis with estimated effect size\n\n\n\n\nSimilar analysis can be done in R. You first need to calculate the effect size d for a t-test comparing 2 means, and then use the pwr.t.test() function from the pwr üì¶. The easiest is to create anew function in R to estimate the effect size dsince we are going to reuse it multiple times during the lab.\n\n# load package pwr\nlibrary(pwr)\n# define d for a 2 sample t-test\nd &lt;- function(u1, u2, sd1, sd2) {\n  abs(u1 - u2) / sqrt((sd1^2 + sd2^2) / 2)\n}\n\n# power analysis\npwr.t.test(\n  n = 18,\n  d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79),\n  sig.level = 0.05,\n  type = \"two.sample\"\n)\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.09833902\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n# plot similar to G*Power\nd_cohen &lt;- d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79)\n\nx &lt;- seq(-4, 4, length = 200)\ny0 &lt;- dt(x, 34)\ny1 &lt;- dt(x, 34, ncp = d_cohen * sqrt(36) / 2)\nplot(x, y0, type = \"l\", col = \"red\", lwd = 2)\nqc &lt;- qt(0.025, 34)\nabline(v = qc, col = \"green\")\nabline(v = -qc, col = \"green\")\nlines(x, y1, type = \"l\", col = \"blue\", lwd = 2)\n\n# type 2 error corresponds to the shaded area\n\npolygon(\n  c(x[x &lt;= -qc], -qc), c(y1[x &lt;= -qc], 0),\n  col = rgb(red = 0, green = 0.2, blue = 1, alpha = 0.5)\n)\n\n\n\n\n\n\nFigure¬†8.2: Post-hoc analysis with estimated effect size in R\n\n\n\n\n\nqc &lt;- qt(0.025, 34)\nncp &lt;- d_cohen * sqrt(36) / 2\ndat &lt;- data.frame(\n  x = seq(-4, 4, length = 200),\n  y0 = dt(x, (n - 1) * 2),\n  y1 = dt(x, (n - 1) * 2, ncp = ncp)\n) %&gt;%\n  mutate(\n    area = ifelse(x &lt;= qc, y1, 0)\n  )\n\nggplot(dat, aes(x = x)) +\n  geom_line(aes(y = y0), color = \"red\") +\n  geom_line(aes(y = y1), color = \"blue\") +\n  geom_vline(xintercept = qcl, color = \"green\") +\n  geom_area(\n    aes(x = x, y = area),\n    fill = rgb(red = 0, green = 0.2, blue = 1, alpha = 0.5)\n  ) +\n  theme_classic() +\n  ylab(\"dt(x)\")\n\nLet‚Äôs examine the figure Figure¬†8.2.\n\nThe curve on the left, in red, corresponds to the expected distribution of the t-statistics when \\(H_0\\) is true (i.e. when the two means are equal) given the sample size (18 per region) and the observed standard deviations.\nThe vertical green lines correspond to the critical values of t for \\(\\alpha = 0.05\\) and a total sample size of 36 (2x18).\nThe shaded pink regions correspond to the rejection zones for \\(H_0\\). If Jaynie had obtained a t-value outside the interval delimited by the critical values ranging from -2.03224 to 2.03224, she would then have rejected \\(H_0\\) , the null hypothesis of equal means. In fact, she obtained a t-value of -0.5746 and concluded that the biomass is equal in the two regions.\nThe curve on the right, in blue, corresponds to the expected distribution of the t-statistics if \\(H_1\\) is true (here \\(H_1\\) is that there is a difference in biomass between the two regions equal to \\(3.33 - 2.64 = 0.69 g/m^2\\) , given the observed standard deviations). This distribution is what we should observe if \\(H_1\\) was true and we repeated a large number of times the experiment using random samples of 18 streams in each of the two regions and calculated a t-statistic for each sample. On average, we would obtain a t-statistic of about 0.6.\nNote that there is considerable overlap of the two distributions and that a large fraction of the surface under the right curve is within the interval where \\(H_0\\) is accepted between the two vertical green lines at -2.03224 and 2.03224. This proportion, shaded in blue under the distribution on the right is labeled \\(\\beta\\) and corresponds to the risk of type II error (accept \\(H_0\\) when \\(H_1\\) is true).\nPower is simply \\(1-\\beta\\), and is here 0.098339. Therefore, if the mean biomass differed by \\(0.69 g/m^2\\) between the two regions, Jaynie had only 9.8% chance of being able to detect it as a statistically significant difference at ÔÅ°=5% with a sample size of 18 streams in each region.\n\nLet‚Äôs recapitulate: The difference in biomass between regions is not statistically significant according to the t-test. It is because the difference is relatively small relative to the precision of the measurements. It is therefore not surprising that that power, i.e.¬†the probability of detecting a statistically significant difference, is small. Therefore, this analysis is not very informative.\nIndeed, a post hoc power analysis using the observed effect size is not useful. It is much more informative to conduct a post hoc power analysis for an effect size that is different from the observed effect size. But what effect size to use? It is the biology of the system under study that will guide you. For example, with respect to fish biomass in streams, one could argue that a two fold change in biomass (say from 2.64 to 5.28 g/m2 ) has ecologically significant repercussions. We would therefore want to know if Jaynie had a good chance of detecting a difference as large as this before accepting her conclusion that the biomass is the same in the two regions. So, what were the odds that Jaynie could detect a difference of 2.64 g/m2 between the two regions? G*Power can tell you if you cajole it the right way.\n\n\n\n\n\n\nExercise\n\n\n\nChange the mean of group 2 to 5.28, recalculate effect size, and click on Calculate to obtain figure Figure¬†8.3.\n\n\n\n\n\n\n\n\n\nFigure¬†8.3: Post-hoc analysis using an effect size different from the one estimated\n\n\n\n\nSame analysis using R (without all the code for the interesting but not really useful plot)\n\npwr.t.test(\n  n = 18,\n  d = d(u1 = 2.64, sd1 = 3.28, u2 = 5.28, sd2 = 2.79),\n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.8670313\n      sig.level = 0.05\n          power = 0.7146763\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\n\nFigure¬†8.4\n\n\n\n\nThe power is 0.71, therefore Jaynie had a reasonable chance (71%) of detecting a doubling of biomass with 18 streams in each region.\nNote that this post hoc power analysis, done for an effect size considered biologically meaningful, is much more informative than the preceeding one done with the observed effect size (which is what too many students do because it is the default of so many power calculation programs). Jaynie did not detect a difference between the two regions. There are two possibilities: 1) there is really no difference between the regions, or 2) the precision of measurements is so low (because the sample size is small and/or there is large variability within a region) that it is very unlikely to be able to detect even large differences. The second power analysis can eliminate this second possibility because Jaynie had 71% chances of detecting a doubling of biomass.\n\n8.2.5 A priori analysis\nSuppose that a difference in biomass of \\(3.31-2.64 = 0.67 g/m^2\\) can be ecologically significant. The next field season should be planned so that Jaynie would have a good chance of detecting such a difference in fish biomass between regions. How many streams should Jaynie sample in each region to have 80% of detecting such a difference (given the observed variability)?\n\n\n\n\n\n\nExercise\n\n\n\nChange the type of power analysis in G*Power to A priori: Compute sample size - given \\(\\alpha\\) , power, and effect size. Ensure that the values for means and standard deviations are those obtained by Jaynie. Recalculate the effect size metric and enter 0.8 for power and you will obtain Figure¬†8.5.\n\n\n\n\n\n\n\n\n\nFigure¬†8.5: A priori analysis\n\n\n\n\n\npwr.t.test(\n  power = 0.8,\n  d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79),\n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 325.1723\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOuch! The required sample would be of 326 streams in each region! It would cost a fortune and require several field teams otherwise only a few dozen streams could be sampled over the summer and it would be very unlikely that such a small difference in biomass could be detected. Sampling fewer streams would probably be in vain and could be considered as a waste of effort and time: why do the work on several dozens of streams if the odds of success are that low?\nIf we recalculate for a power of 95%, we find that 538 streams would be required from each region. Increasing power means more work!\n\npwr.t.test(\n  power = 0.95,\n  d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79),\n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 537.7286\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n8.2.6 Sensitivity analysis - Calculate the detectable effect size\nGiven the observed variability, a sampling effort of 18 streams per region, and with \\(\\alpha\\) = 0.05, what effect size could Jaynie detect with 80% probability \\(\\beta=0.2\\)?\n\n\n\n\n\n\nExercise\n\n\n\nChange analysis type in G*Power to Sensitivity: Compute required effect size - given \\(\\alpha\\) , power, and sample size and size is 18 in each region.\n\n\n\n\n\n\n\n\n\nFigure¬†8.6: Analyse de sensitivit√©\n\n\n\n\n\npwr.t.test(\n  power = 0.8,\n  n = 18,\n  sig.level = 0.05,\n  type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.9612854\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe detectable effect size for this sample size, \\(\\alpha = 0.05\\) and \\(\\beta = 0.2\\) (or power of 80%) is 0.961296.\n\n\n\n\n\n\nWarning\n\n\n\nAttention, this effect size is the metric d and is dependent on sampling variability.\n\n\nHere, d is approximately equal to\n\\[ d = \\frac{| \\bar{X_1} \\bar{X_2} |} {\\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}}\\]\nTo convert this d value without units into a value for the detectable difference in biomass between the two regions, you need to multiply d by the denominator of the equation.\n\\[\n| \\bar{X_1} - \\bar{X_2} | = d * \\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}\n\\]\nIn R this can be done with the following code\n\npwr.t.test(\n  power = 0.8,\n  n = 18,\n  sig.level = 0.05,\n  type = \"two.sample\")$d * sqrt((3.28^2 + 2.79^2) / 2)\n\n[1] 2.926992\n\n\nTherefore, with 18 streams per region, \\(\\alpha\\) = 0.05 and \\(\\beta\\) = 0.2 (so power of 80%), Jaynie could detect a difference of 2.93 g/m2 between regions, a bit more than a doubling of biomass.",
    "crumbs": [
      "Data & Code",
      "Fundamentals of stats",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "25-power.html#important-points-to-remember",
    "href": "25-power.html#important-points-to-remember",
    "title": "\n8¬† Power Analysis\n",
    "section": "\n8.3 Important points to remember",
    "text": "8.3 Important points to remember\n\nPost hoc power analyses are relevant only when the null hypothesis is accepted because it is impossible to make a type II error when rejecting \\(H_0\\) .\nWith very large samples, power is very high and minute differences can be statistically detected, even if they are not biologically significant.\nWhen using a stricter significance criteria (\\(\\alpha &lt; 0.05\\)) power is reduced.\nMaximizing power implies more sampling effort, unless you use a more liberal statistical criteria (\\(\\alpha &gt; 0.05\\))\nThe choice of \\(\\beta\\) is somewhat arbitrary. \\(\\beta=0.2\\) (power of 80%) is considered relatively high by most.",
    "crumbs": [
      "Data & Code",
      "Fundamentals of stats",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Power Analysis</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html",
    "href": "31-reg_lin.html",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "",
    "text": "9.1 R packages and data\nFor this la b you need:\nYou need to load the packages in R with library() and if need needed install them first with install.packages() For the data, load them using the read.csv() function.\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(performance)\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\nlibrary(ggplot2)\nlibrary(pwr)\n\nsturgeon &lt;- read.csv(\"data/sturgeon.csv\")",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#set-lm",
    "href": "31-reg_lin.html#set-lm",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "",
    "text": "R packages:\n\ncar\nlmtest\nboot\npwr\nggplot\nperformance\n\n\ndata:\n\nsturgeon.csv\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the command to read the data assumes that the data file is in a folder named data within the working directory. Adjust as needed.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#scatter-plots",
    "href": "31-reg_lin.html#scatter-plots",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.2 Scatter plots",
    "text": "9.2 Scatter plots\nCorrelation and regression analysis should always begin with an examination of the data: this is a critical first step in determining whether such analyses are even appropriate for your data. Suppose we are interested in the extent to which length of male sturgeon in the vicinity of The Pas and Cumberland House covaries with weight. To address this question, we look at the correlation between fklngth and rdwght. Recall that one of the assumptions in correlation analysis is that the relationship between the two variables is linear. To evaluate this assumption, a good first step is to produce a scatterplot.\n\nLoad the data from sturgeon.csv in an obk=jcet named sturgeon. Make a scatter plot of rdwght vs fklngth fit with a locally weighted regression (Loess) smoother, and a linear regression line.\n\n\nsturgeon &lt;- read.csv(\"data/sturgeon.csv\")\nstr(sturgeon)\n\n'data.frame':   186 obs. of  9 variables:\n $ fklngth : num  37 50.2 28.9 50.2 45.6 ...\n $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...\n $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...\n $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...\n $ age     : int  11 24 7 23 20 23 20 7 23 19 ...\n $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...\n $ sex     : chr  \"MALE\" \"FEMALE\" \"MALE\" \"FEMALE\" ...\n $ location: chr  \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" ...\n $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...\n\n\n\nmygraph &lt;- ggplot(\n  data = sturgeon[!is.na(sturgeon$rdwght), ], # source of data\n  aes(x = fklngth, y = rdwght)\n)\n# plot data points, regression, loess trace\nmygraph &lt;- mygraph +\n  stat_smooth(method = lm, se = FALSE, color = \"green\") + # add linear regression, but no SE shading\n  stat_smooth(color = \"red\", se = FALSE) + # add loess\n  geom_point() # add data points\n\nmygraph # display graph\n\n\n\n\n\n\nFigure¬†9.1: Scatter plot of Weight as a function of length in sturgeons\n\n\n\n\n\n\nDoes this curve suggest a good correlation between the two? Based on visual inspection, does the relationship between these two variables appear linear?\n\nThere is some evidence of nonlinearity, as the curve appears to have a positive second derivative (concave up). This notwithstanding, it does appear the two variables are highly correlated.\n\nRedo the scatterplot, but after logtransformation of both axes.\n\n\n# apply log transformation on defined graph\nmygraph + scale_x_log10() + scale_y_log10()\n\n\n\n\n\n\nFigure¬†9.2: Plot weight-length in sturgeon using a log scale\n\n\n\n\nCompare the diagrams before and after the transformation (Figs Figure¬†9.1 and Figure¬†9.2). Since the relationship is more linear after transformation, correlation analysis should be done on the transformed data",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#data-transformations-and-the-product-moment-correlation",
    "href": "31-reg_lin.html#data-transformations-and-the-product-moment-correlation",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.3 Data transformations and the product-moment correlation",
    "text": "9.3 Data transformations and the product-moment correlation\nRecall that another assumption underlying significance testing of the product-moment correlation is that the distribution of the two variables in question is bivariate normal. We can test to see whether each of the two variables are normally distributed using the same procedures outlined in the exercise on two-sample comparisons. If the two variables are each normally distributed, then one is usually (relatively) safe in assuming the joint distribution is normal, although this needn‚Äôt necessarily be true.\n\nExamine the distribution of the 4 variables (the two original variables and the log-transformed variables). What do you conclude from visual inspection of these plots?\n\nThe following graph contains the 4 QQ plots (qqplot()). It was produced by the code below that starts with the par() command to ensure that all 4 plots would appear together on the same page in 2 rows and 2 columns:\n\npar(mfrow = c(2, 2)) # split graph in 4 (2 rows, 2 cols) filling by rows\nqqnorm(sturgeon$fklngth, ylab = \"fklngth\")\nqqline(sturgeon$fklngth)\nqqnorm(log10(sturgeon$fklngth), ylab = \"log10(fklngth)\")\nqqline(log10(sturgeon$fklngth))\nqqnorm(sturgeon$rdwght, ylab = \"rdwght\")\nqqline(sturgeon$rdwght)\nqqnorm(log10(sturgeon$rdwght), ylab = \"log10(rdwgth)\")\nqqline(log10(sturgeon$rdwght))\npar(mfrow = c(1, 1)) # redefine plotting area to 1 plot\n\n\n\n\n\n\nFigure¬†9.3\n\n\n\n\nNone of these distributions are perfectly normal, but deviations are mostly minor.\n\nTo generate a scatterplot matrix of all pairs of variables, with linear regression and lowess traces, you can use scatterplotMatrix from car üì¶.\n\n\nscatterplotMatrix(\n  ~ fklngth + log10(fklngth) + rdwght + log10(rdwght),\n  data = sturgeon,\n  smooth = TRUE, diagonal = \"density\"\n)\n\n\n\n\n\n\nFigure¬†9.4\n\n\n\n\n\nNext, calculate the Pearson product-moment correlation between each pair (untransformed and log transformed) using the cor() command. However, to do this, it will be easier if you first add your transformed data as columns in the sturgeon data frame.\n\n\nsturgeon$lfklngth &lt;- with(sturgeon, log10(fklngth))\nsturgeon$lrdwght &lt;- log10(sturgeon$rdwght)\n\nThen you can get the correlation matrix by:\n\ncor(sturgeon[, c(\"fklngth\", \"lfklngth\", \"lrdwght\", \"rdwght\")], use = \"complete.obs\")\n\nNote the use=\"complete.obs\" parameter. It tells R to keep only lines of the data frame where all variables were measured. If there are missing data, some lines will be removed, but correlations will be calculated for the same subset of cases for all pairs of variables. One could use, instead, use=\"pairwise.complete.obs\" , to tell R to only eliminate observations when values are missing for this particular pair of variables. In this situation, if there are missing values in the data frame, the sample size for pairwise correlations will vary. In general, I recommend you use the option use=\"complete.obs\", unless you have so many missing values that it eliminates the majority of your data.\n\nWhy is the correlation between the untransformed variables smaller than between the transformed variables?\n\n\ncor(sturgeon[, c(\"fklngth\", \"lfklngth\", \"lrdwght\", \"rdwght\")], use = \"complete.obs\")\n\n           fklngth  lfklngth   lrdwght    rdwght\nfklngth  1.0000000 0.9921435 0.9645108 0.9175435\nlfklngth 0.9921435 1.0000000 0.9670139 0.8756203\nlrdwght  0.9645108 0.9670139 1.0000000 0.9265513\nrdwght   0.9175435 0.8756203 0.9265513 1.0000000\n\n\nSeveral things should be noted here.\n\nthe correlation between fork length and round weight is high, regardless of which variables are used: so as might be expected, heavier fish are also longer, and vice versa\nthe correlation is greater for the transformed variables than the untransformed variables.\n\nWhy? Because the correlation coefficient is inversely proportional to the amount of scatter around a straight line. If the relationship is curvilinear (as it is for the untransformed data), the scatter around a straight line will be greater than if the relationship is linear. Hence, the correlation coefficient will be smaller.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#testing-the-significance-of-correlations-and-bonferroni-probabilities",
    "href": "31-reg_lin.html#testing-the-significance-of-correlations-and-bonferroni-probabilities",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.4 Testing the significance of correlations and Bonferroni probabilities",
    "text": "9.4 Testing the significance of correlations and Bonferroni probabilities\nIt‚Äôs possible to test the significance of individual correlations using the commands window. As an example, let‚Äôs try testing the significance of the correlation between lfklngth and rdwght (the smallest correlation in the above table).\n\nIn the R script window, enter the following to test the correlation between lfkgnth and rdwght :\n\n\ncor.test(\n  sturgeon$lfklngth, sturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"pearson\"\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  sturgeon$lfklngth and sturgeon$rdwght\nt = 24.322, df = 180, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8367345 0.9057199\nsample estimates:\n      cor \n0.8756203 \n\n\nWe see here that the correlation is highly significant (\\(p&lt; 2.2e-16\\)), which is no surprise given how high the correlation coefficient is (0.8756).\nIt‚Äôs important to bear in mind that when you are estimating correlations, the probability of finding any one correlation that is ‚Äúsignificant‚Äù by pure chance increases with the number of pairwise correlations examined. Suppose, for example, that you have five variables; there are then a total of 10 possible pairwise correlations, and from this set, you would probably not be surprised to find at least one that is ‚Äúsignificant‚Äù purely by chance. One way of avoiding the problem is to adjust individual \\(\\alpha\\) levels for pairwise correlations by dividing by the number of comparisons, k, such that: \\(\\alpha' = \\frac{\\alpha}{k}\\) (Bonferroni probabilities), i.e.¬†if initially, \\(\\alpha = 0.05\\) and there are a total of 10 comparisons, then \\(\\alpha'= 0.005\\).\nIn the above example where we examined correlations between fklngth and rdwght and their log, it would be appropriate to adjust the \\(\\alpha\\) at which significance is tested by the total number of correlations in the matrix (in this case, 6, so \\(\\alpha'=0.0083\\)). Does your decision about the significance of the correlation between lfklngth and rdwght change?",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#non-parametric-correlations-spearmans-rank-and-kendalls-tau",
    "href": "31-reg_lin.html#non-parametric-correlations-spearmans-rank-and-kendalls-tau",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.5 Non-parametric correlations: Spearman‚Äôs rank and Kendall‚Äôs \\(\\tau\\)\n",
    "text": "9.5 Non-parametric correlations: Spearman‚Äôs rank and Kendall‚Äôs \\(\\tau\\)\n\nThe analysis done with the sturgeon data in the section above suggests that one of the assumptions of correlation, namely, bivariate normality, may not be valid for fklngth and rdwght nor for the log transforms of these variables. Finding an appropriate transformation is sometimes like looking for a needle in a haystack; indeed, it can be much worse simply because for some distributions, there is no transformation that will normalize the data. In such cases, the best option may be to go to a non-parametric analysis that does not assume bivariate normality or linearity. All such correlations are based on the ranks rather than the data themselves: two options available in R are Spearman and Kendall‚Äôs \\(\\tau\\) (tau).\n\nTest the correlation between fklngth and rdwght using both the Spearman and Kendall‚Äôs tau. The following commands will produce the correlations:\n\n\ncor.test(\n  sturgeon$lfklngth, sturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"spearman\"\n)\n\n\n    Spearman's rank correlation rho\n\ndata:  sturgeon$lfklngth and sturgeon$rdwght\nS = 47971, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9522546 \n\n\n\ncor.test(\n  sturgeon$lfklngth, sturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"kendall\"\n)\n\n\n    Kendall's rank correlation tau\n\ndata:  sturgeon$lfklngth and sturgeon$rdwght\nz = 16.358, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.8208065 \n\n\nContrast these results with those obtained using the Pearson product-moment correlation. Why the difference?\nTest the non-parametric correlations on pairs of the transformed variables. You should immediately note that the non-parametric correlations are identical for untransformed and transformed variables. This is because we are using the ranks, rather than the raw data, and the rank ordering of the data does not change when a transformation is applied to the raw values.\nNote that the correlations for Kendall‚Äôs tau (0.820) are lower than for the Spearman rank (0.952) correlation. This is because Kendall‚Äôs gives more weight to ranks that are far apart, whereas Spearman‚Äôs weights each rank equally. Generally, Kendalls‚Äôs is more appropriate when there is more uncertainty about the reliability of close ranks.\nThe sturgeons in this sample were collected using nets and baited hooks of a certain size. What impact do you think this method of collection had on the shapes of the distributions of fklngth and rdwght ? Under these circumstances, do you think correlation analysis is appropriate at all?\nNote that correlation analysis assumes that each variable is randomly sampled. In the case of sturgeon, this is not the case: baited hooks and nets will only catch sturgeon above a certain minimum size. Note that in the sample, there are no small sturgeons, since the fishing gear targets only larger fish. Thus, we should be very wary of the correlation coefficients associated with our analysis, as the inclusion of smaller fish may well change our estimate of these correlations.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#sec-simple-lm",
    "href": "31-reg_lin.html#sec-simple-lm",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.6 Simple linear regression",
    "text": "9.6 Simple linear regression\nIn correlation analysis we are interested in how pairs of variables covary: However, in regression analysis, we are attempting to estimate a model that predicts a variable (the dependent variable) from another variable (the independent variable).\nAs with any statistical analysis, the best way to begin is by looking at your data. If you are interested in the relationship between two variables, say, Y and X, produce a plot of Y versus X just to get a ‚Äúfeel‚Äù for the relationship.\n\nThe data file sturgeon.csv contains data for sturgeons collected from 1978-1980 at Cumberland House, Saskatchewan and The Pas, Manitoba. Make a scatterplot of fklngth (the dependent variable) versus age (the independent variable) for males and add a linear regression and a loess smoother. What do you conclude from this plot?\n\n\nsturgeon.male &lt;- subset(sturgeon, subset = sex == \"MALE\")\nmygraph &lt;- ggplot(\n  data = sturgeon.male, # source of data\n  aes(x = age, y = fklngth)\n) # aesthetics: y=fklngth, x=rdwght\n# plot data points, regression, loess trace\nmygraph &lt;- mygraph +\n  stat_smooth(method = lm, se = FALSE, color = \"green\") + # add linear regression, but no SE shading\n  stat_smooth(color = \"red\") + # add loess\n  geom_point() # add data points\nmygraph # display graph\n\n\n\n\n\n\nFigure¬†9.5\n\n\n\n\nThis suggests that the relationship between age and fork length is not linear.\nSuppose that we want to know the growth rate of male sturgeon. One estimate (perhaps not a very good one) of the growth rate is given by the slope of the fork length - age regression.\nFirst, let‚Äôs run the regression with the lm() command, and save its results in an object called RegModel.1.\n\nRegModel.1 &lt;- lm(fklngth ~ age, data = sturgeon.male)\n\nNothing appears on the screen, but don‚Äôt worry, it all got saved in memory. To see the statistical results, type:\n\nsummary(RegModel.1)\n\n\nCall:\nlm(formula = fklngth ~ age, data = sturgeon.male)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4936 -2.2263  0.1849  1.7526 10.8234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.50359    1.16873   24.39   &lt;2e-16 ***\nage          0.70724    0.05888   12.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.307 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.664, Adjusted R-squared:  0.6594 \nF-statistic: 144.3 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nR output gives you:\n\n\nCall: A friendly reminder of the model fitted and the data used.\n\nResiduals: General statistics about the residuals around the fitted model.\n\nCoefficients: Fitted model parameter estimates, standard errors, t values and associated probabilities.\n\nResidual standard error: Square root of the residual variance.\n\nMultiple R-squared: Coefficient of determination. It corresponds to the proportion of the total variance of the dependent variable that is accounted for by the regression (i.e.¬†by the independent variable)\n\nAdjusted R-squared: The adjusted R-squared accounts for the number of parameters in the model. If you want to compare the performance of several models with different numbers of parameters, this is the one to use\n\nF-statistic: This is the test of the overall significance of the model. In the simple regression case, this is the same as the test of the slope of the regression.\n\nThe estimated regression equation is therefore:\n\\[ Fklngth = 28.50359 + 0.70724 * age\\]\nGiven the highly significant F-value of the model (or equivalently the highly significant t-value for the slope of the line), we reject the null hypothesis that there is no relationship between fork length and age.\n\n9.6.1 Testing regression assumptions\nSimple model I regression makes four assumptions:\n\nthe X variable is measured without error;\nthe relationship between Y and X is linear;\nthat for any value of X, the Y‚Äôs are independently and normally distributed;\nthe variance of Y for fixed X is independent of X.\n\nHaving done the regression, we can now test the assumptions. For most biological data, the first assumption is almost never valid; usually there is error in both Y and X. This means that in general, slope estimates are biased, but predicted values are unbiased. However, so long as the error in X is small relative to the range of X in your data, the fact that X has an associated error is not likely to influence the outcome dramatically. On the other hand, if there is substantial error in X, regression results based on a model I regression may give poor estimates of the functional relationship between Y and X. In this case, more sophisticated regression procedures must be employed which are, unfortunately, beyond the scope of this course.\nThe other assumptions of a model I regression can, however, be tested, or at least evaluated visually. The plot() command can display diagnostics for linear models.\n\npar(mfrow = c(2, 2), las = 1)\nplot(RegModel.1)\n\nThe par() command is used here to tell R to display 2 rows and 2 columns of graphs per page (there are 4 diagnostic graphs for linear models generated automatically), and the last statement is to tell R to rotate the labels of the Y axis so that they are perpendicular to the Y axis. (Yes, I know, this is not at all obvious.)\nYou will get:\n\n\n\n\n\n\n\nFigure¬†9.6\n\n\n\n\n\n\nUpper left tell you about linearity, normality, and homoscedasticity of the residuals. It shows the deviations around the regression vs the predicted values. Remember that the scatterplot ( fklngth vs age ) suggested that the relationship between fork length and age is not linear. Very young and very old sturgeons tended to fall under the line, and fish of average age tended to be a bit above the line. This is exactly what the residual vs fitted plot shows. The red line is a lowess trace through these data. If the relationship was linear, it would be approximately flat and close to 0. The scatter of residuals tells you a bit about their normality and homoscedasticity, although this graph is not the best way to look at these properties. The next two are better.\n\nUpper right is to assess the normality of the residuals. It is a QQ plot of the residuals . If the residuals were normally distributed, they would fall very close to the diagonal line. Here, we see it is mostly the case, except in the tails\n\nBottom left titled Scale-Location, helps with assessing homoscedasticity. It plots the square root of the absolute value of the standardized residual (residual divided by the standard error of the residuals, this scales the residuals so that their variance is 1 ) as a function of the fitted value. This graph can help you visualize whether the spread of the residuals is constant or not. If residuals are homoscedastic, then the average will not change with increasing fitted values. Here, there is slight variability, but it is not monotonous (i.e.¬†it does not increase or decrease systematically) and there is no strong evidence against the assumption of homoscedasticity.\n\nBottom right plots the residuals as a function of leverage and can help detecting the presence of outliers or points that have a very strong influence on the regression results. The leverage of a point measures how far it is from the other points, but only with respect to the independent variable. In the case of simple linear regression, it is a function of the difference between the observation and the mean of the independent variable. You should look more closely at any observation with a leverage value that is greater than: \\(2(k+1)/n\\), where $\\(k\\) is the number of independent variables (here 1), and \\(n\\) is the number of observations. In this case there is 1 independent variable, 75 observations, and points with a leverage higher than 0.053 may warrant particular scrutiny. The plot also gives you information about how the removal of a point from the data set would change the predictions. This is measured by the Cook‚Äôs distance, illustrated by the red lines on the plot. A data point with a Cook distance larger than 1 has a large influence.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that R automatically labels the most extreme cases on each of these 4 plots. It does not mean that these cases are outliers, or that you necessarily need be concerned with them. In any data set, there will always be a minimum and a maximum residual.\n\n\nThe R package performance offers a new and updated version of those graphs with colours and more plots to help visually assess the assumptions with the function model_check()\n\ncheck_model(RegModel.1)\n\n\n\n\n\n\nFigure¬†9.7\n\n\n\n\nSo, what is the verdict about the linear regression between fklngth and age ? It fails the linearity, possibly fails the normality, passes homoscedasticity, and this does not seem to be too strongly affected by some bizarre points.\n\n9.6.2 Formal tests of regression assumptions\nIn my practice, I seldom use formal tests of regression assumptions and mostly rely on graphs of the residuals to guide my decisions. To my knowledge, this is what most biologists and data analysts do. However, in my early analyst life I was not always confident that I was interpreting these graphs correctly and wished that I had a formal test or a statistic quantifying the degree of deviation from the regression assumptions.\nThe lmtest R package, not part of the base R installation, but available from CRAN, contains a number of tests for linearity and homoscedasticity. And one can test for normality using the Shapiro-Wilk test seen previously.\nFirst, you need to load (and maybe install) the lmtest package.\n\nlibrary(lmtest)\n\n\n\n\n\n\n\nExercise\n\n\n\nRun the following commands\n\n\n\nbptest(RegModel.1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModel.1\nBP = 1.1765, df = 1, p-value = 0.2781\n\n\nThe Breusch-Pagan test examines whether the variability of the residuals is constant with respect to increasing fitted values. A low p value is indicative of heteroscedasticity. Here, the p value is high, and supports my visual assessment that the homoscedasticity assumption is met by these data.\n\ndwtest(RegModel.1)\n\n\n    Durbin-Watson test\n\ndata:  RegModel.1\nDW = 2.242, p-value = 0.8489\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nThe Durbin-Watson test can detect serial autocorrelation in the residuals. Under the assumption of no autocorrelation, the D statistic is 2. This test can detect violation of independence of observations (residuals), although it is not foolproof. Here there is no problem identified.\n\nresettest(RegModel.1)\n\n\n    RESET test\n\ndata:  RegModel.1\nRESET = 14.544, df1 = 2, df2 = 71, p-value = 5.082e-06\n\n\nThe RESET test is a test of the assumption of linearity. If the linearity assumption is met, the RESET statistic will be close to 1. Here, the statistic is much larger (14.54), and very highly significant. This confirms our visual assessment that the relationship is not linear.\n\nshapiro.test(residuals(RegModel.1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModel.1)\nW = 0.98037, p-value = 0.2961\n\n\nThe Shapiro-Wilk normality test on the residual confirms that the deviation from normality of the residuals is not large.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#data-transformations-in-regression",
    "href": "31-reg_lin.html#data-transformations-in-regression",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.7 Data transformations in regression",
    "text": "9.7 Data transformations in regression\nThe analysis above revealed that the linearity assumption underlying regression analysis is not met by the fklngth - age data. If we want to use regression analysis, data transformations are required:\nLet‚Äôs plot the log-transformed data\n\npar(mfrow = c(1, 1), las = 1)\nggplot(\n  data = sturgeon.male,\n  aes(x = log10(age), y = log10(fklngth))\n) +\n  geom_smooth(color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  geom_point()\n\n\n\n\n\n\nFigure¬†9.8\n\n\n\n\nWe can fit the linear regression model on the log-transformed variables.\n\nRegModel.2 &lt;- lm(log10(fklngth) ~ log10(age), data = sturgeon.male)\nsummary(RegModel.2)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082794 -0.016837 -0.000719  0.021102  0.087446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19199    0.02723   43.77   &lt;2e-16 ***\nlog10(age)   0.34086    0.02168   15.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03015 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.772, Adjusted R-squared:  0.7688 \nF-statistic: 247.1 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nNote that by using the log transformed data, the proportion of variation explained by the regression has increased by 10% (from 0.664 to 0.772), a substantial increase. So the relationship has become more linear. Good. Let‚Äôs look at the residual diagnostic plots:\n\npar(mfrow = c(2, 2), las = 1)\nplot(RegModel.2)\ncheck_model(RegModel.2)\n\n\n\n\n\n\nFigure¬†9.9\n\n\n\n\n\n\n\n\n\nFigure¬†9.10\n\n\n\n\nSo things appear a little better than before, although still not ideal. For example, the Residual vs fitted plot still suggests a potential nonlinearity. The QQ plot is nicer than before, indicating that residuals are more normally distributed after the log-log transformation. There is no indication of heteroscedasticity. And, although there are still a few points with somewhat high leverage, none have a Cook‚Äôs distance above 0.5. It thus seems that transforming data improved things: more linear, more normal, less dependence on extreme data. Do the formal tests support this visual assessment?\n\nbptest(RegModel.2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModel.2\nBP = 0.14282, df = 1, p-value = 0.7055\n\ndwtest(RegModel.2)\n\n\n    Durbin-Watson test\n\ndata:  RegModel.2\nDW = 2.1777, p-value = 0.6134\nalternative hypothesis: true autocorrelation is greater than 0\n\nresettest(RegModel.2)\n\n\n    RESET test\n\ndata:  RegModel.2\nRESET = 4.4413, df1 = 2, df2 = 71, p-value = 0.01523\n\nshapiro.test(residuals(RegModel.2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModel.2)\nW = 0.98998, p-value = 0.8246\n\n\nIndeed, they do: residuals are still homoscedastic (Breusch-Pagan test), show no autocorrelation (Durbin-Watson test), are normal (Shapiro-Wilk test), and they are more linear (p value of the RESET test is now 0.015, instead of 0.000005). Linearity has improved, but is still violated somewhat.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#dealing-with-outliers",
    "href": "31-reg_lin.html#dealing-with-outliers",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.8 Dealing with outliers",
    "text": "9.8 Dealing with outliers\nIn this case, there are no real clear outliers. Yes, observations 8, 24, and 112 are labeled as the most extreme in the last set of residual diagnostic plots. But they are still within what I consider the ‚Äúreasonable‚Äù range. But how does one define a limit to the reasonable? When is an extreme value a real outlier we have to deal with? Opinions vary about the issue, but I favor conservatism.\nMy rule is that, unless the value is clearly impossible or an error in data entry, I do not delete ‚Äúoutliers‚Äù; rather, I analyze all my data. Why? Because, I want my data to reflect natural or real variability. Indeed, variability is often what interests biologists the most.\nKeeping extreme values is the fairest way to proceed, but it often creates other issues. These values will often be the main reason why the data fail to meet the assumptions of the statistical analysis. One solution is to run the analysis with and without the outliers, and compare the results. In many cases, the two analyses will be qualitatively similar: the same conclusions will be reached, and the effect size will not be very different. Sometimes, however, this comparison will reveal that the presence of the outliers changes the story. The logical conclusion then is that the results depend on the outliers and that the data at hand are not very conclusive. As an example, let‚Äôs rerun the analysis after eliminating observations labeled 8, 24, and 112.\n\nRegModel.3 &lt;- lm(log10(fklngth) ~ log10(age), data = sturgeon.male, subset = !(rownames(sturgeon.male) %in% c(\"8\", \"24\", \"112\")))\nsummary(RegModel.3)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male, \n    subset = !(rownames(sturgeon.male) %in% c(\"8\", \"24\", \"112\")))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069163 -0.017390  0.000986  0.018590  0.047647 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.22676    0.02431   50.46   &lt;2e-16 ***\nlog10(age)   0.31219    0.01932   16.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02554 on 70 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7855 \nF-statistic:   261 on 1 and 70 DF,  p-value: &lt; 2.2e-16\n\n\nThe intercept, slope, and R squared are about the same, and the significance of the slope is still astronomical. Removing the ‚Äúoutliers‚Äù has little effect in this case.\nAs for the diagnostic residual plots and the formal tests of assumptions:\n\npar(mfrow = c(2, 2))\nplot(RegModel.3)\nbptest(RegModel.3)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModel.3\nBP = 0.3001, df = 1, p-value = 0.5838\n\ndwtest(RegModel.3)\n\n\n    Durbin-Watson test\n\ndata:  RegModel.3\nDW = 2.0171, p-value = 0.5074\nalternative hypothesis: true autocorrelation is greater than 0\n\nresettest(RegModel.3)\n\n\n    RESET test\n\ndata:  RegModel.3\nRESET = 3.407, df1 = 2, df2 = 68, p-value = 0.0389\n\nshapiro.test(residuals(RegModel.3))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModel.3)\nW = 0.98318, p-value = 0.4502\n\n\n\n\n\n\n\nFigure¬†9.11\n\n\n\n\nNo real difference either. Overall, this suggests that the most extreme values do not have undue influence on the results.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#quantifying-effect-size-in-regression-and-power-analysis",
    "href": "31-reg_lin.html#quantifying-effect-size-in-regression-and-power-analysis",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.9 Quantifying effect size in regression and power analysis",
    "text": "9.9 Quantifying effect size in regression and power analysis\nBiological interpretation differs from statistical interpretation. Statistically, we conclude that size increase with age (i.e.¬†the slope is positive and different from 0). But this conclusion alone does not tell if the difference between young and old fish is large. The slope and the scatterplot are more informative than the p-value here. The slope (in log-log space) is 0.34. This means that for each unit increase of X (log10(age)), there is an increase of 0.34 units of log10(fklngth). In other words, when age is multiplied by 10, fork length is multiplied by about 2 (10^0.34^). Humm, length increases more slowly than age. This slope value (0.34) is an estimate of raw effect size. It measure how much length changes with age.\nIt would also be important to estimate the confidence interval around the estimate of the slope. This can beobtained using the confint() function.\n\nconfint(RegModel.2)\n\n                2.5 %   97.5 %\n(Intercept) 1.1377151 1.246270\nlog10(age)  0.2976433 0.384068\n\n\nThe 95% confidence interval for the slope is 0.29-0.38. It is quite narrow and include only values far from zero.\n\n9.9.1 Power to detect a given slope\nYou can compute power with G*Power for some slope value that you deem of sufficient magnitude to warrant detection.\n\nGo to t Tests: Linear bivariate regression: One group, size of slope.\nSelect Post hoc: Compute achieved power- given \\(\\alpha\\), sample size,and effect size\n\n\nFor example, suppose that sturgeon biologists deem that a slope of 0.1 for the relationship between log10(fklngth) and log10(age) is meaningful and you wanted to estimate the power to detect such a slope with a sample of 20 sturgeons. Results from the log-log regression contain most of what you need:\n\nsummary(RegModel.2)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082794 -0.016837 -0.000719  0.021102  0.087446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19199    0.02723   43.77   &lt;2e-16 ***\nlog10(age)   0.34086    0.02168   15.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03015 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.772, Adjusted R-squared:  0.7688 \nF-statistic: 247.1 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nNote the Residual standard error value (0.03015). You will need this. The other thing you need is an estimate of the standard deviation of log10(age). R can (of course) compute it. Be careful, the sd() function will return NA if there are missing values. You can get around this by adding na.rm=TRUE as an argument ot the sd() function.\n\nsd(log10(sturgeon.male$age), na.rm = TRUE)\n\n[1] 0.1616675\n\n\nYou can then enter these values (slope to be detected, sample size, alpha, standard deviation of the independent variable) to calculate another quantity that G*Power needs (standard deviation of y) using the Determine panel. Finally you can calculate Power. The filled panels should look like this\n\n\n\n\n\n\nNote\n\n\n\nNote: The SD of y can‚Äôt just be taken from the data because if the slope chanages (e.g.¬†H1) then this will change the SD of y. SD y needs to be estimated from the observed scatter around the line and the hypothesized slope.\n\n\n\n\n\n\n\n\n\nFigure¬†9.12: Power analysis for age-length in sturgeon with N = 20 and slope = 0.1\n\n\n\n\nPower to detect a significant slope, if the slope is 0.1, variability of data points around the regression is like in our sample, for a sample of 20 sturgeons, with \\(\\alpha = 0.05\\) is 0.621. Only about 2/3 of samples of that size would detect a significant effect of age on fklngth.\nIn R, you can do the analysis also but we will use another trick to work with the pwr.t.test() function. First we, need to estimate the effect size d. IN this case d is estimated as: \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} \\] where \\(b\\) is the slope, \\(s_b\\) is the standard error on the slope, \\(n\\) is the number of observations and \\(k\\) is the number of independent variables (1 for simple liner regression).\nSE of the slope is 0.02168. The model was fitted using 75 fishes (n=75). We can then estimate d. \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} = \\frac{0.1}{0.02168\\sqrt{74-1-1}}=0.54\\]\nWe can simply use the pwr.t.test() function to estimate the power.\n\nlibrary(pwr)\n\n# analyse de puissance\npwr.t.test(n = 20, d = 0.54, sig.level = 0.05, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20\n              d = 0.54\n      sig.level = 0.05\n          power = 0.6299804\n    alternative = two.sided\n\n\nYou can see that the results is really similar but not exactly the same than with G*power which is normal since we did not use the exact same formula to estimate power.\n\n9.9.2 Sample size required to achieve desired power\nTo estimate the sample size required to achieve 99% power to detect a slope of 0.1 (in log-log space), with alpha=0.05, you simply change the type of analysis:\n\n\n\n\n\n\n\nFigure¬†9.13: A priori power analysis to estimate the sample size needed to have a power of 0.99\n\n\n\n\nIn R you can simply do:\n\nlibrary(pwr)\n\n# analyse de puissance\npwr.t.test(power = 0.99, d = 0.54, sig.level = 0.05, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 64.96719\n              d = 0.54\n      sig.level = 0.05\n          power = 0.99\n    alternative = two.sided\n\n\nBy increasing sample size to 66, with the same assumptions as before, power increases to 99%.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#bootstrapping-the-simple-linear-regression",
    "href": "31-reg_lin.html#bootstrapping-the-simple-linear-regression",
    "title": "\n9¬† Correlation and simple linear regression\n",
    "section": "\n9.10 Bootstrapping the simple linear regression",
    "text": "9.10 Bootstrapping the simple linear regression\nA non-parametric test for the intercept and slope of a linear regression can be obtained by bootstrapping.\n\n# load boot\nlibrary(boot)\n# function to obtain regression weights\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ] # allows boot to select sample\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = sturgeon.male,\n  statistic = bs,\n  R = 1000, formula = log10(fklngth) ~ log10(age)\n)\n# view results\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = sturgeon.male, statistic = bs, R = 1000, formula = log10(fklngth) ~ \n    log10(age))\n\n\nBootstrap Statistics :\n     original        bias    std. error\nt1* 1.1919926  1.654955e-04  0.03309166\nt2* 0.3408557 -3.930302e-06  0.02623126\n\n\nFor each parameter in the model (here the intercept is labeled t1\\* and the slope of the regression line is labeled t2\\*) , you obtain:\nPour chaque param√®tre du mod√®le (ici l‚Äôordonn√©e √† l‚Äôorigine est appel√©e t1* et la pente de la r√©gression t2\\*), R imprime :\n\n\noriginal original parameter estimate (on all non-bootstrapped data)\n\nbias the difference between the mean value of all bootstrap estimates and the original value\n\nstd. error standard error of the bootstrap estimate\n\n\npar(mfrow = c(2, 2))\nplot(results, index = 1) # intercept\nplot(results, index = 2) # log10(age)\n\n\n\n\n\n\nFigure¬†9.14\n\n\n\n\n\n\n\n\n\nFigure¬†9.15\n\n\n\n\nThe distribution of the bootstrapped estimates is rather Gaussian, with only small deviations in the tails (where it counts for confidence intervals‚Ä¶). One could use the standard error of the bootstrap estimates to calculate a symmetrical confidence interval as mean +- t SE. But, given that R can as easily calculate a bias-corrected adjusted (BCa) confidence interval, or one based on the actual distribution, (Percentile) why not have it do it all:\n\n# interval de confiance pour l'ordonn√©e √† l'origine\nboot.ci(results, type = \"all\", index = 1)\n\nWarning in boot.ci(results, type = \"all\", index = 1): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"all\", index = 1)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 1.127,  1.257 )   ( 1.127,  1.258 )  \n\nLevel     Percentile            BCa          \n95%   ( 1.126,  1.257 )   ( 1.115,  1.249 )  \nCalculations and Intervals on Original Scale\n\n\n\n# intervalle de confiance pour la pente\nboot.ci(results, type = \"all\", index = 2)\n\nWarning in boot.ci(results, type = \"all\", index = 2): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"all\", index = 2)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.2894,  0.3923 )   ( 0.2881,  0.3928 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.2889,  0.3936 )   ( 0.2948,  0.3999 )  \nCalculations and Intervals on Original Scale\n\n\nHere the 4 types of CI that R managed to calculate are essentially the same. Had data been violating more strongly the standard assumptions (normality, homoscedasticity), then the different methods (Normal, Basic, Percentile, and BCa) would have diverged more. In that case, which one is best? BCa has the favor of most, currently.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Correlation and simple linear regression</span>"
    ]
  },
  {
    "objectID": "32-t_test.html",
    "href": "32-t_test.html",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "",
    "text": "10.1 R packages and data\nFor this la b you need:\nLoading required package: carData\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:car':\n\n    logit\nYou need to load the packages in R with library() and if need needed install them first with install.packages() For the data, load them using the read.csv() function.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#set-ttest",
    "href": "32-t_test.html#set-ttest",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "",
    "text": "R packages:\n\ncar\nlmtest\nboot\npwr\nggplot2\nperformance\nlmPerm\n\n\ndata:\n\nsturgeon.csv\nskulldat_2020.csv",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#visual-examination-of-sample-data",
    "href": "32-t_test.html#visual-examination-of-sample-data",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "\n10.2 Visual examination of sample data",
    "text": "10.2 Visual examination of sample data\nOne of the first steps in any type of data analysis is to visualize your data with plots and summary statistics, to get an idea of underlying distributions, possible outliers, and trends in your data. This often begins with plots of the data, such as histograms, probability plots, and box plots, that allow you to get a feel for whether your data are normally distributed, whether they are correlated one to the other, or whether there are any suspicious looking points that may lead you to go back to the original data file to check for errors.\nSuppose we want to test the null hypothesis that the size, as indexed by fork length ( fklngth in file sturgeon.csv - the length, in cm, from the tip of the nose to the base of the fork in the caudal fin), of sturgeon at The Pas and Cumberland House is the same. To begin, we have a look at the underlying distributions of the sample data to get a feel for whether the data are normally distributed in each sample. We will not actually test for normality at this point; the assumption of normality in parametric analyses refers always to the residuals and not the raw data themselves. However, if the raw data are non-normally distributed, then you usually have good reason to suspect that the residuals also will be non-normally distributed.\nAn excellent way to visually compare a data distribution to a normal distribution is to superimpose a histogram of the data and a normal curve. To do so, we must proceed in two steps:\n\ntell R that we want to make a histogram with a density curve superimposed\ntell R that we want this to be done for both locations.\n\n\nUsing the data file sturgeon.csv , generate histograms for fklngth data at The Pas and Cumberland House.\n\n\n# use \"sturgeon\" dataframe to make plot called mygraph\n# and define x axis as representing fklngth\nmygraph &lt;- ggplot(\n  data = sturgeon,\n  aes(x = fklngth)\n) +\n  xlab(\"Fork length (cm)\")\n# add data to the mygraph ggplot\nmygraph &lt;- mygraph +\n  geom_density() + # add data density smooth\n  geom_rug() + # add rug (bars at the bottom of the plot)\n  geom_histogram( # add black semitransparent histogram\n    aes(y = ..density..),\n    color = \"black\", alpha = 0.3\n  ) +\n  # add normal curve in red, with mean and sd from fklength\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(sturgeon$fklngth),\n      sd = sd(sturgeon$fklngth)\n    ),\n    color = \"red\"\n  )\n# display graph, by location\nmygraph + facet_grid(. ~ location)\n\n\n\nDistribution of sturgeon length at 2 locations\n\n\n\nBased on your visual inspection, are the two samples normally distributed? Visual inspection of these plots suggests that this variable is approximately normally distributed in each sample.\nSince we are interested in finding out if mean fish size differs among the two locations, it is probably also a good idea to generate a graph that compares the two groups of data. A box plot works well for this.\n\nGenerate a box plot of fklngth grouped by location . What do you conclude about differences in size among the two locations?\n\n\nggplot(data = sturgeon, aes(\n  x = location,\n  y = fklngth\n)) +\n  geom_boxplot(notch = TRUE)\n\n\n\nBoxplot of sturgeon legnth at 2 locations\n\n\n\nIt would appear as though there are not big differences in fish size among the two locations, although fish size at The Pas looks to be more variable, with a bigger range in size and outliers (defined as values &gt; 1.5 * inter-quartile range) at both ends of the distribution.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#comparing-means-of-two-independent-samples-parametric-and-non-parametric-comparisons",
    "href": "32-t_test.html#comparing-means-of-two-independent-samples-parametric-and-non-parametric-comparisons",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "\n10.3 Comparing means of two independent samples: parametric and non-parametric comparisons",
    "text": "10.3 Comparing means of two independent samples: parametric and non-parametric comparisons\nTest the null hypothesis that the mean fklngth of The Pas and Cumberland House samples are the same. Using 3 different tests:\n\nparametric test with equal variances\nparametric test with unequal variances\nnon-parametric test\n\nWhat do you conclude?\n\n# t-test assuming equal variances\nt.test(\n  fklngth ~ location,\n  data = sturgeon,\n  alternative = \"two.sided\",\n  var.equal = TRUE\n)\n\n\n    Two Sample t-test\n\ndata:  fklngth by location\nt = 2.1359, df = 184, p-value = 0.03401\nalternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0\n95 percent confidence interval:\n 0.1308307 3.2982615\nsample estimates:\nmean in group CUMBERLAND    mean in group THE_PAS \n                45.08439                 43.36984 \n\n\n\n# t-test assuming unequal variances\nt.test(\n  fklngth ~ location,\n  data = sturgeon,\n  alternative = \"two.sided\",\n  var.equal = FALSE\n)\n\n\n    Welch Two Sample t-test\n\ndata:  fklngth by location\nt = 2.2201, df = 169.8, p-value = 0.02774\nalternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0\n95 percent confidence interval:\n 0.1900117 3.2390804\nsample estimates:\nmean in group CUMBERLAND    mean in group THE_PAS \n                45.08439                 43.36984 \n\n\n\n# test non param√©trique\nwilcox.test(\n  fklngth ~ location,\n  data = sturgeon,\n  alternative = \"two.sided\"\n)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  fklngth by location\nW = 4973, p-value = 0.06296\nalternative hypothesis: true location shift is not equal to 0\n\n\nBased on the t-test, we would reject the null hypothesis, i.e. there is a significant (but not highly significant) difference in mean fork length between the two populations.\nNote that using the Wilcoxon rank sum test, we do not reject the null hypothesis. The two different tests therefore give us two different results. The significant difference obtained using the t-test may, at least in part, be due to deviations from normality or homoscedasticity; on the other hand, the non-significant difference obtained using the U -statistic may be due to the fact that for fixed sample size, the power of a non-parametric test is lower than the corresponding parametric test. Given the p values obtained from both tests, and the fact that for samples of this size (84 and 101), the t-test is comparatively robust with respect to non-normality, I would be inclined to reject the null hypothesis. In practice to avoid P-hacking, you should decide which test is appropriate first and then apply and interpret it, or if you decide to do all you should present results of all and interpret accordingly.\nBefore accepting the results of the parametric t-test and rejecting the null hypothesis that there is no difference in size between the two locations, one should do some sort of assessment to determine if the model fits the assumption of normally distributed residuals and equal variances. Preliminary examination of the raw data suggested the data appeared roughly normal but there might be problems with variances (since the spread of data for The_Pas was much greater than for Cumberland). We can examine this more closely by looking at the residuals. An easy way to do so, is to fit a linear model and use the residual diagnostic plots:\n\nm1 &lt;- lm(fklngth ~ location, data = sturgeon)\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\nModel assumption checks\n\n\n\nThe first plot above shows the spread of the residuals around the estimated values for the two groups and allows us to get a feel for whether there are problems with the assumption of homogeneity of variances. If the variances were equal, the vertical spread of the two clusters of points should be about the same. The above plot shows that the vertical spread of the group with the smaller mean is greater than it is for the larger mean, suggesting again that there are problems with the variances. We can test this formally by examining the mean differences in the absolute value of the residuals.\nThe second graph above is a normal QQ plot (or probability plot) of the residuals of the model. Note that these generally fall on a straight line, suggesting there is no real problem with normality. We can do a formal test for normality on the residuals using the Shapiro-Wilk test.\n\nshapiro.test(residuals(m1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1)\nW = 0.97469, p-value = 0.001857\n\n\nHummm. The test indicates that the residuals are not normal. But, given that (a) the distribution is not very far (at least visually) from normal, and that (b) the number of observations in each location is reasonably large (i.e.¬†&gt;30), we do not need to be overly concerned with this violation of the normality assumption.\nHow about equality of variance?\n\nlibrary(car)\nleveneTest(m1)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1  11.514 0.0008456 ***\n      184                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nbptest(m1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m1\nBP = 8.8015, df = 1, p-value = 0.00301\n\n\nThe above are the results of two tests implemented in R (in the car and lmtest packages üì¶ that can be used to test for equal variances in t-tests or linear models involving only discontinuous or categorical independent variables. Doing the two of them is overkill. There is not much to prefer one test over another. Levene test is possibly the better known. It tests whether the mean of absolute values of the residuals differs among groups. The Breusch-Pagan test has the advantage of being applicable to more linear models (it can deal with regression-type continuous independent variables, at least to some extent). It tests whether the studentized (i.e.¬†scaled by their sd estimate) squared residuals vary with the independent variables in a linear model. In this case, both indicate that variances are unequal.\nOn the basis of these results, we conclude that there is evidence (albeit weak) to reject the null hypothesis of no difference in fklngth by location. We have modified the t-test to accommodate unequal variances, and are satisfied that the assumption of normally distributed residuals is sufficiently met. Thus, it appears that fklngth at Cumberland is greater than fklngth at The Pas.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#bootstrap-and-permutation-tests-to-compare-2-means",
    "href": "32-t_test.html#bootstrap-and-permutation-tests-to-compare-2-means",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "\n10.4 Bootstrap and permutation tests to compare 2 means",
    "text": "10.4 Bootstrap and permutation tests to compare 2 means\n\n10.4.1 Bootstrap\nBootstrap and permutation tests can be used to compare means (or other statistics) between pairs of samples. The general idea is simple, and it can be implemented in more ways than I can count. Here, I use existing tools and the fact that a comparison of means can be construed as a test of a linear model. We will be able to use similar code later on when we fit more complex (but fun!) models.\n\nlibrary(boot)\n\nThe first section defines the function that I called bs that simply extracts coefficients from a fitted model:\n\n# function to obtain model coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ]\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n\nThe second section with the boot() command is where the real work is done: take data in sturgeon, bootstrap \\(R = 1000\\) times, each time fit the model fklngth vs location, and keep the values calculated by the bs() function.\n\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = sturgeon, statistic = bs, R = 1000,\n  formula = fklngth ~ location\n)\n# view results\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = sturgeon, statistic = bs, R = 1000, formula = fklngth ~ \n    location)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 45.084391 -0.027539736   0.4201149\nt2* -1.714546  0.008761405   0.7436475\n\n\nSo we get the original estimates for the two coefficients in this model: the mean at the first (alphabetical) location, Cumberland, and the difference in means between Cumberland and The Pas ). It is the second parameter, the difference between means, which is of interest here.\n\nplot(results, index = 2)\n\n\n\nDistribution of bootstrapped mean difference\n\n\n\n\n# get 95% confidence intervals\nboot.ci(results, type = \"bca\", index = 2)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   (-3.229, -0.265 )  \nCalculations and Intervals on Original Scale\n\n\nThe 95% CI for the difference between the two means does not include 0. Hence, the bootstrap test indicates that the two means are not equals.\n\n10.4.2 Permutation\nPermutation tests for linear models can easily be done using the lmPerm package üì¶.\n\nm1Perm &lt;- lmp(\n  fklngth ~ location,\n  data = sturgeon,\n  perm = \"Prob\"\n)\n\n[1] \"Settings:  unique SS \"\n\n\nThe lmp() function does all the work for us. Here it is run with the option perm to control the stopping rule used. Option Prob stops the sampling when the estimated standard deviation of the p-value falls below some fraction of the estimated. It is one of many stopping rules that one could use to do permutations on a subset of all the possibilities (because it would take foreeeever to do them all, even on your fast machine).\n\nsummary(m1Perm)\n\n\nCall:\nlmp(formula = fklngth ~ location, data = sturgeon, perm = \"Prob\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-18.40921  -3.75370  -0.08439   3.76598  23.48055 \n\nCoefficients:\n          Estimate Iter Pr(Prob)   \nlocation1   0.8573 5000   0.0092 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.454 on 184 degrees of freedom\nMultiple R-Squared: 0.02419,    Adjusted R-squared: 0.01889 \nF-statistic: 4.562 on 1 and 184 DF,  p-value: 0.03401 \n\n\n\n\nIter coefficient: the Prob stopping rule stopped after 0.4013591 iterations. Note that this number will vary each time you run this snippet of code. These are random permutation results, so expect variability.\n\nPr(Prob) coefficient: The estimated probability associated to H0 is 2.1359 . The observed difference in fklngth between the two locations was larger than the permuted differences in about (1 - 2.1359= about -113.6%) of the 0.4013591 cases. Mind you, 0.4013591 permutations is not a large number, so small p values can‚Äôt be expected to be very precise. If it is critical that you get more precise p values, more permutations would be needed. Two parameters can be tweaked: maxIter, the maximum number of iterations (default=5000), and Ca, that stops iterations when estimated standard error of the estimated p is less than Ca*p.¬†Default 0.1.\n\nF-statistic: The rest is the standard output for the model fitted to the data, with the standard parametric test. Here the p-value, assuming all assumptions are met, is 0.034.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#comparing-the-means-of-paired-samples",
    "href": "32-t_test.html#comparing-the-means-of-paired-samples",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "\n10.5 Comparing the means of paired samples",
    "text": "10.5 Comparing the means of paired samples\nIn some experimental designs, individuals are measured twice: common examples are the measurement of the same individual at two different times during development, or of the same individual subjected to two different experimental treatments. In these cases, the two samples are not independent (they include the same individuals), and a paired comparison must be made.\nThe file skulldat_2020.csv shows measurements of lower face width of 15 North American girls measured at age 5 and again at age 6 years (data from Newman and Meredith, 1956).\n\nLet‚Äôs first run a standard t-test comparing the face width at age 5 and 6, not taking into account that the data are not independent and that they are consecutive measurements on the same individuals.\n\n\nskull &lt;- read.csv(\"data/skulldat_2020.csv\")\nt.test(width ~ age,\n  data = skull,\n  alternative = \"two.sided\"\n)\n\n\n    Welch Two Sample t-test\n\ndata:  width by age\nt = -1.7812, df = 27.93, p-value = 0.08576\nalternative hypothesis: true difference in means between group 5 and group 6 is not equal to 0\n95 percent confidence interval:\n -0.43002624  0.03002624\nsample estimates:\nmean in group 5 mean in group 6 \n       7.461333        7.661333 \n\n\nSo far, we specified the t-test using a formula notation as y ~ x where y is the variable for which we want to compare the means and x is a variable defining the groups. This works really well when the samples are not paired and when the data is presented in a long format. For example theskull data is presented in a long format and contains 3 variables:\n\n\nwidth: head width for each observations\n\nage: age at measurement 5 or 6\n\nid: person identity\n\n\nhead(skull)\n\n  width age id\n1  7.33   5  1\n2  7.53   6  1\n3  7.49   5  2\n4  7.70   6  2\n5  7.27   5  3\n6  7.46   6  3\n\n\nWhen data are paired, we need to indicate how they are paired. In the skulldata, samples are paired by an individual identity, id, with mearurement taken at different ages. However, the function t.test does not cope well with this data structure. We need to transpose the data from a long to a wide format where we have a column per group, with the data of a given individual on the same line. Here is how we can do it.\n\nskull_w &lt;- data.frame(id = unique(skull$id))\nskull_w$width5 &lt;- skull$width[match(skull_w$id, skull$id) & skull$age == 5]\nskull_w$width6 &lt;- skull$width[match(skull_w$id, skull$id) & skull$age == 6]\nhead(skull_w)\n\n  id width5 width6\n1  1   7.33   7.53\n2  2   7.49   7.70\n3  3   7.27   7.46\n4  4   7.93   8.21\n5  5   7.56   7.81\n6  6   7.81   8.01\n\n\nNow, let‚Äôs run the appropriate paired t-test. What do you conclude? Compare this with the previous result and explain any differences.\n\nt.test(skull_w$width5, skull_w$width6,\n  alternative = \"two.sided\",\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  skull_w$width5 and skull_w$width6\nt = -19.72, df = 14, p-value = 1.301e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2217521 -0.1782479\nsample estimates:\nmean difference \n           -0.2 \n\n\nThe first analysis above assumes that the two samples of girls at age 5 and 6 are independent samples, whereas the second analysis assumes that the same girl is measured twice, once at age 5 and once at age 6 years.\nNote that in the former case, we accept the null based on \\(p = 0.05\\), but in the latter we reject the null. In other words, the appropriate (paired sample) test shows a very significant effect of age, whereas the inappropriate one does not. The reason is because there is a strong correlation between face width at age 5 and face width at age 6:\n\ngraphskull &lt;- ggplot(data = skull_w, aes(x = width5, y = width6)) +\n  geom_point() +\n  labs(x = \"Skull width at age 5\", y = \"Skull width at age 6\") +\n  geom_smooth() +\n  scale_fill_continuous(low = \"lavenderblush\", high = \"red\")\ngraphskull\n\n\n\nRelation between head width at age 5 and 6\n\n\n\nWith r = 0.9930841. In the presence of correlation, the standard error of the pairwise difference in face width at age 5 and 6 is much smaller than the standard error of the difference between the mean face width at age 5 and 6. Thus, the associated t-statistic will be much larger for a paired sample test, i.e. the power of the test is much greater, and the p values are smaller.\n\nRepeat the above procedure with the nonparametric alternative, the Wilcoxon signed-rank test. What do you conclude?\n\n\nwilcox.test(skull_w$width5, skull_w$width6,\n  alternative = \"two.sided\",\n  paired = TRUE\n)\n\nWarning in wilcox.test.default(skull_w$width5, skull_w$width6, alternative =\n\"two.sided\", : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  skull_w$width5 and skull_w$width6\nV = 0, p-value = 0.0007193\nalternative hypothesis: true location shift is not equal to 0\n\n\nSo, we reach the same conclusion as we did using the paired sample t-test and conclude there are significant differences in skull sizes of girls aged 5 and 6 (what a surprise!).\nBut, wait a minute. We have used two-tailed tests here. But, given what we know about how children grow, a one-tail hypothesis would be preferable. This can be done by changing the alternative option. One uses the alternative hypothesis to decide if it is ‚Äúless‚Äù or greater‚Äù. Here, we expect that if there is an effect (i.e the alternative hypothesis), width5 will be less than width6\n\nt.test(skull_w$width5, skull_w$width6,\n  alternative = \"less\",\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  skull_w$width5 and skull_w$width6\nt = -19.72, df = 14, p-value = 6.507e-12\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n       -Inf -0.1821371\nsample estimates:\nmean difference \n           -0.2 \n\nwilcox.test(skull_w$width5, skull_w$width6,\n  alternative = \"less\",\n  paired = TRUE\n)\n\nWarning in wilcox.test.default(skull_w$width5, skull_w$width6, alternative =\n\"less\", : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  skull_w$width5 and skull_w$width6\nV = 0, p-value = 0.0003597\nalternative hypothesis: true location shift is less than 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that instead of rerunning the t-test specifying a one-tailed test, you can:\n\nif the sign of the estimate goes in the same direction as the alternative hypothesis, simply divide by 2 the probability you obtain with the two-tailed test\nif not the sign of the estimate is in the opposite direction of the alternative hypothesis, use \\(1 - p/2\\)\n\n\n\n\nTo estimate the power of a paired t-test in R, we can use the function power.t.test()as for other t-tests but we need to specify the argument type = \"paired\". You need to specify the mean diference within the pairs as the deltaand standard deviance of difference within pairs as sd.\n\nskull_w$diff &lt;- skull_w$width6 - skull_w$width5\npower.t.test(\n  n = 15,\n  delta = mean(skull_w$diff),\n  sd = sd(skull_w$diff),\n  type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 15\n          delta = 0.2\n             sd = 0.03927922\n      sig.level = 0.05\n          power = 1\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#bibliography",
    "href": "32-t_test.html#bibliography",
    "title": "\n10¬† Two - sample comparisons\n",
    "section": "\n10.6 Bibliography",
    "text": "10.6 Bibliography\nBumpus, H.C. (1898) The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. Biological Lectures, Woods Hole Biology Laboratory, Woods Hole, 11 th Lecture: 209 - 226.\nNewman, K.J. and H.V. Meredith. (1956) Individual growth in skeletal bigonial diameter during the childhood period from 5 to 11 years of age. Amer. J. Anat. 99: 157 - 187.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Two - sample comparisons</span>"
    ]
  },
  {
    "objectID": "33-anova.html",
    "href": "33-anova.html",
    "title": "\n11¬† One-way ANOVA\n",
    "section": "",
    "text": "11.1 R packages and data\nFor this lab you need:\nlibrary(ggplot2)\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "33-anova.html#set-ano",
    "href": "33-anova.html#set-ano",
    "title": "\n11¬† One-way ANOVA\n",
    "section": "",
    "text": "R packages:\n\nggplot2\nmultcomp\ncar\n\n\ndata\n\ndam10dat.csv",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "33-anova.html#one-way-anova-with-multiple-comparisons",
    "href": "33-anova.html#one-way-anova-with-multiple-comparisons",
    "title": "\n11¬† One-way ANOVA\n",
    "section": "\n11.2 One-way ANOVA with multiple comparisons",
    "text": "11.2 One-way ANOVA with multiple comparisons\nThe one-way ANOVA is the multi-group analog of the t-test, which is used to compare two groups/levels. It makes essentially the same assumptions, and in the case of two groups/levels, is in fact mathematically equivalent to the t-test.\nIn 1960-1962, the Grand Rapids Dam was built on the Saskatchewan River upstream of Cumberland House. There are anecdotal reports that during dam construction, a number of large sturgeon were stranded and died in shallow pools. Surveys of sturgeon were carried out in 1954, 1958, 1965 and 1966 with fork length (fklngth) and round weight (rdwght) being recorded (not necessarily both measurements for each individual). These data are in the data file Dam10dat.csv.\n\n11.2.1 Visualiser les donn√©es\n\nUsing Dam10dat.csv, you must first change the data type of the numerical variable year , so that R recognizes that we wish to treat this variable as a factor variable and not a continuous variable.\n\n\n\n\n\n\n\nSolution\n\n\n\n\ndam10dat &lt;- read.csv(\"data/Dam10dat.csv\")\ndam10dat$year &lt;- as.factor(dam10dat$year)\nstr(dam10dat)\n\n'data.frame':   118 obs. of  21 variables:\n $ year    : Factor w/ 4 levels \"1954\",\"1958\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ fklngth : num  45 50 39 46 54.5 49 42.5 49 56 54 ...\n $ totlngth: num  49 NA 43 50.5 NA 51.7 45.5 52 60.2 58.5 ...\n $ drlngth : logi  NA NA NA NA NA NA ...\n $ drwght  : num  16 20.5 10 17.5 19.7 21.3 9.5 23.7 31 27.3 ...\n $ rdwght  : num  24.5 33 15.5 28.5 32.5 35.5 15.3 40.5 51.5 43 ...\n $ sex     : int  1 1 1 2 1 2 1 1 1 1 ...\n $ age     : int  24 33 17 31 37 44 23 34 33 47 ...\n $ lfkl    : num  1.65 1.7 1.59 1.66 1.74 ...\n $ ltotl   : num  1.69 NA 1.63 1.7 NA ...\n $ ldrl    : logi  NA NA NA NA NA NA ...\n $ ldrwght : num  1.2 1.31 1 1.24 1.29 ...\n $ lrdwght : num  1.39 1.52 1.19 1.45 1.51 ...\n $ lage    : num  1.38 1.52 1.23 1.49 1.57 ...\n $ rage    : int  4 6 3 6 7 7 4 6 6 7 ...\n $ ryear   : int  1954 1954 1954 1954 1954 1954 1954 1954 1954 1954 ...\n $ ryear2  : int  1958 1958 1958 1958 1958 1958 1958 1958 1958 1958 ...\n $ ryear3  : int  1966 1966 1966 1966 1966 1966 1966 1966 1966 1966 ...\n $ location: int  1 1 1 1 1 1 1 1 1 1 ...\n $ girth   : logi  NA NA NA NA NA NA ...\n $ lgirth  : logi  NA NA NA NA NA NA ...\n\n\n\n\n\nNext, have a look at the fklngth data, just as we did in the last lab for t-tests. Create a histogram with density line grouped by year to get a feel for what‚Äôs happening with your data and a boxplot of length per year. What can you say about these data?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmygraph &lt;- ggplot(dam10dat, aes(x = fklngth)) +\n  labs(x = \"Fork length (cm)\") +\n  geom_density() +\n  geom_rug() +\n  geom_histogram(aes(y = ..density..),\n    color = \"black\",\n    alpha = 0.3\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(dam10dat$fklngth),\n      sd = sd(dam10dat$fklngth)\n    ),\n    color = \"red\"\n  )\n\n# display graph, by year\nmygraph + facet_wrap(~year, ncol = 2)\n\n\n\nDistribution of sturgeon length per year\n\n\n\n\nboxplot(fklngth ~ year, data = dam10dat)\n\n\n\nBoxplot of sturgeon length per year\n\n\n\n\n\nIt appears as though there may have been a small drop in fklngth after the construction of the dam, but the data are variable and the effects are not clear. There might also be some problems with normality in the 1954 and 1966 samples, and it looks as though there are outliers in the 1958 and 1966 samples. Let‚Äôs proceed with testing the assumptions of the ANOVA by running the analysis and looking at the residuals.\n\n11.2.2 Testing the assumptions of a parametric ANOVA\nParametric one-way ANOVAs have three major assumptions:\n\nthe residuals are normally distributed\nthe error variance is the same for all groups (homoscedasticity)\nthe residuals are independent.\n\nThese assumptions must be tested before we can accept the results of any parametric ANOVA.\n\nCarry out a one-way ANOVA on fklngth by year and produce the residual diagnostic plots\n\n\n# Fit anova model and plot residual diagnostics\nanova.model1 &lt;- lm(fklngth ~ year, data = dam10dat)\npar(mfrow = c(2, 2))\nplot(anova.model1)\n\n\n\nDiagnostic plots for a one-way ANOVA\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDouble check that the independent variable is a factor. If the dependent variable is a character, then you will obtain only 3 graphs and an error message like:\n`hat values (leverages) are all = 0.1\nand there are no factor predictors; no plot no. 5`\n\n\nD‚Äôapr√®s les graphiques, on peut douter de la normalit√© et de l‚Äôhomog√©n√©it√© des variances. Judging from the plots, it looks as though there may be problems with both normality and variance heterogeneity. Note that there is one point (case 59) with large expected values and a large residual that appear to lie well off the line: this is the outlier we noted earlier. This point might be expected to inflate the variance for the group it belongs to. Formal tests may also provide some insight as to whether we should be concerned about normality and variance heterogeneity.\n\nPerform a normality test on the residuals from the ANOVA.\n\n\nshapiro.test(residuals(anova.model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model1)\nW = 0.91571, p-value = 1.63e-06\n\n\nThis test confirms our suspicions from the probability plot: the residuals are not normally distributed. Recall, however, that the power here is high, so only small deviations from normality are required to reject the null.\n\nNext, test for homoscedasticity:\n\n\nleveneTest(fklngth ~ year, data = dam10dat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   3  2.8159 0.04234 *\n      114                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe probability value tells you that you can reject the null hypothesis that there is no difference in variances among years. Thus, we conclude there is evidence that the variances in the groups are not equal.\n\n11.2.3 Performing the ANOVA\nLet‚Äôs look at the results of the ANOVA, assuming for the moment that assumptions are met well enough.\n\nsummary(anova.model1)\n\n\nCall:\nlm(formula = fklngth ~ year, data = dam10dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2116  -2.6866  -0.7116   2.2103  26.7885 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.0243     0.8566  56.061  &lt; 2e-16 ***\nyear1958      0.1872     1.3335   0.140  0.88859    \nyear1965     -5.5077     1.7310  -3.182  0.00189 ** \nyear1966     -3.3127     1.1684  -2.835  0.00542 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.211 on 114 degrees of freedom\nMultiple R-squared:  0.1355,    Adjusted R-squared:  0.1128 \nF-statistic: 5.957 on 3 and 114 DF,  p-value: 0.0008246\n\n\n\n\nCoefficients: Estimates Note the 4 coefficients printed. They can be used to obtain the predicted values for the model (i.e.¬†the group means). The mean fklngth for the first year (1954) is 48.0243. The coefficients for the 3 other years are the difference between the mean for that year and for 1954. So, the mean for 1965 is (48.0243-5.5077=42.5166). For each estimated coefficient, there is a standard error, a t-value and associated probability (for H0 that the coefficient is 0). Note here that coefficients for 1965 and 1966 are both negative and significantly less than 0. Fish were smaller after the construction of the dam than in 1954. Take these p-values with a grain of salt: these are not corrected for multiple comparisons, and they constitute only a subset of the possible comparisons. In general, I pay little attention to this part of the output and look more at what comes next.\n\nResidual standard error: The square root of the variance of the residuals (observed minus fitted values) corresponds to the amount of variability that is unexplained by the models (here an estimate of how much size varied among fish, once corrected for differences among years)\n\nMutiple R-squared The R-squared is the proportion of the variance of the dependent variable that can be explained by the model. Here the model explains only 13.5% of the variability. Size differences among year are relatively small compared to the ranges of sizes that can occur within years. This corresponds well to the visual impression left by the histograms of fklngth per year\n\n\n\n\nF-Statistic This is the p-value for the ‚Äúomnibus‚Äù test, the test that all means are equal. Here it is much smaller than 0.05 and hence we would reject H0 and conclude that fklngth varies among the years\n\nThe anova() command produces the standard ANOVA table that contains most of the same information:\n\nanova(anova.model1)\n\nAnalysis of Variance Table\n\nResponse: fklngth\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nyear        3  485.26 161.755  5.9574 0.0008246 ***\nResiduals 114 3095.30  27.152                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe total variability in fklngth sums of square is partitioned into what can be accounted for by year (485.26) and what is left unexplained as residual variability (3095.30). Year indeed explains \\((485.26/(3095.30+485.26)=.1355\\) or 13.55% of the variability). The mean square of the residuals is their variance.\n\n11.2.4 Performing multiple comparisons of means test\n\nThe pairwise.t.test() function can be used to compare means and adjust (or not) probabilities for multiple comparisons by choosing one of the options for the argument p.adj:\n\nComparing all means without corrections for multiple comparisons.\n\npairwise.t.test(dam10dat$fklngth, dam10dat$year,\n  p.adj = \"none\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dam10dat$fklngth and dam10dat$year \n\n     1954   1958   1965  \n1958 0.8886 -      -     \n1965 0.0019 0.0022 -     \n1966 0.0054 0.0079 0.1996\n\nP value adjustment method: none \n\n\nOption \"bonf\" adjusts the p-values according to the Bonferroni correction. In this case, since there are 6 p-values calculated, it amounts to simply multiplying the uncorrected p-values by 6 (unless the result is above 1, in that case the adjusted p-value is 1).\n\npairwise.t.test(dam10dat$fklngth, dam10dat$year,\n  p.adj = \"bonf\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dam10dat$fklngth and dam10dat$year \n\n     1954  1958  1965 \n1958 1.000 -     -    \n1965 0.011 0.013 -    \n1966 0.033 0.047 1.000\n\nP value adjustment method: bonferroni \n\n\nOption \"holm\" is the sequential Bonferroni correction, where the p-values are ranked from (i=1) smallest to (N) largest. The correction factor for p-values is then $\\((N-i+1)\\). Here, for example, we have N=6 pairs that are compared. The lowest uncorrected p-value is 0.0019 for 1954 vs 1965. The corrected p-value becomes \\(0.0019*(6-1+1)= 0.011\\). The second lowest p-value is 0.0022. The corrected p/value is therefore \\(0.0022*(6-2+1)=0.011\\). For the highest p-value, the correction is \\((N-N+1)=1\\), hence it is equal to the uncorrected probability.\n\npairwise.t.test(dam10dat$fklngth, dam10dat$year,\n  p.adj = \"holm\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dam10dat$fklngth and dam10dat$year \n\n     1954  1958  1965 \n1958 0.889 -     -    \n1965 0.011 0.011 -    \n1966 0.022 0.024 0.399\n\nP value adjustment method: holm \n\n\nThe ‚Äúfdr‚Äù option is for controlling the false discovery rate.\n\npairwise.t.test(dam10dat$fklngth, dam10dat$year,\n  p.adj = \"fdr\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  dam10dat$fklngth and dam10dat$year \n\n     1954   1958   1965  \n1958 0.8886 -      -     \n1965 0.0066 0.0066 -     \n1966 0.0108 0.0119 0.2395\n\nP value adjustment method: fdr \n\n\nThe four post-hoc tests here tell us the same thing: differences are all between two groups of years: 1954/58 and 1965/66, since all comparisons show differences between the 50‚Äôs and 60‚Äôs but no differences within the 50‚Äôs or 60‚Äôs. So, in this particular case, the conclusion is not affected by the choice of adjustment method. But in other situations, you will observe contradictory results.\nWhich one to choose? Unadjusted p-values are certainly suspect when there are multiple tests. On the other hand, the traditional Bonferroni correction is very conservative, and becomes even more so when there are a large number of comparisons. Recent work suggest that the fdr approach may be a good compromise when there are a lot of comparisons. The Tukey method of multiple comparisons is one of the most popular and is easily performed with R (note, however, that there is a pesky bug that manifests itself when the independent variable can look like a number rather than a factor, hence the little pirouette with paste0() to add a letter m before the first digit):\n\ndam10dat$myyear &lt;- as.factor(paste0(\"m\", dam10dat$year))\nTukeyHSD(aov(fklngth ~ myyear, data = dam10dat))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fklngth ~ myyear, data = dam10dat)\n\n$myyear\n                  diff        lwr        upr     p adj\nm1958-m1954  0.1872141  -3.289570  3.6639986 0.9990071\nm1965-m1954 -5.5076577 -10.021034 -0.9942809 0.0100528\nm1966-m1954 -3.3126964  -6.359223 -0.2661701 0.0274077\nm1965-m1958 -5.6948718 -10.436304 -0.9534397 0.0116943\nm1966-m1958 -3.4999106  -6.875104 -0.1247171 0.0390011\nm1966-m1965  2.1949612  -2.240630  6.6305526 0.5710111\n\n\n\npar(mar = c(4, 7, 2, 1))\nplot(TukeyHSD(aov(fklngth ~ myyear, data = dam10dat)), las = 2)\n\n\n\nInter-annual differences in sturgeon length\n\n\n\nThe confidence intervals, corrected for multiple tests by the Tukey method, are plotted for differences among years. Unfortunately, the labels are not all printed because they would overlap, but the order is the same as in the preceding table. The multcomp üì¶ can produce a better plot version, but requires a bit more code:\n\n# Alternative way to compute Tukey multiple comparisons\n# set up a one-way ANOVA\nanova_fkl_year &lt;- aov(fklngth ~ myyear, data = dam10dat)\n# set up all-pairs comparisons for factor `year'\n\nmeandiff &lt;- glht(anova_fkl_year, linfct = mcp(\n  myyear =\n    \"Tukey\"\n))\nconfint(meandiff)\n\n\n     Simultaneous Confidence Intervals\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = fklngth ~ myyear, data = dam10dat)\n\nQuantile = 2.5928\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n                   Estimate lwr      upr     \nm1958 - m1954 == 0   0.1872  -3.2703   3.6447\nm1965 - m1954 == 0  -5.5077  -9.9960  -1.0194\nm1966 - m1954 == 0  -3.3127  -6.3423  -0.2831\nm1965 - m1958 == 0  -5.6949 -10.4100  -0.9798\nm1966 - m1958 == 0  -3.4999  -6.8564  -0.1435\nm1966 - m1965 == 0   2.1950  -2.2160   6.6059\n\npar(mar = c(5, 7, 2, 1))\nplot(meandiff)\n\n\n\nInter-annual differences in sturgeon length\n\n\n\nThis is better. Also useful is a plot the means and their confidence intervals with the Tukey groupings shown as letters above:\n\n# Compute and plot means and Tukey CI\nmeans &lt;- glht(\n  anova_fkl_year,\n  linfct = mcp(myyear = \"Tukey\")\n)\ncimeans &lt;- cld(means)\n# use sufficiently large upper margin\n# plot\nold_par &lt;- par(mai = c(1, 1, 1.25, 1))\nplot(cimeans)\n\n\n\nInter-annual differences in sturgeon length\n\n\n\nNote the letters appearing on top. Years labelled with the same letter do not differ significantly.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "33-anova.html#data-transformations-and-non-parametric-anova",
    "href": "33-anova.html#data-transformations-and-non-parametric-anova",
    "title": "\n11¬† One-way ANOVA\n",
    "section": "\n11.3 Data transformations and non-parametric ANOVA",
    "text": "11.3 Data transformations and non-parametric ANOVA\nIn the above example to examine differences in fklngth among years , we detected evidence of non-normality and variance heterogeneity. If the assumptions underlying a parametric ANOVA are not valid, there are several options:\n\nif sample sizes in each group are reasonably large, parametric ANOVA is reasonably robust with respect to the normality assumption, for the same reason that the t-test is, so the results are probably not too bad;\nwe can transform the data;\nwe can go the non-parametric route.\n\n\nRepeat the one-way ANOVA in the section above, but this time run the analysis on the log 10 fklngth . With this transformation, do some of the problems encountered previously disappear?\n\n\n# Fit anova model on log10 of fklngth and plot residual diagnostics\npar(mfrow = c(2, 2))\nanova.model2 &lt;- lm(log10(fklngth) ~ year, data = dam10dat)\nplot(anova.model2)\n\n\n\nDiagnostic plots for the ANOVA of sturgeon length by year\n\n\n\nLooking at the residuals, things look barely better than before without the log transformation. Running the Wilks-Shapiro test for normality on the residuals, we get:\n\nshapiro.test(residuals(anova.model2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model2)\nW = 0.96199, p-value = 0.002048\n\n\nSo, it would appear that we still have some problems with the assumption of normality and are just on the border line of meeting the assumption of homogeneity of variances. You have several choices here:\n\ntry to find a different transformation to satisfy the assumptions,\nassume the data are close enough to meeting the assumptions, or\nperform a non-parametric ANOVA.\n\n\nThe most commonly used non-parametric analog of the parametric one-way ANOVA is the Kruskall-Wallis one-way ANOVA. Perform a Kruskall-Wallis one-way ANOVA of fklngth , and compare these results to the parametric analysis above. What do you conclude?\n\n\nkruskal.test(fklngth ~ year, data = dam10dat)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fklngth by year\nKruskal-Wallis chi-squared = 15.731, df = 3, p-value = 0.001288\n\n\nSo, the conclusion is the same as with the parametric ANOVA: we reject the null that the mean rank is the same for each year. Thus, despite violation of one or more assumptions, the parametric analysis is telling us the same thing as the non-parametric analysis: the conclusion is, therefore, quite robust.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "33-anova.html#dealing-with-outliers",
    "href": "33-anova.html#dealing-with-outliers",
    "title": "\n11¬† One-way ANOVA\n",
    "section": "\n11.4 Dealing with outliers",
    "text": "11.4 Dealing with outliers\nOur preliminary analysis of the relationship between fklngth and year suggested there might be some outliers in the data. These were evident in the box plots of fklngth by year and flagged as cases 59, 23 and 87 in the residual probability plot and residual-fit plot. In general, you have to have very good reasons for removing outliers from a data set (e.g., you know there was a mistake made in the data collection/entry). However, it is often useful to know how the analysis changes if you remove the outliers from the data set.\n\nRepeat the original ANOVA of fklngth by year but work with a subset of the data without the outliers. Have any of the conclusions changed?\n\n\ndamsubset &lt;- dam10dat[-c(23, 59, 87), ] # removes obs 23, 59 and 87\naov_damsubset &lt;- aov(fklngth ~ as.factor(year), damsubset)\nsummary(aov_damsubset)\n\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(year)   3  367.5  122.50   6.894 0.000267 ***\nResiduals       111 1972.4   17.77                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nshapiro.test(residuals(aov_damsubset))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(aov_damsubset)\nW = 0.98533, p-value = 0.2448\n\n\n\nleveneTest(fklngth ~ year, damsubset)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   3  4.6237 0.004367 **\n      111                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nElimination of three outliers, in this case, makes things better in terms of the normality assumption, but does not improve the variances. Moreover, the fact that the conclusion drawn from the original ANOVA with outliers retained does not change upon their removal reinforces the fact that there is no good reason to remove the points. Instead of a Kruskall-Wallis rank-based test, a permutation test could be used.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "33-anova.html#permutation-test",
    "href": "33-anova.html#permutation-test",
    "title": "\n11¬† One-way ANOVA\n",
    "section": "\n11.5 Permutation test",
    "text": "11.5 Permutation test\nThis is an example for a more complex way of doing permutation that we used when lmPerm was not available.\n\n#############################################################\n# Permutation Test for one-way ANOVA\n# modified from code written by David C. Howell\n# http://www.uvm.edu/~dhowell/StatPages/\n# More_Stuff/Permutation%20Anova/PermTestsAnova.html\n# set desired number of permutations\nnreps &lt;- 500\n# to simplify reuse of this code, copy desired dataframe to mydata\nmydata &lt;- dam10dat\n# copy model formula to myformula\nmyformula &lt;- as.formula(\"fklngth ~ year\")\n# copy dependent variable vector to mydep\nmydep &lt;- mydata$fklngth\n# copy independent variable vector to myindep\nmyindep &lt;- as.factor(mydata$year)\n################################################\n# You should not need to modify code chunk below\n################################################\n# Compute observed F value for original sample\nmod1 &lt;- lm(myformula, data = mydata) # Standard Anova\nsum_anova &lt;- summary(aov(mod1)) # Save summary to variable\nobs_f &lt;- sum_anova[[1]]$\"F value\"[1] # Save observed F value\n# Print standard ANOVA results\ncat(\n  \" The standard ANOVA for these data follows \",\n  \"\\n\"\n)\n\nprint(sum_anova, \"\\n\")\ncat(\"\\n\")\ncat(\"\\n\")\nprint(\"Resampling as in Manly with unrestricted sampling of observations. \")\n\n# Now start resampling\nboot_f &lt;- numeric(nreps) # initalize vector to receive permuted\nvalues\nboot_f[1] &lt;- obs_f\nfor (i in 2:nreps) {\n  newdependent &lt;- sample(mydep, length(mydep)) # randomize dep\n  var\n  mod2 &lt;- lm(newdependent ~ myindep) # refit model\n  b &lt;- summary(aov(mod2))\n  boot_f[i] &lt;- b[[1]]$\"F value\"[1] # store F stats\n}\npermprob &lt;- length(boot_f[boot_f &gt;= obs_f]) / nreps\ncat(\n  \" The permutation probability value is: \", permprob,\n  \"\\n\"\n)\n# end of code chunk for permutation\n\nVersion lmPerm du test de permutation.\n\n## lmPerm version of permutation test\nlibrary(lmPerm)\n# for generality, copy desired dataframe to mydata\n# and model formula to myformula\nmydata &lt;- dam10dat\nmyformula &lt;- as.formula(\"fklngth ~ year\")\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate permutation p-value\nanova(lmp(myformula, data = mydata, perm = \"Prob\", center = FALSE, Ca = 0.001))",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>One-way ANOVA</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html",
    "href": "34-anova_mult.html",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "",
    "text": "12.1 R packages and data needed\nFor this lab you need:\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚úñ dplyr::recode() masks car::recode()\n‚úñ dplyr::select() masks MASS::select()\n‚úñ purrr::some()   masks car::some()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(effects)\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#set-anomul",
    "href": "34-anova_mult.html#set-anomul",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "",
    "text": "R packages:\n\ntidyverse\nmulticomp\ncar\neffects\n\n\ndata files:\n\nStu2wdat.csv\nStu2mdat.csv\nnr2wdat.csv\nnestdat.csv\nwmcdat2.csv\nwmc2dat2.csv",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#two-way-factorial-design-with-replication",
    "href": "34-anova_mult.html#two-way-factorial-design-with-replication",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.2 Two-way factorial design with replication",
    "text": "12.2 Two-way factorial design with replication\nMany experiments are designed to investigate the joint effects of several different factors: in a two-way ANOVA, we examine the effect of two factors, but in principle the analysis can be extended to three, four or even five factors, although interpreting the results from 4- and 5-way ANOVAs can be very difficult.\nSuppose that we are interested in the effects of two factors: location (Cumberland House and The Pas) and sex (male or female) on sturgeon size (data can be found in Stu2wdat.csv). Note that because the sample sizes are not the same for each group, this is an unbalanced design. Note also that there are missing data for some of the variables, meaning that not every measurement was made on every fish.\n\n12.2.1 Fixed effects ANOVA (Model I)\n\nBegin by having a look at the data by generating box plots of rdwght for sex and location from the file Stu2wdat.csv .\n\n\n\n\n\n\n\nSolution\n\n\n\n\nStu2wdat &lt;- read.csv(\"data/Stu2wdat.csv\")\n\nggplot(Stu2wdat, aes(x = sex, y = rdwght)) +\ngeom_boxplot(notch = TRUE) +\nfacet_grid(~location)\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nFrom this, it appears as though females might be larger at both locations. It‚Äôs difficult to get an idea of whether fish differ in size between the two locations. The presence of outliers on these plots suggests there might be problems meeting normality assumptions for the residuals.\n\nGenerate summary statistics for rdwght by sex and location .\n\n\nStu2wdat %&gt;%\n  group_by(sex, location) %&gt;%\n  summarise(\n    mean = mean(rdwght, na.rm = TRUE), sd = sd(rdwght, na.rm = TRUE), n = n()\n  )\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 √ó 5\n# Groups:   sex [2]\n  sex            location        mean    sd     n\n  &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 \"FEMALE      \" \"CUMBERLAND  \"  27.4  9.33    51\n2 \"FEMALE      \" \"THE_PAS     \"  28.0 12.5     55\n3 \"MALE        \" \"CUMBERLAND  \"  22.1  4.79    34\n4 \"MALE        \" \"THE_PAS     \"  20.6  9.92    46\n\n\nThe summary statistics confirm our interpretation of the box plots: females appear to be larger than males, and differences in fish size between locations are small.\n\nUsing the file Stu2wdat.csv , do a two-way factorial ANOVA:\n\n\n# Fit anova model and plot residual diagnostics\n# but first, save current par and set graphic page to hold 4 graphs\nopar &lt;- par(mfrow = c(2, 2))\nanova.model1 &lt;- lm(rdwght ~ sex + location + sex:location,\n  contrasts = list(sex = contr.sum, location = contr.sum),\n  data = Stu2wdat\n)\nanova(anova.model1) \n\nAnalysis of Variance Table\n\nResponse: rdwght\n              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1  1839.6 1839.55 18.6785 2.569e-05 ***\nlocation       1     4.3    4.26  0.0433    0.8355    \nsex:location   1    48.7   48.69  0.4944    0.4829    \nResiduals    178 17530.4   98.49                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful here. R gives you the sequential sums of squares (Type I) and associated Mean squares and probabilities. These are not to be trusted unless the design is perfectly balanced. In this case, there are varying numbers of observations across sex and location combinations and therefore the design is not balanced.\n\n\nWhat you want are the partial sums of squares (type III). The easiest way to get them is to use the Anova() function in the car üì¶ package (note the subtle difference, Anova() is not the same as anova(), remember case matters in R.). However, this is not enough by itself. To get the proper values for the type III sums of square, one also needs to specify contrasts, hence the cryptic contrasts = list(sex = contr.sum,location = contr.sum).\n\nlibrary(car)\nAnova(anova.model1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex            1745   1   17.7220 4.051e-05 ***\nlocation          9   1    0.0891    0.7656    \nsex:location     49   1    0.4944    0.4829    \nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn the basis of the ANOVA, there is no reason to reject two null hypotheses: (1) that the effect of sex (if any) does not depend on location (no interaction), and (2) that there is no difference in the size of sturgeon (pooled over sex ) between the two locations . On the other hand, we reject the null hypothesis that there is no difference in size between male and female sturgeon (pooled over location ), precisely as expected from the graphs.\n\npar(mfrow = c(2, 2))\nplot(anova.model1)\n\n\n\nChecking model assumptions for ANOVA model1\n\n\n\nAs usual, we cannot accept the above results without first ensuring that the assumptions of ANOVA are met. Examination of the residuals plots above shows that the residuals are reasonably normally distributed, with the exception of three potential outliers flagged on the QQ plot (cases 101, 24, & 71; the latter two are on top of one another). However, Cook‚Äôs distances are not large for these (the 0.5 contour is not even visible on the plot), so there is little indication that these are a concern.The residuals vs fit plot shows that the spread of residuals is about equal over the range of the fitted values, again with the exception of a few cases. When we test for normality of residuals we get:\n\nshapiro.test(residuals(anova.model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model1)\nW = 0.87213, p-value = 2.619e-11\n\n\nSo, there is evidence of non-normality in the residuals.\nWe will use the Levene‚Äôs test to examine the assumption of homogeneity of variances, just as we did with the 1-way anova.\n\nleveneTest(rdwght ~ sex * location, data = Stu2wdat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   3  3.8526 0.01055 *\n      178                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf the assumption of homogeneity of variances was valid, we would be accepting the null that the mean of the absolute values of residuals does not vary among levels of sex and location (i.e., group ). The above table shows that the hypothesis is rejected and we conclude there is evidence of heteroscedascticity. All in all, there is some evidence that several important assumptions have been violated. However, whether these violations are sufficiently large to invalidate our conclusions remains to be seen.\n\n\n\n\n\n\nExercise\n\n\n\nRepeat this procedure using the data file Stu2mdat.Rdata . Now what do you conclude? Suppose you wanted to compare the sizes of males and females: in what way would these comparisons differ between Stu2wdat.Rdata and Stu2mdat.Rdata ?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nStu2mdat &lt;- read.csv(\"data/Stu2mdat.csv\")\nanova.model2 &lt;- lm(\n  formula = rdwght ~ sex + location + sex:location,\n  contrasts = list(sex = contr.sum, location = contr.sum),\n  data = Stu2mdat\n)\nsummary(anova.model2)\nAnova(anova.model2, type = 3)\n\n\n\n\n\n\nCall:\nlm(formula = rdwght ~ sex + location + sex:location, data = Stu2mdat, \n    contrasts = list(sex = contr.sum, location = contr.sum))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.917  -6.017  -0.580   4.445  65.743 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     24.5346     0.7461  32.885  &lt; 2e-16 ***\nsex1            -0.5246     0.7461  -0.703    0.483    \nlocation1        0.2227     0.7461   0.299    0.766    \nsex1:location1   3.1407     0.7461   4.210 4.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.924 on 178 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.09744,   Adjusted R-squared:  0.08223 \nF-statistic: 6.405 on 3 and 178 DF,  p-value: 0.0003817\n\n\nNote that in this case, we see that at Cumberland House, females are larger than males, whereas the opposite is true in The Pas (you can confirm this observation by generating summary statistics). What happens with the ANOVA (remember, you want Type III sum of squares)?\n\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex              49   1    0.4944    0.4829    \nlocation          9   1    0.0891    0.7656    \nsex:location   1745   1   17.7220 4.051e-05 ***\nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, the interaction term sex:location is significant but the main effects are not significant.\n\nYou might find it useful here to generate plots for the two data files to compare the interactions between sex and location. The effect plot shows the relationship between means for each combination of factors (also called cell means). Generate an effect plot for the two models using the allEffects() command from the effects üì¶ package:\n\n\nlibrary(effects)\nallEffects(anova.model1)\n\n model: rdwght ~ sex + location + sex:location\n\n sex*location effect\n              location\nsex            CUMBERLAND   THE_PAS     \n  FEMALE           27.37347     27.97717\n  MALE             22.14118     20.64652\n\nplot(allEffects(anova.model1), \"sex:location\")\n\n\n\nEffet du sexe et du lieu sur le poids des esturgeons\n\n\n\n\nallEffects(anova.model2)\n\n model: rdwght ~ sex + location + sex:location\n\n sex*location effect\n              location\nsex            CUMBERLAND   THE_PAS     \n  FEMALE           27.37347     20.64652\n  MALE             22.14118     27.97717\n\nplot(allEffects(anova.model2), \"sex:location\")\n\n\n\nEffet du sexe et du lieu sur le poids des esturgeons\n\n\n\nThere is a very large difference between the results from Stu2wdat and Stu2mdat. In the former case, because there is no significant interaction, we can essentially pool over the levels of factor 1 (sex, say) to test for the effects of location , or over the levels of factor 2 (location) to test for the effects of sex . In fact, if we do so and simply run a one-way ANOVA on the Stu2wdat data with sex as the grouping variable, we get:\n\nAnova(aov(rdwght ~ sex, data = Stu2wdat), type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n            Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)  78191   1 800.440 &lt; 2.2e-16 ***\nsex           1840   1  18.831 2.377e-05 ***\nResiduals    17583 180                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that here the residual sum of squares (17583) is only slightly higher than for the 2-way model (17530), simply because, in the 2-way model, only a small fraction of the explained sums of squares is due to the location main effect or the sex:LOCATION interaction. On the other hand, if you try the same trick with stu2mdat, you get:\n\n\nAnova(aov(rdwght ~ sex, data = Stu2mdat), type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n            Sum Sq  Df  F value Pr(&gt;F)    \n(Intercept)  55251   1 515.0435 &lt;2e-16 ***\nsex            113   1   1.0571 0.3053    \nResiduals    19309 180                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, the residuals sum of squares (19309) is much larger than in the 2-way model (17530), because most of the explained sums of squares is due to the interaction. Note that if we did this, we would conclude that male and female sturgeons don‚Äôt differ in size. But in fact they do: it‚Äôs just that the difference is in different directions, depending on location. This is why it is always dangerous to try and make too much of main effects in the presence of interactions!\n\n12.2.2 Mixed effects ANOVA (Model III)\nWe have neglected an important component in the above analyses, and that is related to the type of ANOVA model we wish to run. In this example, Location could be considered a random effect, whereas sex is a fixed effect (because it is ‚Äúfixed‚Äù biologically), and so this model could be treated as a mixed model (Model III) ANOVA. Note that in these analyses, R treats analyses by default as Model I ANOVA, so that the main effects and the interaction are tested over the residuals mean square. Recall, however, that in a Model III ANOVA, main effects are tested over the interaction mean square or the pooled interaction mean square and residual mean square (depending on which statistician you consult!)\n\nWorking with the Stu2wdat data, rebuild the ANOVA table for rdwght for the situation in which location is a random factor and sex is a fixed factor. To do this, you need to recalculate the F-ratio for sex using the sex:location interaction mean square instead of the residual mean square. This is most easily accomplished by hand, making sure you are working with the Type III Sums of squares ANOVA table.\n\n\n\n\n\n\n\nSolution\n\n\n\n\nAnova(anova.model1, type = 3)\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex            1745   1   17.7220 4.051e-05 ***\nlocation          9   1    0.0891    0.7656    \nsex:location     49   1    0.4944    0.4829    \nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor sex, the new ratio of mean squares is\n\\[F = \\frac{(1745/1)}{(49/1)} = 35.6\\]\nTo assign a probability to the new F-value, enter the following in the commands window: pf(F, df1, df2, lower.tail = FALSE) , where F is the newly calculated F-value, and df1 and df2 are the degrees of freedom of the numerator (sex) and denominator (SEX:location), respectively.\n\npf(35.6, 1, 1, lower.tail = FALSE)\n\n[1] 0.1057152\n\n\nNote that the p value for sex is now non-significant. This is because the error MS of the initial ANOVA is smaller than the interaction MS, but mostly because the number of degrees of freedom of the denominator of the F test has dropped from 178 to 1. In general, a drop in the denominator degrees of freedom makes it much more difficult to reach significance.\n\n\n\n\n\n\nNote\n\n\n\nMixed model which are a generalisation of mixed-effect ANOVA are now really developped and are to be favoured intead of doing it by hand.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#way-factorial-anova-without-replication",
    "href": "34-anova_mult.html#way-factorial-anova-without-replication",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.3 2-way factorial ANOVA without replication",
    "text": "12.3 2-way factorial ANOVA without replication\nIn some experimental designs, there are no replicates within data cells: perhaps it is simply too expensive to obtain more than one datum per cell. A 2-way ANOVA is still possible under these circumstances, but there is an important limitation.\n\n\n\n\n\n\nWarning\n\n\n\nBecause there is no replication within cells, there is no error variance: we have simply a row sum of squares, a column sum of squares, and a remainder sum of squares. This has important implications: if there is an interaction in a Model III ANOVA, only the fixed effect can be tested (over the remainder MS); for Model I ANOVAs, or for random effects in Model III ANOVAs, it is not appropriate to test main effects over the remainder unless we are sure there is no interaction.\n\n\nA limnologist studying Round Lake in Algonquin Park takes a single temperature ( temp ) reading at 10 different depths ( depth , in m) at four times ( date) over the course of the summer. Her data are shown in Nr2wdat.csv.\n\nDo a two-way unreplicated ANOVA using temp as the dependent vari able, date and depth as the factor variables (you will need to recode depth to tell R to treat this variable as a factor). Note that there is no interaction term included in this model.\n\n\nnr2wdat &lt;- read.csv(\"data/nr2wdat.csv\")\nnr2wdat$depth &lt;- as.factor(nr2wdat$depth)\nanova.model4 &lt;- lm(temp ~ date + depth, data = nr2wdat)\nAnova(anova.model4, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: temp\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 1511.99  1 125.5652 1.170e-11 ***\ndate         591.15  3  16.3641 2.935e-06 ***\ndepth       1082.82  9   9.9916 1.450e-06 ***\nResiduals    325.12 27                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAssuming that this is a Model III ANOVA ( date random, depth fixed), what do you conclude? (Hint: you may want to generate an interaction plot of temp versus depth and month, just to see what‚Äôs going on.)\n\ninteraction.plot(nr2wdat$depth, nr2wdat$date, nr2wdat$temp)\n\n\n\nEffet du mois et de la profondeur sur la temp√©rature\n\n\n\nThere is a highly significant decrease in temperature as depth increases. To test the effect of month (the (assumed) random factor), we must assume that there is no interaction between depth and month, i.e.¬†that the change in temperature with depth is the same for each month. This is a dubious assumption: if you plot temperature against depth for each month, you should see that the temperature profile becomes increasingly non-linear as the summer progresses (i.e.¬†the thermocline develops), from almost a linear decline in early spring to what amounts to a step decline in August. In other words, the relationship between temperature and depth does change with month, so that if you were to use the above fitted model to estimate, say, the temperature at a depth of 5 m in July, you would not get a particularly good estimate.\nIn terms of residual diagnostics, have a look at the residuals probability plot and residuals vs fitted values plot.\n\npar(mfrow = c(2, 2))\nplot(anova.model4)\n\n\n\nConditions d‚Äôapplications du mod√®le anova.model4\n\n\n\n\nshapiro.test(residuals(anova.model4))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model4)\nW = 0.95968, p-value = 0.1634\n\n\nTesting the residuals for normality, we get p = 0.16, so that the normality assumption seems to be O.K. In terms of heteroscedasticity, we can only test among months, using depths as replicates (or among depths using months as replicates). Using depths as replicates within months, we find\n\nleveneTest(temp ~ date, data = nr2wdat)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(&gt;F)    \ngroup  3  17.979 2.679e-07 ***\n      36                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo there seems to be some problem here, as can be plainly seen in the above plot of residuals vs fit. All in all, this analysis is not very satisfactory: there appears to be some problems with the assumptions, and the assumption of no interaction between depth and date would appear to be invalid.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#nested-designs",
    "href": "34-anova_mult.html#nested-designs",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.4 Nested designs",
    "text": "12.4 Nested designs\nA common experimental design occurs when each major group (or treatment) is divided into randomly chosen subgroups. For example, a geneticist interested in the effects of genotype on desiccation resistance in fruit flies might conduct an experiment with larvae of three different genotypes. For each genotype (major group), she sets up three environmental chambers (sub-groups, replicates within groups) with a fixed temperature humidity regime, and in each chamber, she has five larvae for which she records the number of hours each larvae survived.\n\nThe file Nestdat.csv contains the results of just such an experi ment. The file lists three variables: genotype , chamber and survival . Run a nested ANOVA with survival as the dependent variable, genotype/chamber as the independent variables (this is the shorthand notation for a chamber effect nested under genotype).\n\n\nnestdat &lt;- read.csv(\"data/nestdat.csv\")\nnestdat$chamber &lt;- as.factor(nestdat$chamber)\nnestdat$genotype &lt;- as.factor(nestdat$genotype)\nanova.nested &lt;- lm(survival ~ genotype / chamber, data = nestdat)\n\nWhat do you conclude from this analysis? What analysis would (should) you do next? (Hint: if there is a non-significant effect of chambers within genotypes, then you can increase the power of between-genotype comparisons by pooling over chambers within genotypes, although not everyone (Dr.¬†Rundle included) agrees with such pooling.) Do it! Make sure you check your assumptions!\n\n\n\n\n\n\nSolution\n\n\n\n\nanova(anova.nested)\n\nAnalysis of Variance Table\n\nResponse: survival\n                 Df  Sum Sq Mean Sq  F value Pr(&gt;F)    \ngenotype          2 2952.22 1476.11 292.6081 &lt;2e-16 ***\ngenotype:chamber  6   40.65    6.78   1.3432 0.2639    \nResiduals        36  181.61    5.04                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow = c(2, 2))\nplot(anova.nested)\n\n\n\nConditions d‚Äôapplications du mod√®le anova.nested\n\n\n\n\n\nWe conclude from this analysis that there is no (significant) variation among chambers within genotypes, but that the null hypothesis that all genotypes have the same dessiccation resistance (as measured by survival) is rejected (Test of genotype using MS genotype:chamber as denominator: F = 1476.11/6.78 = 217.7153, P&lt;0.0001). In other words, genotypes differ in their survival.\nSince the chambers within genotypes effect is non-significant, we may want to pool over chambers to increase our degrees of freedom:\n\nanova.simple &lt;- lm(survival ~ genotype, data = nestdat)\nanova(anova.simple)\n\nAnalysis of Variance Table\n\nResponse: survival\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ngenotype   2 2952.22 1476.11  278.93 &lt; 2.2e-16 ***\nResiduals 42  222.26    5.29                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus, we conclude that there is significant variation among the three genotypes in dessiccation resistance.\nA box plot of survival across genotypes shows clearly that there is significant variation among the three genotypes in dessiccation resistance. This can be combined with a formal Tukey multiple comparison test:\n\npar(mfrow = c(1, 1))\n# Compute and plot means and Tukey CI\nmeans &lt;- glht(anova.simple, linfct = mcp(\n  genotype =\n    \"Tukey\"\n))\ncimeans &lt;- cld(means)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(cimeans, las = 1) # las option to put y-axis labels as God intended them\n\n\n\nEffet du genotype sur la r√©sistance √† la dessication avec un test de Tukey\n\n\n\nSo, we conclude from the Tukey analysis and plot that dessiccation resistance (R) , as measured by larval survival under hot, dry conditions, varies significantly among all three genotypes with R(AA) &gt; R(Aa) &gt; R(aa).\nBefore concluding this, however, we must test the assumptions. Here are the residual plots and diagnostics for the one-way (unnested) design:\n\n\n\n\n\n\nSolution\n\n\n\n\npar(mfrow = c(2, 2))\nplot(anova.simple)\n\n\n\nConditions d‚Äôapplications du mod√®le anova.simple\n\n\n\n\n\nSo, all the assumptions appear to be valid, and the conclusion reached above still holds. Note that if you compare the residual mean squares of the nested and one-way ANOVAs (5.04 vs 5.29), they are almost identical. This is not surprising, given the small contribution of the chamber %in% genotype effect to the explained sum of squares.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#two-way-non-parametric-anova",
    "href": "34-anova_mult.html#two-way-non-parametric-anova",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.5 Two-way non-parametric ANOVA",
    "text": "12.5 Two-way non-parametric ANOVA\nTwo-way non-parametric ANOVA is an extension of the non-parametric one-way methods discussed previously. The basic procedure is to rank all the data in the sample from smallest to largest, then carry out a 2-way ANOVA on the ranks. This can be done either for replicated or unreplicated data.\nUsing the data file Stu2wdat.csv , do a two-factor ANOVA to examine the effects of sex and location on rank(rdwght).\n\naov.rank &lt;- aov(\n  rank(rdwght) ~ sex * location,\n  contrasts = list(\n    sex = contr.sum, location = contr.sum\n  ),\n  data = Stu2wdat\n)\n\nThe Scheirer-Ray-Hare extension of the Kruskall-Wallis test is done by computing a statistic H given by the effect sums of squares (SS) divided by the total MS. The latter can be calculated as the variance of the ranks. We compute an H statistic for each term. The H-statistics are then compared to a theoretical \\(\\chi^2\\) (chi-square) distribution using the command line: pchisq(H, df, lower.tail = FALSE) , where H and df are the calculated H-statistics and associated degrees of freedom, respectively.\n\nUse the ANOVA table based on ranks to test the effects of sex and on rdwght. What do you conclude? How does this result compare with the result obtained with the parametric 2-way ANOVA done before?\n\n\nAnova(aov.rank, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rank(rdwght)\n              Sum Sq  Df  F value    Pr(&gt;F)    \n(Intercept)  1499862   1 577.8673 &lt; 2.2e-16 ***\nsex            58394   1  22.4979 4.237e-06 ***\nlocation        1128   1   0.4347    0.5105    \nsex:location    1230   1   0.4738    0.4921    \nResiduals     472383 182                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo calculate the Scheirer-Ray-Hare extension to the Kruskall-Wallis test, you must first calculate the total mean square (MS), i.e.¬†the variance of the ranked data. In this case, there are 186 observations, their ranks are therefore the series 1, 2, 3, ‚Ä¶, 186. The variance can be calculated simply as var(1:186) (Isn‚Äôt R neat? Cryptic maybe, but neat). So we can compute the H statistic for each term:\n\nHsex &lt;- 58394 / var(1:186)\nHlocation &lt;- 1128 / var(1:186)\nHsexloc &lt;- 1230 / var(1:186)\n\nAnd convert these statistics into p-values:\n\n# sex\nHsex\n\n[1] 20.14628\n\npchisq(Hsex, 1, lower.tail = FALSE)\n\n[1] 7.173954e-06\n\n# location\nHlocation\n\n[1] 0.3891668\n\npchisq(Hlocation, 1, lower.tail = FALSE)\n\n[1] 0.5327377\n\n# sex:location\nHsexloc\n\n[1] 0.4243574\n\npchisq(Hsexloc, 1, lower.tail = FALSE)\n\n[1] 0.5147707\n\n\nNote that these results are the same as those obtained in our original two-way parametric ANOVA. Despite the reduced power, we still find significant differences between the sexes, but still no interaction and no effect due to location.\nThere is, however, an important difference. Recall that in the original parametric ANOVA, there was a significant effect of sex when we considered the problem as a Model I ANOVA. However, if we consider it as Model III, the significant sex effect could in principle disappear, because the df associated with the interaction MS are much smaller than the df associated with the Model I error MS. In this case, however, the interaction MS is about half that of the error MS. So, the significant sex effect becomes even more significant if we analyze the problem as a Model III ANOVA. Once again, we see the importance of specifying the appropriate ANOVA design.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#multiple-comparisons",
    "href": "34-anova_mult.html#multiple-comparisons",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.6 Multiple comparisons",
    "text": "12.6 Multiple comparisons\nFurther hypothesis testing in multiway ANOVAs depends critically on the outcome of the initial ANOVA. If you are interested in comparing groups of marginal means (that is, means of treatments for one factor pooled over levels of the other factor, e.g., between male and female sturgeon pooled over location), this can be done exactly as outlined for multiple comparisons for one-way ANOVAs. For comparison of individual cell means, you must specify the interaction as the group variable.\nThe file wmcdat2.csv shows measured oxygen consumption ( o2cons ) of two species ( species = A, B)) of limpets at three different concentrations of seawater ( conc = 100, 75, 50%) taken from Sokal and Rohlf, 1995, p.¬†332.\n\nRun a 2-way factorial ANOVA on wmcdat2 data, using o2cons as the dependent variable and species and conc as the factors. What do you conclude?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nwmcdat2 &lt;- read.csv(\"data/wmcdat2.csv\")\nwmcdat2$species &lt;- as.factor(wmcdat2$species)\nwmcdat2$conc &lt;- as.factor(wmcdat2$conc)\nanova.model5 &lt;- lm(o2cons ~ species * conc, data = wmcdat2)\nAnova(anova.model5, type = 3)\n\n\n\nThe ANOVA table is shown below. Technically, because the sample sizes in individual cells are rather small, this analysis should be repeated using a non-parametric ANOVA. For the moment, let‚Äôs stick with the parametric analysis.\n\n\nAnova Table (Type III tests)\n\nResponse: o2cons\n              Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept)  1185.60  1 124.0165 4.101e-14 ***\nspecies         0.09  1   0.0097   0.92189    \nconc           74.90  2   3.9172   0.02755 *  \nspecies:conc   23.93  2   1.2514   0.29656    \nResiduals     401.52 42                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLook at the diagnostic plots:\n\n\n\n\n\n\nSolution\n\n\n\n\npar(mfrow = c(2, 2))\nplot(anova.model5)\n\n\n\n\n\n\n\n\n\nHomoscedasticity looks ok, but normality less so.. Testing for normality, we get:\n\n\n\n\n\n\nSolution\n\n\n\n\nshapiro.test(residuals(anova.model5))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model5)\nW = 0.93692, p-value = 0.01238\n\n\n\n\nSo there is evidence of non-normality, but otherwise everything looks O.K. Since the ANOVA is relatively robust with respect to non-normality, we proceed, but if we wanted to reassure ourselves, we could run a non-parametric ANOVA, and get the same answer.\n\nOn the basis of the ANOVA results obtained above, which means would you proceed to compare? Why?\n\n\n\n\n\n\n\nSolution\n\n\n\nNeed to add an explnation here\n\n\nOverall, we conclude that there are no differences among species, and that the effect of concentration does not depend on species (no interaction). Since there is no interaction and no main effect due to species, the only comparison of interest is among salinity concentrations:\n\n# fit simplified model\nanova.model6 &lt;- aov(o2cons ~ conc, data = wmcdat2)\n# Make Tukey multiple comparisons\nTukeyHSD(anova.model6)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = o2cons ~ conc, data = wmcdat2)\n\n$conc\n           diff       lwr        upr     p adj\n75-50  -4.63625 -7.321998 -1.9505018 0.0003793\n100-50 -3.25500 -5.940748 -0.5692518 0.0141313\n100-75  1.38125 -1.304498  4.0669982 0.4325855\n\npar(mfrow = c(1, 1))\n# Graph of all comparisons for conc\ntuk &lt;- glht(anova.model6, linfct = mcp(conc = \"Tukey\"))\n# extract information\ntuk.cld &lt;- cld(tuk)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(tuk.cld)\n\n\n\nComparaison de Tukey des moyennes de consommation d‚Äôoxyg√®n en fonction del la concentration\n\n\npar(old.par)\n\nSo there is evidence of a significant difference in oxygen consumption at a reduction in salinity to 50% of regular seawater, but not at a reduction of only 25%.\n\nRepeat the analysis described above using wmc2dat2.csv . How do your results compare with those obtained for wmcdat2.csv ?\n\n\n\n\n\n\n\nSolution\n\n\n\n\nwmc2dat2 &lt;- read.csv(\"data/wmc2dat2.csv\")\nwmc2dat2$species &lt;- as.factor(wmc2dat2$species)\nwmc2dat2$conc &lt;- as.factor(wmc2dat2$conc)\nanova.model7 &lt;- lm(o2cons ~ species * conc, data = wmc2dat2)\n\n\n\nUsing wmc2dat2.csv,we get:\n\n\nAnova Table (Type III tests)\n\nResponse: o2cons\n             Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  343.09  1 36.2132 3.745e-07 ***\nspecies      133.52  1 14.0929 0.0005286 ***\nconc          66.76  2  3.5232 0.0385011 *  \nspecies:conc 168.15  2  8.8742 0.0006101 ***\nResiduals    397.91 42                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere there is a large interaction effect, and consequently, there is no point in comparing marginal means. This is made clear by examining an interaction plot:\n\nwith(wmc2dat2, interaction.plot(conc, species, o2cons))\n\n\n\n\n\n\n\n\nWorking still with the wmc2dat2 data set, compare individual cell means (6 in all), with the Bonferonni adjustment. To do this, it is helpful to create a new variable to indicate all the combinations of species and conc:\n\n\nwmc2dat2$species.conc &lt;- as.factor(paste0(wmc2dat2$species, wmc2dat2$conc))\n\nThen we can conduct pairwise bonferroni comparisons:\n\nwith(wmc2dat2, pairwise.t.test(o2cons, species.conc, p.adj = \"bonf\"))\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  o2cons and species.conc \n\n     A100   A50    A75    B100   B50   \nA50  0.1887 -      -      -      -     \nA75  1.0000 1.0000 -      -      -     \nB100 0.7223 1.0000 1.0000 -      -     \nB50  1.0000 0.0079 0.0929 0.0412 -     \nB75  0.6340 1.0000 1.0000 1.0000 0.0350\n\nP value adjustment method: bonferroni \n\n\nThese comparisons are a little more difficult to interpret, but the analysis essentially examines for differences among seawater concentrations within species A and for differences among concentrations within species B. We see here that the o2Cons at 50% seawater for species B is significantly different from that of 75% and 100% seawater for species B, whereas there are no significant differences in o2cons for species A across all seawater concentrations.\nI find these outputs rather unsatisfying because they show only p-values, but no indication of effect size. One can get both the conclusion from the multiple comparison procedure and an indication of effect size from the graph produced with the following code:\n\n# fit one-way anova comparing all combinations of species.conc combinations\nanova.modelx &lt;- aov(o2cons ~ species.conc, data = wmc2dat2)\ntuk2 &lt;- glht(anova.modelx, linfct = mcp(species.conc = \"Tukey\"))\n# extract information\ntuk2.cld &lt;- cld(tuk2)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(tuk2.cld)\n\n\n\n\n\n\npar(old.par)\n\nNote that in this analysis, we have used the error MS = 9.474 from the original model to contrast cell means. Recall, however, that this assumes that in fact we are dealing with a Model I ANOVA, which may or may not be the case ( conc is certainly a fixed factor, but species might be either fixed or random).",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#test-de-permutation-pour-lanova-√†-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#test-de-permutation-pour-lanova-√†-deux-facteurs-de-classification",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.7 Test de permutation pour l‚ÄôANOVA √† deux facteurs de classification",
    "text": "12.7 Test de permutation pour l‚ÄôANOVA √† deux facteurs de classification\nWhen data do not meet the assumptions of the parametric analysis in two- and multiway ANOVA, as an alternative to the non-parametric ANOVA, it is possible to run permutation tests to calculate p-values. The lmPerm package does this easily.\n\n#######################################################################\n## lmPerm version of permutation test\nlibrary(lmPerm)\n# for generality, copy desired dataframe to mydata\n# and model formula to myformula\nmydata &lt;- Stu2wdat\nmyformula &lt;- as.formula(\"rdwght ~ sex+location+sex:location\")\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate permutation p-value\nanova(lmp(myformula, data = mydata, perm = \"Prob\", center = FALSE, Ca = 0.001))\n\nlmPerm was orphaned for a while and the code below, while clunkier, provided an alternative way of doing it. You would have to adapt it for other situations.\n\n###########################################################\n# Permutation test for two way ANOVA\n# Ter Braak creates residuals from cell means and then permutes across\n# all cells\n# This can be accomplished by taking residuals from the full model\n# modified from code written by David C. Howell\n# http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Permutation%20Anova/PermTestsAnova.html\nnreps &lt;- 500\ndependent &lt;- Stu2wdat$rdwght\nfactor1 &lt;- as.factor(Stu2wdat$sex)\nfactor2 &lt;- as.factor(Stu2wdat$location)\nmy.dataframe &lt;- data.frame(dependent, factor1, factor2)\nmy.dataframe.noNA &lt;- my.dataframe[complete.cases(my.dataframe), ]\nmod &lt;- lm(dependent ~ factor1 + factor2 + factor1:factor2,\n  data = my.dataframe.noNA\n)\nres &lt;- mod$residuals\nTBint &lt;- numeric(nreps)\nTB1 &lt;- numeric(nreps)\nTB2 &lt;- numeric(nreps)\nANOVA &lt;- summary(aov(mod))\ncat(\n  \" The standard ANOVA for these data follows \",\n  \"\\n\"\n)\nF1 &lt;- ANOVA[[1]]$\"F value\"[1]\nF2 &lt;- ANOVA[[1]]$\"F value\"[2]\nFinteract &lt;- ANOVA[[1]]$\"F value\"[3]\nprint(ANOVA)\ncat(\"\\n\")\ncat(\"\\n\")\nTBint[1] &lt;- Finteract\nfor (i in 2:nreps) {\n  newdat &lt;- sample(res, length(res), replace = FALSE)\n  modb &lt;- summary(aov(newdat ~ factor1 + factor2 +\n    factor1:factor2,\n  data = my.dataframe.noNA\n  ))\n  TBint[i] &lt;- modb[[1]]$\"F value\"[3]\n  TB1[i] &lt;- modb[[1]]$\"F value\"[1]\n  TB2[i] &lt;- modb[[1]]$\"F value\"[2]\n}\nprobInt &lt;- length(TBint[TBint &gt;= Finteract]) / nreps\nprob1 &lt;- length(TB1[TB1 &gt;= F1]) / nreps\nprob2 &lt;- length(TB2[TB1 &gt;= F2]) / nreps\ncat(\"\\n\")\ncat(\"\\n\")\nprint(\"Resampling as in ter Braak with unrestricted sampling\nof cell residuals. \")\ncat(\n  \"The probability for the effect of Interaction is \",\n  probInt, \"\\n\"\n)\ncat(\n  \"The probability for the effect of Factor 1 is \",\n  prob1, \"\\n\"\n)\ncat(\n  \"The probability for the effect of Factor 2 is \",\n  prob2, \"\\n\"\n)",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#bootstrap-for-two-way-anova",
    "href": "34-anova_mult.html#bootstrap-for-two-way-anova",
    "title": "\n12¬† Multiway ANOVA: factorial and nested designs\n",
    "section": "\n12.8 Bootstrap for two-way ANOVA",
    "text": "12.8 Bootstrap for two-way ANOVA\nIn most cases, permutation tests will be more appropriate than bootstrap in ANOVA designs. However, for the sake of completedness, I have a snippet of code to do bootstrap for you::\n\n############################################################\n###########\n# Bootstrap for two-way ANOVA\n# You possibly want to edit bootfunction.mod1 to return other values\n# Here it returns the standard coefficients of the fitted model\n# Requires boot library\n#\nnreps &lt;- 5000\ndependent &lt;- Stu2wdat$rdwght\nfactor1 &lt;- as.factor(Stu2wdat$sex)\nfactor2 &lt;- as.factor(Stu2wdat$location)\nmy.dataframe &lt;- data.frame(dependent, factor1, factor2)\nmy.dataframe.noNA &lt;- my.dataframe[complete.cases(my.dataframe), ]\nlibrary(boot)\n# Fit model on observed data\nmod1 &lt;- aov(dependent ~ factor1 + factor2 + factor1:factor2,\n  data = my.dataframe.noNA\n)\n\n\n# Bootstrap 1000 time using the residuals bootstraping methods to\n# keep the same unequal number of observations for each level of the indep. var.\nfit &lt;- fitted(mod1)\ne &lt;- residuals(mod1)\nX &lt;- model.matrix(mod1)\nbootfunction.mod1 &lt;- function(data, indices) {\n  y &lt;- fit + e[indices]\n  bootmod &lt;- lm(y ~ X)\n  coefficients(bootmod)\n}\nbootresults &lt;- boot(my.dataframe.noNA, bootfunction.mod1,\n  R = 1000\n)\nbootresults\n## Calculate 90% CI and plot bootstrap estimates separately for each model parameter\nboot.ci(bootresults, conf = 0.9, index = 1)\nplot(bootresults, index = 1)\nboot.ci(bootresults, conf = 0.9, index = 3)\nplot(bootresults, index = 3)\nboot.ci(bootresults, conf = 0.9, index = 4)\nplot(bootresults, index = 4)\nboot.ci(bootresults, conf = 0.9, index = 5)\nplot(bootresults, index = 5)",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Multiway ANOVA: factorial and nested designs</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html",
    "href": "35-reg_mult.html",
    "title": "\n13¬† Multiple regression\n",
    "section": "",
    "text": "13.1 R packages and data\nFor this lab you need:",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#set-reg-mul",
    "href": "35-reg_mult.html#set-reg-mul",
    "title": "\n13¬† Multiple regression\n",
    "section": "",
    "text": "R packages:\n\nggplot2\ncar\nlmtest\nsimpleboot\nboot\nMuMIn\n\n\ndata files:\n\nMregdat.csv",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#points-to-keep-in-mind",
    "href": "35-reg_mult.html#points-to-keep-in-mind",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.2 Points to keep in mind",
    "text": "13.2 Points to keep in mind\nMultiple regression models are used in cases where there is one dependent variable and several independent, continuous variables. In many biological systems, the variable of interest may be influenced by several different factors, so that accurate description or prediction requires that several independent variables be included in the regression model. Before beginning, be aware that multiple regression takes time to learn well. Beginners should keep in mind several important points:\n\nAn overall regression model may be statistically significant even if none of the individual regression coefficients in the model are (caused by multicollinearity)\nA multiple regression model may be ‚Äúnonsignificant‚Äù even though some of the individual coefficients are ‚Äúsignificant‚Äù (caused by overfitting)\nUnless ‚Äúindependent‚Äù variables are uncorrelated in the sample, different model selection procedures may yield different results.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#first-look-at-the-data",
    "href": "35-reg_mult.html#first-look-at-the-data",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.3 First look at the data",
    "text": "13.3 First look at the data\nThe file Mregdat.Rdata contains data collected in 30 wetlands in the Ottawa-Cornwall-Kingston area. The data included are\n\nthe richness (number of species) of:\n\nbirds (bird , and its log transform logbird),\nplants (plant, logpl),\nmammals (mammal, logmam),\nherptiles (herptile, logherp)\ntotal species richness of all four groups combined (totsp, logtot)\n\n\nGPS coordinates of the wetland (lat , long)\nits area (logarea)\nthe percentage of the wetland covered by water at all times during the year (swamp)\nthe percentage of forested land within 1 km of the wetland (cpfor2)\nthe density (in m/hectare) of hard-surface roads within 1 km of the wetland (thtden).\n\nWe will focus on herptiles for this exercise, so we better first have a look at how this variable is distributed and correlated to the potential independent variables:\n\nmydata &lt;- read.csv(\"data/Mregdat.csv\")\nscatterplotMatrix(\n  ~ logherp + logarea + cpfor2 + thtden + swamp,\n  regLine = TRUE, smooth = TRUE, diagonal = TRUE,\n  data = mydata\n)\n\n\n\nMatrice de r√©lation et densit√© pour la richesse sp√©cifique des amphibiens et reptiles",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#multiple-regression-models-from-scratch",
    "href": "35-reg_mult.html#multiple-regression-models-from-scratch",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.4 Multiple regression models from scratch",
    "text": "13.4 Multiple regression models from scratch\nWe begin the multiple regression exercise by considering a situation with one dependent variable and three (possibly) independent variables. First, we will start from scratch and build a multiple regression model based on what we know from building simple regression models. Next, we will look at automated methods of building multiple regressions models using simultaneous, forward, and backward stepwise procedures.\n\n\n\n\n\n\nExercise\n\n\n\nUsing the subset of the Mregdat.csv data file, regress logherp on logarea.\n\n\nOn the basis of the regression, what do you conclude?\n\nmodel_loga &lt;- lm(logherp ~ logarea, data = mydata)\nsummary(model_loga)\n\n\nCall:\nlm(formula = logherp ~ logarea, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38082 -0.09265  0.00763  0.10409  0.46977 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.18503    0.15725   1.177 0.249996    \nlogarea      0.24736    0.06536   3.784 0.000818 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1856 on 26 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3552,    Adjusted R-squared:  0.3304 \nF-statistic: 14.32 on 1 and 26 DF,  p-value: 0.0008185\n\npar(mfrow = c(2, 2))\nplot(model_loga)\n\n\n\nChecking model asusmptions for regression of logherp as a function of logarea\n\n\n\nIt looks like there is a positive relationship between herptile species richness and wetland area: the larger the wetland, the greater the number of species. Note, however, that about 2/3 of the observed variability in species richness among wetlands is not ‚Äúexplained‚Äù by wetland area (R2 = 0.355). Residual analysis shows no major problems with normality, heteroscedasticity or independence of residuals.\n\n\n\n\n\n\nExercise\n\n\n\nRerun the above regression, this time replacing logarea with cpfor2 as the independent variable, such that the expression in the formula field reads: logherp ~ cpfor2 . What do you conclude?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_logcp &lt;- lm(logherp ~ cpfor2, data = mydata)\nsummary(model_logcp)\n\n\nCall:\nlm(formula = logherp ~ cpfor2, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49095 -0.10266  0.05881  0.16027  0.25159 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.609197   0.104233   5.845 3.68e-06 ***\ncpfor2      0.002706   0.001658   1.632    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2202 on 26 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.09289,   Adjusted R-squared:  0.058 \nF-statistic: 2.662 on 1 and 26 DF,  p-value: 0.1148\n\n\n\n\nAccording to this result, we would accept the null hypothesis, and conclude that there is no relationship between herptile density and the proportion of forest on adjacent lands. But what happens when we enter both variables into the regression simultaneously?\n\n\n\n\n\n\nExercise\n\n\n\nRerun the above regression one more time, this time adding both independent variables into the model at once, such that logherp ~ logarea + cpfor2 . What do you conclude?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_mcp &lt;- lm(logherp ~ logarea + cpfor2, data = mydata)\nsummary(model_mcp)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40438 -0.11512  0.01774  0.08187  0.36179 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.027058   0.166749   0.162 0.872398    \nlogarea     0.247789   0.061603   4.022 0.000468 ***\ncpfor2      0.002724   0.001318   2.067 0.049232 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.175 on 25 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4493,    Adjusted R-squared:  0.4052 \nF-statistic:  10.2 on 2 and 25 DF,  p-value: 0.0005774\n\n\n\n\nNow we reject both null hypotheses that the slope of the regression of logherp on logarea is zero and that the slope of the regression of logherp on cpfor2 is zero.\nWhy is cpfor2 a significant predictor of logherp in the combined model when it was not significant in the simple linear model? The answer lies in the fact that it is sometimes necessary to control for one variable in order to detect the effect of another variable. In this case, there is a significant relationship between logherp and logarea that masks the relationship between logherp and cpfor2 . When both variables are entered into the model at once, the effect of logarea is controlled for, making it possible to detect a cpfor2 effect (and vice versa).\n\n\n\n\n\n\nExercise\n\n\n\nRun another multiple regression, this time substituting thtden for cpfor2 as an independent variable (logherp ~ logarea + thtden).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_mden &lt;- lm(logherp ~ logarea + thtden, data = mydata)\nsummary(model_mden)\n\n\nCall:\nlm(formula = logherp ~ logarea + thtden, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31583 -0.12326  0.02095  0.13201  0.31674 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.37634    0.14926   2.521 0.018437 *  \nlogarea      0.22504    0.05701   3.947 0.000567 ***\nthtden      -0.04196    0.01345  -3.118 0.004535 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1606 on 25 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5358,    Adjusted R-squared:  0.4986 \nF-statistic: 14.43 on 2 and 25 DF,  p-value: 6.829e-05\n\n\n\n\nIn this case we reject the null hypotheses that there are no effects of wetland area ( logarea ) and road density ( thtden ) on herptile richness ( logherp ). Note here that road density has a negative effect on richness, whereas wetland area and forested area ( cpfor2; results from previous regression) both have positive effects on herptile richness.\nThe R2 of this model is even higher than the previous multiple regression model, reflecting a higher correlation between logherp and thtden than between logherp and cpfor2 (if you run a simple regression between logherp and thtden and compare it to the cpfor2 regression you should be able to detect this).\nThus far, it appears that herptile richness is related to wetland area ( logarea ), road density ( thtden ), and possibly forest cover on adjacent lands ( cpfor2 ). But, does it necessarily follow that if we build a regression model with all three independent variables, that all three will show significant relationships? No, because we have not yet examined the relationship between Logarea , cpfor2 and thtden . Suppose, for example, two of the variables (say, cpfor2 and thtden ) are perfectly correlated. Then the thtden effect is nothing more than the cpfor2 effect (and vice versa), so that once we include one or the other in the regression model, none of the remaining variability would be explained by the third variable.\n\n\n\n\n\n\nExercise\n\n\n\nFit a regression model with logherp as the dependent variable and logarea , cpfor2 and thtden as the independent variables. What do you conclude?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_mtri &lt;- lm(logherp ~ logarea + cpfor2 + thtden, data = mydata)\nsummary(model_mtri)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30729 -0.13779  0.02627  0.11441  0.29582 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.284765   0.191420   1.488 0.149867    \nlogarea      0.228490   0.057647   3.964 0.000578 ***\ncpfor2       0.001095   0.001414   0.774 0.446516    \nthtden      -0.035794   0.015726  -2.276 0.032055 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1619 on 24 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5471,    Adjusted R-squared:  0.4904 \nF-statistic: 9.662 on 3 and 24 DF,  p-value: 0.0002291\n\n\n\n\nSeveral things to note here:\n\nThe regression coefficient for cpfor2 has become non-significant: once the variability explained by logarea and thtden is removed, a non-significant part of the remaining variability is explained by cpfor2.\nThe R2 for this model (.547 is only marginally larger than the R2 for the model with only logarea and thtden (.536, which is again consistent with the non-significant coefficient for cpfor2.\n\nNote also that although the regression coefficient for thtden has not changed much from that obtained when just thtden and logarea were included in the fitted model (-.036 vs -.042, the standard error for the regression coefficient for thtden has increased slightly, meaning the estimate is less precise. If the correlation between thtden and cpfor2 was greater, the change in precision would also be greater.\nWe can compare the fit of the last two models (i.e., the model with all 3 variables and the model with only logarea and thtden to decide which model is best to include.\n\nanova(model_mtri, model_mden)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + cpfor2 + thtden\nModel 2: logherp ~ logarea + thtden\n  Res.Df     RSS Df Sum of Sq     F Pr(&gt;F)\n1     24 0.62937                          \n2     25 0.64508 -1 -0.015708 0.599 0.4465\n\n\nNote that this is the identical result we obtained via the t-test of the effect of cpfor2 in the model with all 3 variables above as they are testing the same thing (this should make sense to you). From this analysis, we would conclude that the full model with all three variables included does not offer a significant improvement in fit over the model with only logarea and thtden. This isn‚Äôt surprising given that we already know that we cannot reject the null hypothesis of no effect of cpfor2 in the full model. Overall, we would conclude, on the basis of these analyses, that:\n\nGiven the three variables thtden , logarea and cpfor2 , the best model is one that includes the first two variables.\nThere is evidence of a negative relationship between herptile richness and the density of roads on adjacent lands.\nThere is evidence that the larger the wetland area, the greater the herptile species richness. Note that by ‚Äúbest‚Äù, I don‚Äôt mean the best possible model, I mean the best one given the three predictor variables we started with. It seems pretty clear that there are other factors controlling richness in wetlands, since even with the ‚Äúbest‚Äù model, almost half of the variability in richness is unexplained.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#stepwise-multiple-regression-procedures",
    "href": "35-reg_mult.html#stepwise-multiple-regression-procedures",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.5 Stepwise multiple regression procedures",
    "text": "13.5 Stepwise multiple regression procedures\nThere are a number of techniques available for selecting the multiple regression model that best suits your data. When working with only three independent variables it is often sufficient to work through the different combinations of possible variables yourself, until you are satisfied you have fit the best model. This is, essentially, what we did in the first section of this lab. However, the process can become tedious when dealing with numerous independent variables, and you may find an automatic procedure for fitting models to be easier to work with.\nStepwise regression in R relies on the Akaike Information Criterion, as a measure of goodness of fit\n\\[AIC = 2k + 2ln(L))\\]\nwhere k is the number of regressors, and L is the maximized value of the likelihood function for the model). This is a statistic that rewards prediction precision while penalizing model complexity. If a new model has an AIC lower than that of the current model, the new model is a better fit to the data.\n\n\n\n\n\n\nExercise\n\n\n\nStill working with the Mregdat data, run a stepwise multiple regression on the same set of variables:\n\n\n\n# Stepwise Regression\nstep_mtri &lt;- step(model_mtri, direction = \"both\")\n\nStart:  AIC=-98.27\nlogherp ~ logarea + cpfor2 + thtden\n\n          Df Sum of Sq     RSS     AIC\n- cpfor2   1   0.01571 0.64508 -99.576\n&lt;none&gt;                 0.62937 -98.267\n- thtden   1   0.13585 0.76522 -94.794\n- logarea  1   0.41198 1.04135 -86.167\n\nStep:  AIC=-99.58\nlogherp ~ logarea + thtden\n\n          Df Sum of Sq     RSS     AIC\n&lt;none&gt;                 0.64508 -99.576\n+ cpfor2   1   0.01571 0.62937 -98.267\n- thtden   1   0.25092 0.89600 -92.376\n- logarea  1   0.40204 1.04712 -88.013\n\nstep_mtri$anova # display results\n\n      Step Df   Deviance Resid. Df Resid. Dev       AIC\n1          NA         NA        24  0.6293717 -98.26666\n2 - cpfor2  1 0.01570813        25  0.6450798 -99.57640\n\n\nExamining the output, we find:\n\nR calculated the AIC for the starting model (here the full model with the 3 independent variables.\nThe AIC for models where terms are deleted. Note here that the only way to reduce the AIC is to drop 2.\nThe AIC for models where terms are added or deleted from the model selected in the first step (i.e.¬†logherp ~ logarea + thtden. Note that none of these models are better.\n\nInstead of starting from the full (saturated) model and removing and possibly re-adding terms (i.e.¬†direction = ‚Äúboth‚Äù), one can start from the null model and only add terms:\n\n# Forward selection approach\nmodel_null &lt;- lm(logherp ~ 1, data = mydata)\nstep_f &lt;- step(\n  model_null,\n  scope = ~ . + logarea + cpfor2 + thtden, direction = \"forward\"\n)\n\nStart:  AIC=-82.09\nlogherp ~ 1\n\n          Df Sum of Sq    RSS     AIC\n+ logarea  1   0.49352 0.8960 -92.376\n+ thtden   1   0.34241 1.0471 -88.013\n+ cpfor2   1   0.12907 1.2605 -82.820\n&lt;none&gt;                 1.3895 -82.091\n\nStep:  AIC=-92.38\nlogherp ~ logarea\n\n         Df Sum of Sq     RSS     AIC\n+ thtden  1   0.25093 0.64508 -99.576\n+ cpfor2  1   0.13078 0.76522 -94.794\n&lt;none&gt;                0.89600 -92.376\n\nStep:  AIC=-99.58\nlogherp ~ logarea + thtden\n\n         Df Sum of Sq     RSS     AIC\n&lt;none&gt;                0.64508 -99.576\n+ cpfor2  1  0.015708 0.62937 -98.267\n\nstep_f$anova # display results\n\n       Step Df  Deviance Resid. Df Resid. Dev       AIC\n1           NA        NA        27  1.3895281 -82.09073\n2 + logarea -1 0.4935233        26  0.8960048 -92.37639\n3  + thtden -1 0.2509250        25  0.6450798 -99.57640\n\n\nYou should first notice that the final result is the same as the default stepwise regression and as what we got building the model from scratch. In forward selection, R first fits the least complex model (i.e, with only an intercept), and then adds variables, one by one, according to AIC statistics. Thus, in the above example, the model was first fit with only an intercept. Next, logarea was added, followed by thtden. cpfor2 was not added because it would make AIC increase to above that of the model fit with the first two variables. Generally speaking, when doing multiple regressions, it is good practice to try several different methods (e.g.¬†all regressions, stepwise, and backward elimination, etc.) and see whether you get the same results. If you don‚Äôt, then the ‚Äúbest‚Äù model may not be so obvious, and you will have to think very carefully about the inferences you draw. In this case, regardless of whether we use automatic, or forward/backward stepwise regression, we arrive at the same model.\nWhen doing multiple regression, always bear in mind the following:\n\nDifferent procedures may produce different ‚Äúbest‚Äù models, i.e.¬†the ‚Äúbest‚Äù model obtained using forward stepwise regression needn‚Äôt necessarily be the same as that obtained using backward stepwise. It is good practice to try several different methods and see whether you end up with the same result. If you don‚Äôt, it is almost invariably due to multicollinearity among the independent variables.\n\nBe wary of stepwise regression. As the authors of SYSTAT, another commonly used statistical package, note:\n\nStepwise regression is probably the most abused computerized statistical technique ever devised. If you think you need automated stepwise regression to solve a particular problem, you probably don‚Äôt. Professional statisticians rarely use automated stepwise regression because it does not necessarily find the ‚Äúbest‚Äù fitting model, the ‚Äúreal‚Äù model, or alternative ‚Äúplausible‚Äù models. Furthermore, the order in which variables enter or leave a stepwise program is usually of no theoretical significance. You are always better off thinking about why a model could generate your data and then testing that model.\n\n\nRemember that just because there is a significant regression of Y on X doesn‚Äôt mean that X causes Y: correlation does not imply causation!",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#detecting-multicollinearity",
    "href": "35-reg_mult.html#detecting-multicollinearity",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.6 Detecting multicollinearity",
    "text": "13.6 Detecting multicollinearity\nMulticollinearity is the presence of correlations among independent variables. In extreme cases (perfect collinearity) it will prevent you from fitting some models.\n\n\n\n\n\n\nWarning\n\n\n\nWhen collinearity is not perfect, it reduces your ability to test for the effect of individual variables, but does not affect the ability of the model to predict.\n\n\nThe help file for the HH üì¶package contains this clear passage about one of the indices of multicollinearity, the variance inflation factors:\n\nA simple diagnostic of collinearity is the variance inflation factor, VIF one for each regression coefficient (other than the intercept). Since the condition of collinearity involves the predictors but not the response, this measure is a function of the X‚Äôs but not of Y. The VIF for predictor i is \\[1/(1-R_i^2)\\] where Ri2 is the R2 from a regression of predictor i against the remaining predictors. If Ri2 is close to 1, this means that predictor i is well explained by a linear function of the remaining predictors, and, therefore, the presence of predictor i in the model is redundant. Values of VIF exceeding 5 are considered evidence of collinearity: The information carried by a predictor having such a VIF is contained in a subset of the remaining predictors. If, however, all of a model‚Äôs regression coefficients differ significantly from 0 (p-value &lt; .05), a somewhat larger VIF may be tolerable.\n\nVIFs indicate by how much the variance of each regression coefficient is increased by the presence of collinearity.\n\n\n\n\n\n\nNote\n\n\n\nThere are several vif() functions (I know of at least three in the packages car, HH and DAAG) and I do not know if and how they differ.\n\n\nTo quantify multicollinarity, one can simply call the vif() function from the package car:\n\nlibrary(car)\nvif(model_mtri)\n\n logarea   cpfor2   thtden \n1.022127 1.344455 1.365970 \n\n\nHere there is no evidence that multicollinearity is a problem since all vif are close to 1.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#polynomial-regression",
    "href": "35-reg_mult.html#polynomial-regression",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.7 Polynomial regression",
    "text": "13.7 Polynomial regression\nIn the regression models considered so far, we have assumed that the relationship between the dependent and independent variables is linear. If not, in some cases it can be made linear by transforming one or both variables. On the other hand, for many biological relationships no transformation in the world will help, and we are forced to go with some sort of non-linear regression method.\nThe simplest type of nonlinear regression method is polynomial regression, in which you fit regression models that include independent variables raised to some power greater than one, e.g.¬†X2, X3, etc.\n\n\n\n\n\n\nExercise\n\n\n\nPlot the relationship between the residuals of the logherp ~ logarea regression and swamp.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n# probl√®me avec les donn√©es de manquantes dans logherp\nmysub &lt;- subset(mydata, !is.na(logherp))\n# ajouter les r√©sidus dans les donn√©e\nmysub$resloga &lt;- residuals(model_loga)\nggplot(data = mysub, aes(y = resloga, x = swamp)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nRelation entre swamp et les r√©sidus de la r√©gression entre logherp et logarea\n\n\n\n\n\nVisual inspection of this graph suggests that there is a strong, but highly nonlinear, relationship between these two variables.\n\n\n\n\n\n\nExercise\n\n\n\nTry regressing the residuals of the logherp ~ logarea regression on swamp. What do you conclude?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_resloga &lt;- lm(resloga ~ swamp, mysub)\nsummary(model_resloga)\n\n\nCall:\nlm(formula = resloga ~ swamp, data = mysub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35088 -0.13819  0.00313  0.10849  0.45802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.084571   0.109265   0.774    0.446\nswamp       -0.001145   0.001403  -0.816    0.422\n\nResidual standard error: 0.1833 on 26 degrees of freedom\nMultiple R-squared:  0.02498,   Adjusted R-squared:  -0.01252 \nF-statistic: 0.666 on 1 and 26 DF,  p-value: 0.4219\n\n\n\n\nIn other words, the fit is terrible, even though you can see from the graph that there is in fact quite a strong relationship between the two - it‚Äôs just that it is a non-linear relationship. (If you look at model assumptions for this model, you will see strong evidence of nonlinearity, as expected) The pattern might be well described by a quadratic relation.\n\n\n\n\n\n\nExercise\n\n\n\nRerun the above regression but add a second term in the Formula field to represent swamp2 . If you simply add swamp2 in the model R won‚Äôt fit a quadratic effect, you need to use the functionI() which indicates that the formula within should be evaluated before fitting the model.\nThe expression should appear as:\n\\[ residuals ~ swamp + I(swamp^2)\\].\nWhat do you conclude? What does examination of the residuals from this multiple regression tell you?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_resloga2 &lt;- lm(resloga ~ swamp + I(swamp^2), mysub)\nsummary(model_resloga2)\n\n\nCall:\nlm(formula = resloga ~ swamp + I(swamp^2), data = mysub)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.181185 -0.085350  0.007377  0.067327  0.242455 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.804e-01  1.569e-01  -4.975 3.97e-05 ***\nswamp        3.398e-02  5.767e-03   5.892 3.79e-06 ***\nI(swamp^2)  -2.852e-04  4.624e-05  -6.166 1.90e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1177 on 25 degrees of freedom\nMultiple R-squared:  0.6132,    Adjusted R-squared:  0.5823 \nF-statistic: 19.82 on 2 and 25 DF,  p-value: 6.972e-06\n\npar(mfrow = c(2, 2))\nplot(model_resloga2)\n\n\n\n\n\n\n\n\n\nIt is clear that once the effects of area are controlled for, a considerable amount of the remaining variability in herptile richness is explained by swamp , in a nonlinear fashion. If you examine model assumptions, you will see that compared to the linear model, the fit is much better.\nBased on the results from the above analyses, how would you modify the regression model arrived at above? What, in your view, is the ‚Äúbest‚Äù overall model? Why? How would you rank the various factors in terms of their effects on herptile species richness?\nIn light of these results, we might want to try and fit a model which includes logarea, thtden, cpfor2, swamp and swamp^2^ :\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_poly1 &lt;- lm(\n  logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nsummary(model_poly1)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), \n    data = mydata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.201797 -0.056170 -0.002072  0.051814  0.205626 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.203e-01  1.813e-01  -1.766   0.0912 .  \nlogarea      2.202e-01  3.893e-02   5.656 1.09e-05 ***\ncpfor2      -7.864e-04  9.955e-04  -0.790   0.4380    \nthtden      -2.929e-02  1.048e-02  -2.795   0.0106 *  \nswamp        3.113e-02  5.898e-03   5.277 2.70e-05 ***\nI(swamp^2)  -2.618e-04  4.727e-05  -5.538 1.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1072 on 22 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8181,    Adjusted R-squared:  0.7767 \nF-statistic: 19.78 on 5 and 22 DF,  p-value: 1.774e-07\n\n\n\n\nNote that on the basis of this analysis, we could potentially drop cpfor2 and refit using the remaining variables:\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_poly2 &lt;- lm(\n  logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nsummary(model_poly2)\n\n\nCall:\nlm(formula = logherp ~ logarea + thtden + swamp + I(swamp^2), \n    data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19621 -0.05444 -0.01202  0.07116  0.21295 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.461e-01  1.769e-01  -1.957   0.0626 .  \nlogarea      2.232e-01  3.842e-02   5.810 6.40e-06 ***\nthtden      -2.570e-02  9.364e-03  -2.744   0.0116 *  \nswamp        2.956e-02  5.510e-03   5.365 1.89e-05 ***\nI(swamp^2)  -2.491e-04  4.409e-05  -5.649 9.46e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1063 on 23 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8129,    Adjusted R-squared:  0.7804 \nF-statistic: 24.98 on 4 and 23 DF,  p-value: 4.405e-08\n\n\n\n\nHow about multicollinearity in this model?\n\nvif(model_poly2)\n\n   logarea     thtden      swamp I(swamp^2) \n  1.053193   1.123491  45.845845  45.656453 \n\n\nVIF for the two swamp terms are much higher than the standard threshold of 5. However, this is expected for polynomial terms, and not really a concern given that both terms are highly significant in the model. The high VIF means that these two coefficients are not estimated precisely, but using both in the model still allows to make a good prediction (i.e.¬†account for the response to swamp).",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#checking-assumptions-of-a-multiple-regression-model",
    "href": "35-reg_mult.html#checking-assumptions-of-a-multiple-regression-model",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.8 Checking assumptions of a multiple regression model",
    "text": "13.8 Checking assumptions of a multiple regression model\nAll the model selection techniques or the manual model crafting assumes that the standard assumptions (independence, normality, homoscedasticity, linearity) are met. Given that a large number of models can be fitted, it may seem that testing the assumptions at each step would be an herculean task. However, it is generally sufficient to examine the residuals of the full (saturated) model and of the final model. Terms not contributing significantly to the fit do not affect residuals much, and therefore, the residuals to the full model, or the residuals to the final model, are generally sufficient.\nLet‚Äôs have a look at the diagnostic plots for the final model. Here we use the check_model() function from the performance üì¶.\n\n\n\n\n\n\nSolution\n\n\n\n\nlibrary(performance)\ncheck_model(model_poly2)\n\n\n\nConditions d‚Äôapplication du mod√®le model_poly2\n\n\n\n\n\nAlternatively it can be done with the classic method\n\n\n\n\n\n\nCode\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model_poly2)\n\n\n\nConditions d‚Äôapplication du mod√®le model_poly2\n\n\n\n\n\nEverything looks about right here. For the skeptic, let‚Äôs run the formal tests.\n\nshapiro.test(residuals(model_poly2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model_poly2)\nW = 0.9837, p-value = 0.9278\n\n\nThe residuals do not deviate from normality. Good.\n\nlibrary(lmtest)\nbptest(model_poly2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_poly2\nBP = 3.8415, df = 4, p-value = 0.4279\n\n\nNo deviation from homoscedasticity either. Good.\n\ndwtest(model_poly2)\n\n\n    Durbin-Watson test\n\ndata:  model_poly2\nDW = 1.725, p-value = 0.2095\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nNo serial correlation in the residuals, so no evidence of non-independence.\n\nresettest(model_poly2, type = \"regressor\", data = mydata)\n\n\n    RESET test\n\ndata:  model_poly2\nRESET = 0.9823, df1 = 8, df2 = 15, p-value = 0.4859\n\n\nAnd no significant deviation from linearity. So it seems that all is fine.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#visualizing-effect-size",
    "href": "35-reg_mult.html#visualizing-effect-size",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.9 Visualizing effect size",
    "text": "13.9 Visualizing effect size\nHow about effect size? How is that measured or viewed? The regression coefficients can be used to measure effect size, although it may be better to standardize them so that they become independent of measurement units. But a graph is often useful as well. In this context, some of the most useful graphs are called partial residual plots (or component + residual plots). These plots show how the dependent variable, corrected for other variables in the model, varies with each individual variable. Let‚Äôs have a look:\n\n# Evaluate visually linearity and effect size\n# component + residual plot\ncrPlots(model_poly2)\n\n\n\nGraphiques de r√©sidus partiels du mod√®le model_poly2\n\n\n\nNote that the vertical scale varies among plots. For thtden, the dependent variable (log10(herptile richness)) varies by about 0.4 units over the range of thtden in the sample. For logarea, the variation is about 0.6 log units. For swamp, it is a bit tricky since there are two terms and they have opposite effect (leading to a peaked relationship), so the plots are less informative. However, there is no deviation from linearity to be seen.\nTo illustrate what these graphs would look like if there was deviation from linearity, let‚Äôs drop swamp2 term and produce the graphs and run the RESET test\n\n\n\n\n\n\nSolution\n\n\n\n\nmodel_nopoly &lt;- lm(\n  logherp ~ logarea + thtden + swamp,\n  data = mydata\n)\ncrPlots(model_nopoly)\n\n\n\nGraphiques de r√©sidus partiels du mod√®le model_nopoly\n\n\n\n\n\nThe lack of linearity along the gradient of swamp becomes obvious. The RESET test also detects a violation from linearity:\n\nresettest(model_nopoly, type = \"regressor\")\n\n\n    RESET test\n\ndata:  model_nopoly\nRESET = 6.7588, df1 = 6, df2 = 18, p-value = 0.0007066",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#testing-for-interactions",
    "href": "35-reg_mult.html#testing-for-interactions",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.10 Testing for interactions",
    "text": "13.10 Testing for interactions\nWhen there are multiple independent variables one should always be ready to assess interactions. In most multiple regression contexts this is somewhat difficult because adding interaction terms increases overall multicollinearity and because in many cases there are not enough observations to test all interactions, or the observations are not well balanced to make powerful tests for interactions. Going back to our final model, see what happens if one tries to fit the fully saturated model with all interactions:\n\nfullmodel_withinteractions &lt;- lm(\n  logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2),\n  data = mydata\n)\nsummary(fullmodel_withinteractions)\n\n\nCall:\nlm(formula = logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), \n    data = mydata)\n\nResiduals:\nALL 28 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (4 not defined because of singularities)\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                            -5.948e+03        NaN     NaN      NaN\nlogarea                                 3.293e+03        NaN     NaN      NaN\ncpfor2                                  7.080e+01        NaN     NaN      NaN\nthtden                                  9.223e+02        NaN     NaN      NaN\nswamp                                   1.176e+02        NaN     NaN      NaN\nI(swamp^2)                             -3.517e-01        NaN     NaN      NaN\nlogarea:cpfor2                         -3.771e+01        NaN     NaN      NaN\nlogarea:thtden                         -4.781e+02        NaN     NaN      NaN\ncpfor2:thtden                          -1.115e+01        NaN     NaN      NaN\nlogarea:swamp                          -7.876e+01        NaN     NaN      NaN\ncpfor2:swamp                           -1.401e+00        NaN     NaN      NaN\nthtden:swamp                           -1.920e+01        NaN     NaN      NaN\nlogarea:I(swamp^2)                      5.105e-01        NaN     NaN      NaN\ncpfor2:I(swamp^2)                       3.825e-03        NaN     NaN      NaN\nthtden:I(swamp^2)                       7.826e-02        NaN     NaN      NaN\nswamp:I(swamp^2)                       -2.455e-03        NaN     NaN      NaN\nlogarea:cpfor2:thtden                   5.359e+00        NaN     NaN      NaN\nlogarea:cpfor2:swamp                    8.743e-01        NaN     NaN      NaN\nlogarea:thtden:swamp                    1.080e+01        NaN     NaN      NaN\ncpfor2:thtden:swamp                     2.620e-01        NaN     NaN      NaN\nlogarea:cpfor2:I(swamp^2)              -5.065e-03        NaN     NaN      NaN\nlogarea:thtden:I(swamp^2)              -6.125e-02        NaN     NaN      NaN\ncpfor2:thtden:I(swamp^2)               -1.551e-03        NaN     NaN      NaN\nlogarea:swamp:I(swamp^2)               -4.640e-04        NaN     NaN      NaN\ncpfor2:swamp:I(swamp^2)                 3.352e-05        NaN     NaN      NaN\nthtden:swamp:I(swamp^2)                 2.439e-04        NaN     NaN      NaN\nlogarea:cpfor2:thtden:swamp            -1.235e-01        NaN     NaN      NaN\nlogarea:cpfor2:thtden:I(swamp^2)        7.166e-04        NaN     NaN      NaN\nlogarea:cpfor2:swamp:I(swamp^2)                NA         NA      NA       NA\nlogarea:thtden:swamp:I(swamp^2)                NA         NA      NA       NA\ncpfor2:thtden:swamp:I(swamp^2)                 NA         NA      NA       NA\nlogarea:cpfor2:thtden:swamp:I(swamp^2)         NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 27 and 0 DF,  p-value: NA\n\n\nIndeed, it is not possible to include all 32 terms with only 28 observations. There are not enough data points, R square is one, and the model perfectly overfits the data.\nIf you try to use an automated routine to ‚Äúpick‚Äù the best model out of this soup, R complains:\n\nstep(fullmodel_withinteractions)\n\nError in step(fullmodel_withinteractions): AIC is -infinity for this model, so 'step' cannot proceed\n\n\nDoes this mean you can forget about potential interactions and simply accept the final model without a thought? No.¬†You simply do not have enough data to test for all interactions. But there is a compromise worth attempting, comparing the final model to a model with a subset of the interactions, say all second order interactions, to check whether the inclusion of these interactions improves substantially the fit:\n\nfull_model_2ndinteractions &lt;- lm(\n  logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2)\n    + logarea:cpfor2\n    + logarea:thtden\n    + logarea:swamp\n    + cpfor2:thtden\n    + cpfor2:swamp\n    + thtden:swamp,\n  data = mydata\n)\nsummary(full_model_2ndinteractions)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + \n    logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + \n    cpfor2:swamp + thtden:swamp, data = mydata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.216880 -0.036534  0.003506  0.042990  0.175490 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4.339e-01  6.325e-01   0.686 0.502581    \nlogarea        -1.254e-01  2.684e-01  -0.467 0.646654    \ncpfor2         -9.344e-03  7.205e-03  -1.297 0.213032    \nthtden         -1.833e-01  9.035e-02  -2.028 0.059504 .  \nswamp           3.569e-02  7.861e-03   4.540 0.000334 ***\nI(swamp^2)     -3.090e-04  7.109e-05  -4.347 0.000500 ***\nlogarea:cpfor2  2.582e-03  2.577e-03   1.002 0.331132    \nlogarea:thtden  7.017e-02  3.359e-02   2.089 0.053036 .  \nlogarea:swamp  -5.290e-04  2.249e-03  -0.235 0.816981    \ncpfor2:thtden  -2.095e-04  6.120e-04  -0.342 0.736544    \ncpfor2:swamp    4.651e-05  5.431e-05   0.856 0.404390    \nthtden:swamp    2.248e-04  4.764e-04   0.472 0.643336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.108 on 16 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8658,    Adjusted R-squared:  0.7735 \nF-statistic: 9.382 on 11 and 16 DF,  p-value: 4.829e-05\n\n\nThis model fits the data slightly better than the ‚Äúfinal‚Äù model (it explains 86.6% of the variance in logherp, compared to 81.2% for the ‚Äúfinal‚Äù model without interactions), but has twice as many parameters.\nIf you look at the individual coefficients, some weird things happen: for example, the sign for logarea has changed. This is one of the symptoms of multicollinearity. Let‚Äôs look at the variance inflation factors:\n\nvif(full_model_2ndinteractions)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n       logarea         cpfor2         thtden          swamp     I(swamp^2) \n      49.86060       78.49622      101.42437       90.47389      115.08457 \nlogarea:cpfor2 logarea:thtden  logarea:swamp  cpfor2:thtden   cpfor2:swamp \n      66.97792       71.69894       67.27034       14.66814       29.41422 \n  thtden:swamp \n      20.04410 \n\n\nOuch. All VIF are above 5, not only the ones involving the swamp terms. This model is not very satisfying it seems. Indeed the AIC for the two models indicate that the model with interactions has less information than the full model (remember, models with the lowest AIC value are to be preferred):\n\nAIC(model_poly1)\n\n[1] -38.3433\n\nAIC(full_model_2ndinteractions)\n\n[1] -34.86123\n\n\nThe anova() command can be used to test whether the addition of all interaction terms improves the fit significantly:\n\nanova(model_poly1, full_model_2ndinteractions)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2)\nModel 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + \n    logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + \n    thtden:swamp\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 0.25282                           \n2     16 0.18651  6  0.066314 0.9481  0.489\n\n\nThis test indicates that the addition of interaction terms did not reduce significantly the residual variance around the full model. How about a comparison with the final model without cpfor2?\n\nanova(model_poly2, full_model_2ndinteractions)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + thtden + swamp + I(swamp^2)\nModel 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + \n    logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + \n    thtden:swamp\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1     23 0.25999                           \n2     16 0.18651  7  0.073486 0.9006 0.5294\n\n\nAnd this comparison suggests that our final model does not make worse predictions than the full model with interactions.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#dredging-and-the-information-theoretical-approach",
    "href": "35-reg_mult.html#dredging-and-the-information-theoretical-approach",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.11 Dredging and the information theoretical approach",
    "text": "13.11 Dredging and the information theoretical approach\nOne of the main critiques of stepwise methods is that the p-values are not strictly correct because of the large number of tests that are actually done. This is the multiple testing problem. In building linear models (multiple regression for example) from a large number of independent variables, and possibly their interactions, there are so many possible combinations that if one were to use Bonferroni type corrections, it would make tests very conservative.\nAn alternative, very elegantly advocated by Burnham and Anderson (2002, Model selection and multimodel inference: a practical information-theoretic approach. 2nd ed), is to use AIC (or better the AICc that is more appropriate for samples where the number of observations is less that about 40 times the number of variables) to rank potential models, and identify the set of models that are the best ones. One can then average the parameters across models, weighting using the probability that it is the best model to obtain coefficients that are more robust and less likely to be unduly affected by multicollinearity.\n\n\n\n\n\n\nWarning\n\n\n\nTo compare models using AIC, models need to be fitted using the exact same data for each model. You thus need to be careful that there are no missing data when using an AIC based approach to model selection\n\n\nThe approach of comparing model fit using AIC was first developed to compare a set of model carefully build and chosen by the person doing the analysis based on a-priori knowledge and biological hypotheses. Some, however, developped an approach that I consider brainless and brutal to fit all potential models and then compare them using AIC. This approach has been implemented in the package MuMIn.\n\n\n\n\n\n\nNote\n\n\n\nI do not support the use of stepwise AIC or data dredging which are going against the philosophy of AIC and parsimony. Develop a model based on biological hypothesis and report all the results significant or not without dredging the data.\n\n\n\n# redo the model double chekcing there are no \"NA\"\n# specifying na.action\n\nfull_model_2ndinteractions &lt;- update(\n  full_model_2ndinteractions,\n  . ~ .,\n  data = mysub,\n  na.action = \"na.fail\"\n)\n\nlibrary(MuMIn)\ndd &lt;- dredge(full_model_2ndinteractions)\n\nFixed term is \"(Intercept)\"\n\n\nObject dd will contain all possible models using the terms of our full model with 2nd order interactions. Then, we can have a look at the subset of models that have an AICc within 4 units from the lowest AICc model. (Burnham and Anderson suggest that models that deviate by more than 2 AICc units have very little empirical support):\n\n# get models within 4 units of AICc from the best model\ntop_models_1 &lt;- get.models(dd, subset = delta &lt; 4)\navgmodel1 &lt;- model.avg(top_models_1) # compute average parameters\nsummary(avgmodel1) # display averaged model\n\n\nCall:\nmodel.avg(object = top_models_1)\n\nComponent model call: \nlm(formula = logherp ~ &lt;8 unique rhs&gt;, data = mysub, na.action = \n     na.fail)\n\nComponent models: \n       df logLik   AICc delta weight\n23457   7  27.78 -35.95  0.00   0.34\n2345    6  25.78 -35.56  0.39   0.28\n123457  8  28.30 -33.02  2.93   0.08\n234578  8  28.26 -32.95  3.00   0.08\n12345   7  26.17 -32.74  3.21   0.07\n23458   7  26.06 -32.51  3.44   0.06\n234567  8  27.88 -32.17  3.78   0.05\n23456   7  25.79 -31.99  3.97   0.05\n\nTerm codes: \n        cpfor2     I(swamp^2)        logarea          swamp         thtden \n             1              2              3              4              5 \n logarea:swamp logarea:thtden   swamp:thtden \n             6              7              8 \n\nModel-averaged coefficients:  \n(full average) \n                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)    -2.075e-01  2.484e-01   2.593e-01   0.800    0.424    \nlogarea         1.314e-01  1.185e-01   1.222e-01   1.076    0.282    \nswamp           3.193e-02  6.125e-03   6.438e-03   4.960    7e-07 ***\nI(swamp^2)     -2.676e-04  4.904e-05   5.154e-05   5.193    2e-07 ***\nthtden         -6.843e-02  5.324e-02   5.459e-02   1.254    0.210    \nlogarea:thtden  2.139e-02  2.506e-02   2.565e-02   0.834    0.404    \ncpfor2         -1.202e-04  4.710e-04   4.886e-04   0.246    0.806    \nswamp:thtden   -3.277e-05  1.419e-04   1.475e-04   0.222    0.824    \nlogarea:swamp   4.378e-05  5.378e-04   5.676e-04   0.077    0.939    \n \n(conditional average) \n                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)    -2.075e-01  2.484e-01   2.593e-01   0.800   0.4236    \nlogarea         1.314e-01  1.185e-01   1.222e-01   1.076   0.2820    \nswamp           3.193e-02  6.125e-03   6.438e-03   4.960    7e-07 ***\nI(swamp^2)     -2.676e-04  4.904e-05   5.154e-05   5.193    2e-07 ***\nthtden         -6.843e-02  5.324e-02   5.459e-02   1.254   0.2100    \nlogarea:thtden  3.924e-02  2.125e-02   2.251e-02   1.743   0.0813 .  \ncpfor2         -8.187e-04  9.692e-04   1.027e-03   0.797   0.4253    \nswamp:thtden   -2.402e-04  3.127e-04   3.313e-04   0.725   0.4684    \nlogarea:swamp   4.462e-04  1.664e-03   1.762e-03   0.253   0.8001    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(avgmodel1) # display CI for averaged coefficients\n\n                       2.5 %        97.5 %\n(Intercept)    -0.7157333646  0.3007147516\nlogarea        -0.1080048582  0.3708612563\nswamp           0.0193158426  0.0445532538\nI(swamp^2)     -0.0003686653 -0.0001666418\nthtden         -0.1754184849  0.0385545120\nlogarea:thtden -0.0048800385  0.0833595106\ncpfor2         -0.0028313465  0.0011940283\nswamp:thtden   -0.0008894138  0.0004090457\nlogarea:swamp  -0.0030067733  0.0038991294\n\n\n\n\ncomponents models: You first get the list of the models with an AICc within the desired 4 units of the best model. The variables that are included in the model are coded with the key just below.\nFor each model, in addition to the AICc, the Akaike weights are calculated. They represent the relative likelihood of a model, and indicate the relative importance of a model compared to the other models tested.\n\nMode-averaged coefficients:ÔÅ∑ For the subset of models, weighted averages (using Akaike weights) for model parameters are calculated, with 95% CI. Note that, by default, terms missing from a model are assumed to have a coefficient of 0.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#bootstrapping-multiple-regression",
    "href": "35-reg_mult.html#bootstrapping-multiple-regression",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.12 Bootstrapping multiple regression",
    "text": "13.12 Bootstrapping multiple regression\nWhen data do not meet the assumptions of normality and homoscedasticity and it is not possible to transform the data to meet the assumptions, bootstraping can be used to compute confidence intervals for coefficients. If the distribution of the bootstrapped coefficients is symmetrical and approximately Gaussian, then empirical percentiles can be used to estimate the confidence limits.\nThe following code, using the simpleboot üì¶ has been designed to be easily modifiable and will compute CI using empirical percentiles. Following this is an easier approach using the library boot that will calculate several different bootstrap confidence limits.\n\n############################################################\n#######\n# Bootstrap analysis the simple way with library simpleboot\n# Define model to be bootstrapped and the data source used\nmymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata)\n# Set the number of bootstrap iterations\nnboot &lt;- 1000\nlibrary(simpleboot)\n# R is the number of bootstrap iterations\n# Setting rows to FALSE indicates resampling of residuals\nmysimpleboot &lt;- lm.boot(mymodel, R = nboot, rows = FALSE)\n# Extract bootstrap coefficients\nmyresults &lt;- sapply(mysimpleboot$boot.list, function(x) x$coef)\n# Transpose matrix so that lines are bootstrap iterations\n# and columns are coefficients\ntmyresults &lt;- t(myresults)\n\nYou can then plot the results using the follwoing code. When run, it will pause to let you have a look at the distribution for each coefficient in the model by producing plots like:\n\n# Plot histograms of bootstrapped coefficients\nncoefs &lt;- length(data.frame(tmyresults))\npar(mfrow = c(1, 2), mai = c(0.5, 0.5, 0.5, 0.5), ask = TRUE)\nfor (i in 1:ncoefs) {\n  lab &lt;- colnames(tmyresults)[i]\n  x &lt;- tmyresults[, i]\n  plot(density(x), main = lab, xlab = \"\")\n  abline(v = mymodel$coef[i], col = \"red\")\n  abline(v = quantile(x, c(0.025, 0.975)))\n  hist(x, main = lab, xlab = \"\")\n  abline(v = quantile(x, c(0.025, 0.975)))\n  abline(v = mymodel$coef[i], col = \"red\")\n}\n\n\n\n\n\nDistribution of bootstrapped estimates for logarea\n\n\n\nThe top plot is the probability density function and the bottom one is the histogram of the bootstrap estimates for the coefficient. On these plots, the red line indicate the value of the parameter in the ordinary analysis, and the two vertical black lines mark the limits of the 95% confidence interval. Here the CI does not include 0 and one can conclude that the effect of logarea on logherp is significantly positive.\nPrecise values for the limits can be obtained by:\n\n# Display empirical bootstrap quantiles (not corrected for bias)\np &lt;- c(0.005, 0.01, 0.025, 0.05, 0.95, 0.975, 0.99, 0.995)\napply(tmyresults, 2, quantile, p)\n\n       (Intercept)   logarea       thtden      swamp    I(swamp^2)\n0.5%  -0.703337469 0.1372880 -0.045562085 0.01704499 -0.0003491593\n1%    -0.682330238 0.1437596 -0.044178131 0.01765760 -0.0003417409\n2.5%  -0.634774104 0.1552722 -0.040448896 0.01992423 -0.0003262314\n5%    -0.598948564 0.1692405 -0.038330203 0.02156104 -0.0003135141\n95%   -0.097107227 0.2829351 -0.011854745 0.03770433 -0.0001839501\n97.5% -0.064126662 0.2931585 -0.010018307 0.03946455 -0.0001712404\n99%   -0.007850978 0.3021190 -0.007672276 0.04064188 -0.0001547049\n99.5%  0.031935680 0.3061687 -0.004818113 0.04145426 -0.0001500559\n\n\nThese confidence limits are not reliable when the distribution of the bootstrap estimates deviate from Gaussian. If they do,, then it is preferable to compute so-called bias-corrected accelerated (BCa) confidence limits. The following code does just that:\n\n################################################\n# Bootstrap analysis in multiple regression with BCa confidence intervals\n# Preferable when parameter distribution is far from normal\n# Bootstrap 95% BCa CI for regression coefficients\n\nlibrary(boot)\n# function to obtain regression coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ] # allows boot to select sample\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = mydata, statistic = bs, R = 1000,\n  formula = logherp ~ logarea + thtden + swamp + I(swamp^2)\n)\n# view results\n\nTo get teh results, the following code will produce the standard graph for each coefficient and the resulting BCa interval.\n\nplot(results, index = 1) # intercept\nplot(results, index = 2) # logarea\nplot(results, index = 3) # thtden\nplot(results, index = 4) # swamp\nplot(results, index = 5) # swamp2\n\n# get 95% confidence intervals\nboot.ci(results, type = \"bca\", index = 1)\nboot.ci(results, type = \"bca\", index = 2)\nboot.ci(results, type = \"bca\", index = 3)\nboot.ci(results, type = \"bca\", index = 4)\nboot.ci(results, type = \"bca\", index = 5)\n\nFor logarea, we get:\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   ( 0.1185,  0.3176 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n\n\nNote that the BCa interval is from 0.12 to 0.32, whereas the simpler percentile interval is 0.16 to 0.29. BCa interval here is longer on the low side, and shorter on the high side, which it should be given the distribution of bootstrap estimates.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#permutation-test",
    "href": "35-reg_mult.html#permutation-test",
    "title": "\n13¬† Multiple regression\n",
    "section": "\n13.13 Permutation test",
    "text": "13.13 Permutation test\nPermutation tests are more rarely performed in multiple regression contexts than bootstrap. But here is code to do it.\n\n############################################################\n##########\n# Permutation in multiple regression\n#\n# using lmperm library\nlibrary(lmPerm)\n# Fit desired model on the desired dataframe\nmy_model &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nmy_model_prob &lt;- lmp(\n  logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata, perm = \"Prob\"\n)\nsummary(my_model)\nsummary(my_model_prob)",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html",
    "href": "36-ancova_glm.html",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "",
    "text": "14.1 R packages and data\nThis laboratory requires the following:\nLoading required package: carData\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#set-anco",
    "href": "36-ancova_glm.html#set-anco",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "",
    "text": "R packages:\n\nggplot2\ncar\nlmtest\n\n\ndata files\n\nanc1dat.csv\nanc3dat.csv",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#linear-models",
    "href": "36-ancova_glm.html#linear-models",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.2 Linear models",
    "text": "14.2 Linear models\nGLM sometimes stands for General Linear Model, however, it is much more frequently used for Generalized linear models. Thus I always rather talk about Linear models or LMs instead of General linear models to avoid confusion in the acronym. LMs are statistical models that can be written as \\(Y = XB + E\\), where Y is a vector (or matrix) containing the dependent variable, X is a matrix of independent variables, B is a matrix of estimated parameters, and E is the vector (or matrix) of independent, normally distributed and homoscedastic residuals. All tests we have seen to date (t-test, simple linear regression, One-Way ANOVA, Multiway ANOVA, and multiple regression) are LMs. Note that all models we have encountered until now contain only one type of variable (either continuous or categorical) as independent variables. In this laboratory exercise, you will fit models that have both type of independent variables. These models are also LMs.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#ancova",
    "href": "36-ancova_glm.html#ancova",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.3 ANCOVA",
    "text": "14.3 ANCOVA\nANCOVA stands for Analysis of Covariance. It is a type of LM where there is one (or more) continuous independent variable (sometimes called a covariate) and one (or more) categorical independent variable. In the traditional treatment of ANCOVA in biostatistical textbooks, the ANCOVA model does not contain interaction terms between the continuous and categorical independent variables. Hence, the traditional ANCOVA analysis assumes that there is no interaction, and is preceeded by a test of significance of interactions, equivalent to testing that the slopes (coefficients for the continuous independent variables) do not differ among level of the categorical independent variables (a test for homogeneity of slopes). Some people, me included, use the term ANCOVA a bit more loosely for any LM that involves both continuous and categorical variables. Be aware that, depending on the author, ANCOVA may refer to a model with or without interaction terms.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#homogeneity-of-slopes",
    "href": "36-ancova_glm.html#homogeneity-of-slopes",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.4 Homogeneity of slopes",
    "text": "14.4 Homogeneity of slopes\nIn many biological problems, a question arises as to whether the slopes of two or more regression lines are significantly different; for example, whether two different insecticides differ in their efficacy, whether males and females differ in their growth curves, etc. These problems call for direct comparisons of slopes. GLMs (ANCOVAs) can test for equality of slopes (homogeneity of slopes).\nRemember that there are two parameters that describe a regression line, the intercept and the slope. The ANCOVA model (sensu stricto) tests for homogeneity of intercepts, but the starting point for the analysis is a test for homogeneity of slopes. This test can be performed by fitting a model with main effects for both the categorical and continuous independent variables, plus the interaction term(s), and testing for significance of the addition of the interaction terms.\n\n14.4.1 Case 1 - Size as a function of age (equal slopes example)\n\n\n\n\n\n\nExercise\n\n\n\nUsing the file anc1dat.csv , test the hypothesis that female and male sturgeon at The Pas over the years 1978-1980 have the same observed growth rate, defined as the slope of the regression of log10 of fork length, lfkl , on the log10 age, lage .\n\n\nFirst, let‚Äôs have a look at the data. It would help to draw the regression line and a lowess trace to better assess linearity. Being fancy, one could also use more of R magic to spruce up the axis legends (note the use of expression() to get subscripts):\n\nanc1dat &lt;- read.csv(\"data/anc1dat.csv\")\nanc1dat$sex &lt;- as.factor(anc1dat$sex)\nmyplot &lt;- ggplot(data = anc1dat, aes(x = lage, y = log10(fklngth))) +\n  facet_grid(. ~ sex) +\n  geom_point()\nmyplot &lt;- myplot +\n  stat_smooth(method = lm, se = FALSE) +\n  stat_smooth(se = FALSE, color = \"red\") +\n  labs(\n    y = expression(log[10] ~ (Fork ~ length)),\n    x = expression(log[10] ~ (Age))\n  )\nmyplot\n\n\n\nSturgeon length as a function of age\n\n\n\nThe log-log transformation makes the relationship linear and, at first glance, there is no issues with assumptions of LMs (although this should be confirmed by appropriate examination of the residuals). Let‚Äôs fit the full model with both main effects and the interaction:\n\nmodel.full1 &lt;- lm(lfkl ~ sex + lage + sex:lage, data = anc1dat)\nAnova(model.full1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.64444  1 794.8182 &lt; 2.2e-16 ***\nsex         0.00041  1   0.5043    0.4795    \nlage        0.07259  1  89.5312 4.588e-15 ***\nsex:lage    0.00027  1   0.3367    0.5632    \nResiduals   0.07135 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the previous output, on line 4, 0.5632277 is the probability of observing an lage:sex interaction this strong or stronger under the null hypothesis that slope of the relationship between fork length and age does not vary between the sexes, or equivalently that the difference in fork length between males and females (if it exists) does not vary with age (and providing the assumptions of the analysis have been met).\n\n\n\n\n\n\nNote\n\n\n\nNote that I used the Anova() function with an uppercase ‚Äúa‚Äù (from the car library) instead of the ‚Äúbuilt in‚Äù anova() (with a lowercase ‚Äúa‚Äù) command to get the results using Type III sums of squares. The type III (partial) sums of squares are calculated as if each variable was the last entered in the model and correspond to the difference in explained SS between the full model and a reduced model where only that variable is excluded. The standard anova() function returns Type I (sequential) SS, calculated as each variable is added sequentially to a null model with only an intercept. In rare cases, the type I and type III SS are equal (when the design is perfectly balanced and there is no multicolinearity). In the vast majority of cases, they will differ, and I recommend that you always use the Type III SS in your analyses.\n\n\nOn the basis of this analysis, we would accept the null hypotheses that:\n\nthe slope of the regression of log(fork length) on log(age) is the same for males and females (the interaction term is not significant)\nthat the intercepts are also the same for the two sexes (the sex term is also not significant).\n\nBut before accepting these conclusions, we should test the assumptions in the usual way\n\n\n\n\n\n\nSolution\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.full1)\n\n\n\nModel assumptions for model.full1\n\n\n\n\n\nWith respect to normality, things look O.K., although there are several points in the top right corner that appear to lie off the line. We could also run a Wilk-Shapiro normality test and find W = 0.9764, p = 0.09329, also suggesting this assumption is valid. Homoscedasticity seems fine too, but if you want further evidence of this, you can run one of the tests. Here I use the Breusch-Pagan test, which is appropriate when some of the independent variables are continuous (Levene‚Äôs test is for categorical independent variables only):\n\nbptest(model.full1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model.full1\nBP = 0.99979, df = 3, p-value = 0.8013\n\n\nSince the null is that the residuals are homoscedastic, and p is rather large, the test confirm the visual assessment.\nFurther, there is no obvious pattern in the residuals, which implies there is no problem with the assumption of linearity. This too can be formally tested:\n\nresettest(model.full1, power = 2:3, type = \"regressor\", data = anc1dat)\n\n\n    RESET test\n\ndata:  model.full1\nRESET = 0.59861, df1 = 2, df2 = 86, p-value = 0.5519\n\n\nThe last assumption in this sort of analysis is that the covariate (in this case, lage ) has no measurement error. We really have no way of knowing whether this assumption is justified, although multiple aging of fish by several different investigators usually gives ages that are within 1-2 years of each other, which is within the 10% considered by most to be the maximum for Type I modelling. Note that there is no ‚Äútest‚Äù that you can do with the data to determine what the error is, at least in this case. If we had replicate ages for individual fish, it could be estimated quantitatively\n\n\n\n\n\n\nExercise\n\n\n\nYou will notice that there is one datum with a large studentized residual, i.e.¬†an outlier (case 49). Eliminate this datum from your data file and rerun the analysis. Do your conclusions change?\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nmodel.full.no49 &lt;- lm(lfkl ~ sex + lage + sex:lage, data = anc1dat[c(-49), ])\nAnova(model.full.no49, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value Pr(&gt;F)    \n(Intercept) 0.64255  1 895.9394 &lt;2e-16 ***\nsex         0.00038  1   0.5273 0.4697    \nlage        0.07378  1 102.8746 &lt;2e-16 ***\nsex:lage    0.00022  1   0.3135 0.5770    \nResiduals   0.06239 87                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nSo the conclusion does not change if the outlier is deleted (not surprising as Cook‚Äôs distance is low for this point reflecting its low leverage). Since there is no good reason to delete this data point, and since (at least qualitatively) our conclusions do not change, it is probably best to go with the full data set. A test of the assumptions for the refit model (with the outlier removed) shows that all are met, and no more outliers are detected. (I won‚Äôt report these analyses, but you can and should do them just to assure yourself that everything is O.K.)\n\n14.4.2 Case 2 - Size as a function of age (different slopes example)\n\n\n\n\n\n\nExercise\n\n\n\nThe file anc3dat.csv records data on male sturgeon collected at two locations ( locate) , Lake of the Woods, in northwestern Ontario, and the Churchill River in northern Manitoba. Using the same procedure as outlined above (with locate as the categorical variable instead of sex ), test the null hypothesis that the slope of the regression of lfkl on lage is the same in the two locations. What do you conclude?\n\n\n\nanc3dat &lt;- read.csv(\"data/anc3dat.csv\")\nmyplot &lt;- ggplot(data = anc3dat, aes(x = lage, y = log10(fklngth))) +\n  facet_grid(. ~ locate) +\n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  stat_smooth(se = FALSE, color = \"red\") +\n  labs(\n    y = expression(log[10] ~ (Fork ~ length)),\n    x = expression(log[10] ~ (Age))\n  )\nmyplot\n\n\n\nLongueur des esturgeons en fonction de l‚Äôage d‚Äôapr√®s anc3dat\n\n\nmodel.full2 &lt;- lm(lfkl ~ lage + locate + lage:locate, data = anc3dat)\nAnova(model.full2, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.62951  1 1078.632 &lt; 2.2e-16 ***\nlage        0.07773  1  133.185 &lt; 2.2e-16 ***\nlocate      0.00968  1   16.591 0.0001012 ***\nlage:locate 0.00909  1   15.575 0.0001592 ***\nResiduals   0.05136 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, we reject the null hypotheses that (1) the slopes of the regressions are the same in the two locations; and (2) that the intercepts are the same in the two locations. In other words, if we want to predict the fork length of a sturgeon of a particular age (accurately) we need to know from which location it came. The fact that we reject the null hypothesis that the slopes of the lfkl - lage regressions are the same in both locations means that we should be doing individual regressions for each location separately (that is in fact what the full model is fitting). But we are jumping the gun here. Before you can trust these p values, you need to confirm that assumptions are met:\n\n\n\n\n\n\nTip\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.full2)\n\n\n\nConditions d‚Äôapplications du mod√®le model.full2\n\n\n\n\n\nIf you analyze the residuals (in the way described above), you will find that there is no problem with the linearity assumption, nor the homoscedasticity assumption (BP = 1.2267, p = 0.268). However, Wilk-Shapiro test of normality of residuals is suspicious (W=0.97, p = 0.03). Given the relatively large sample size (\\(N = 92\\)), this normality test has high power and the magnitude of deviation from normality does not appear to be large. Considering the robustness of GLM to non-normality with large samples, we should not be overly concerned with this violation.\nGiven that the assumptions appear sufficiently met, we can accept the results as calculated by R. All terms in the model are significant (location, lage, and the interaction). This full model is equivalent to fitting separate regressions for each location. To get the coefficients of these regression, one can either fit the two regressions on data subsets for each location, or extract the fitted coefficients from the full model:\n\nmodel.full2\n\n\nCall:\nlm(formula = lfkl ~ lage + locate + lage:locate, data = anc3dat)\n\nCoefficients:\n            (Intercept)                     lage       locateNELSON        \n                 1.2284                   0.3253                   0.2207  \nlage:locateNELSON        \n                -0.1656  \n\n\nBy default, the variable locate in the model is internally encoded as 0 for the location that comes first alphabetically (LofW) and 1 for the other (Nelson). So the regression equations for each location become:\nFor LofW: \\[\\begin{aligned}\nlfkl &= 1.2284 + 0.3253 \\times lage + 0.2207 \\times 0 - 0.1656 \\times 0 \\times lage \\\\\n&= 1.2284 + 0.3253 \\times lage\n\\end{aligned}\\]\nFor Nelson: \\[\\begin{aligned}\nlfkl &= 1.2284 + 0.3253 \\times lage + 0.2207 \\times 1 - 0.1656 \\times 1 \\times lage \\\\\n&= 1.4491 + 0.1597 \\times lage\n\\end{aligned}\\]\nYou can convince youself that this is the same as fitting 2 regressions separately.\n\nby(anc3dat, anc3dat$locate, function(x) lm(lfkl ~ lage, data = x))\n\nanc3dat$locate: LOFW        \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nCoefficients:\n(Intercept)         lage  \n     1.2284       0.3253  \n\n------------------------------------------------------------ \nanc3dat$locate: NELSON      \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nCoefficients:\n(Intercept)         lage  \n     1.4491       0.1597",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#the-ancova-model",
    "href": "36-ancova_glm.html#the-ancova-model",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.5 The ANCOVA model",
    "text": "14.5 The ANCOVA model\nIf the test for homogeneity of slopes indicates that the two or more slopes are not significantly different, i.e.¬†there is no significant interaction between the categorical and continuous variable, then a single slope parameter can be fit. How about the intercepts? Do they differ among levels of the categorical variable? There are two school of thoughts on how to proceed to test for equality of intercepts when slopes are equal:\n\nThe old school fits a reduced model, with the categorical and continuous variables, but no interactions (this is the ANCOVA model, sensus stricto) and uses the partitioned sums of squares to test for significance, say with the Anova() function. This approach is the one presented in many statistical textbooks.\nOthers simply use the full model results, and test significance of each term from the partial sums of squares. This approach has the advantage of being faster as only one model needs to be fitted to make all inferences. However, this approach is less powerful.\n\nIn most practical cases, it does not matter unless one has very complex models with a large number of terms and higher level interactions and that many of these terms are not significant. My suggestion is that you use the faster approach first, and use the traditional approach only when you accept the null hypothesis for equal intercepts. Why? Since the faster approach is less powerful, if you nevertheless reject H0, then this conclusion will not be changed, only reinforced, by using the traditional approach.\nHere I will compare the old school and the other approach. Recall that we want to assess equality of intercepts once we determined that slopes are equal. Test for equality of intercepts when slopes differ (or, if you prefer, when there is a significant interaction) are rarely directly meaningful, are often misinterpreted, and should rarely be conducted.\nGoing back to anc1dat.csv, comparing the relationships between lfkl and lage among sexes, we obtained the following results for the full model with interactions\n\nAnova(model.full1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.64444  1 794.8182 &lt; 2.2e-16 ***\nsex         0.00041  1   0.5043    0.4795    \nlage        0.07259  1  89.5312 4.588e-15 ***\nsex:lage    0.00027  1   0.3367    0.5632    \nResiduals   0.07135 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe already concluded that the slope of the regression for males and females does not differ (the interaction sex:lage is not significant). Note that the p-values associated with sex (0.4795) is not significant either.\nFor the old-school approach, one would fit a reduced model (the sensus stricto ANCOVA model):\n\nmodel.ancova &lt;- lm(lfkl ~ sex + lage, data = anc1dat)\nAnova(model.ancova, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df   F value Pr(&gt;F)    \n(Intercept) 1.13480  1 1410.1232 &lt;2e-16 ***\nsex         0.00149  1    1.8513 0.1771    \nlage        0.14338  1  178.1627 &lt;2e-16 ***\nResiduals   0.07162 89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model.ancova)\n\n\nCall:\nlm(formula = lfkl ~ sex + lage, data = anc1dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.093992 -0.018457 -0.000876  0.022491  0.081161 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.225533   0.032636  37.552   &lt;2e-16 ***\nsexMALE         -0.008473   0.006228  -1.361    0.177    \nlage             0.327253   0.024517  13.348   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02837 on 89 degrees of freedom\nMultiple R-squared:  0.696, Adjusted R-squared:  0.6892 \nF-statistic: 101.9 on 2 and 89 DF,  p-value: &lt; 2.2e-16\n\n\nAccording to this test, sex is not significant and therefore we can conclude that the intercept does not vary significantly between males and females. Note that the p-value is lower this time (0.1771 vs 0.4795), reflecting the higher power of this old-school approach. However, the conclusion remains qualitatively the same: intercepts do not differ.\nSo we accept the null hypothesis that the intercepts are the same for the two sexes. Running the residual diagnostics, we find no problems with linearity, independence, homogeneity of variances, and normality.\n\n\n\n\n\n\nExercise\n\n\n\nYou will notice, in the above analysis that the residuals plots flag three data points (cases 19, 49, and 50) as having high residuals. These points are a bit worrisome, and may be having a disproportionate effect on your analysis. Eliminate these ‚Äúoutliers‚Äù and re-run the analysis. Now what do you conclude?\n\n\n\nmodel.ancova.nooutliers &lt;- lm(lfkl ~ sex + lage, data = anc1dat[c(-49, -50, -19), ])\nAnova(model.ancova.nooutliers, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df   F value  Pr(&gt;F)    \n(Intercept) 1.09160  1 1896.5204 &lt; 2e-16 ***\nsex         0.00232  1    4.0374 0.04764 *  \nlage        0.13992  1  243.0946 &lt; 2e-16 ***\nResiduals   0.04950 86                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model.ancova.nooutliers)\n\n\nCall:\nlm(formula = lfkl ~ sex + lage, data = anc1dat[c(-49, -50, -19), \n    ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.058397 -0.018469 -0.000976  0.020696  0.040288 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.224000   0.028106  43.549   &lt;2e-16 ***\nsexMALE         -0.010823   0.005386  -2.009   0.0476 *  \nlage             0.328604   0.021076  15.591   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02399 on 86 degrees of freedom\nMultiple R-squared:  0.7706,    Adjusted R-squared:  0.7653 \nF-statistic: 144.4 on 2 and 86 DF,  p-value: &lt; 2.2e-16\n\n\nWell, well. Now we would, according to convention, reject the null hypothesis, and conclude that in fact, the intercepts of the regressions for the two sexes are different! This is a qualitatively different result from that obtained using all the data. Why? There are two possible reasons:\n\nthe ‚Äúoutliers‚Äù have significant impacts on the fitted regression lines, so that the intercepts of the lines change depending on whether the ‚Äúoutliers‚Äù are included (or not);\nthe exclusion of the outliers increases the precision, i.e.¬†reduces the standard error of the intercept estimates, and therefore increases the likelihood that the two intercepts will in fact be ‚Äústatistically‚Äù different.\n\n\nis unlikely, since none of the outliers had high leverage (hence Cook‚Äôs distances were not large), so (2) is more likely, and you can verify this by fitting separate regressions for each sex with and without these three outliers. If you do, you will notice that the estimated intercepts for each sex do not change very much, but the standard errors of these intercepts change quite a lot.\n\n\n\n\n\n\n\nExercise\n\n\n\nFit separate regresssions by sex with vs.¬†without the outliers. Pay attention to the intercepts.\n\n\nIncluding all data.\n\nby(\n  anc1dat,\n  anc1dat[, \"sex\"],\n  function(x) {\n    summary(lm(lfkl ~ lage, data = x))\n }\n)\n\nanc1dat[, \"sex\"]: FEMALE      \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.093728 -0.020510 -0.000618  0.024066  0.078844 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.24264    0.04660  26.664  &lt; 2e-16 ***\nlage         0.31431    0.03512   8.949 4.16e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03011 on 52 degrees of freedom\nMultiple R-squared:  0.6063,    Adjusted R-squared:  0.5987 \nF-statistic: 80.09 on 1 and 52 DF,  p-value: 4.16e-12\n\n------------------------------------------------------------ \nanc1dat[, \"sex\"]: MALE        \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.046663 -0.014875 -0.004275  0.013489  0.078910 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19730    0.04209   28.45  &lt; 2e-16 ***\nlage         0.34300    0.03337   10.28 2.97e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02594 on 36 degrees of freedom\nMultiple R-squared:  0.7458,    Adjusted R-squared:  0.7388 \nF-statistic: 105.6 on 1 and 36 DF,  p-value: 2.972e-12\n\n\nDifference in intercept is indeed really small. Now let‚Äôs have a look when we exclude outliers.\n\nby(\n  anc1dat,\n  anc1dat[, \"sex\"],\n  function(x) {\n    summary(lm(lfkl ~ lage, data = x[c(-49, -50, -19), ]))\n }\n)\n\nanc1dat[, \"sex\"]: FEMALE      \n\nCall:\nlm(formula = lfkl ~ lage, data = x[c(-49, -50, -19), ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.092746 -0.020176 -0.000078  0.023779  0.079995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.24029    0.04815  25.760  &lt; 2e-16 ***\nlage         0.31533    0.03614   8.724 1.53e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03021 on 49 degrees of freedom\nMultiple R-squared:  0.6083,    Adjusted R-squared:  0.6003 \nF-statistic: 76.11 on 1 and 49 DF,  p-value: 1.526e-11\n\n------------------------------------------------------------ \nanc1dat[, \"sex\"]: MALE        \n\nCall:\nlm(formula = lfkl ~ lage, data = x[c(-49, -50, -19), ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.047429 -0.012818 -0.005274  0.013495  0.077538 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19361    0.04188   28.50  &lt; 2e-16 ***\nlage         0.34662    0.03325   10.42 2.83e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02574 on 35 degrees of freedom\nMultiple R-squared:  0.7563,    Adjusted R-squared:  0.7494 \nF-statistic: 108.6 on 1 and 35 DF,  p-value: 2.835e-12\n\n\nDifferences in intercepts are really small and similar than in previous models but now the precision (i.e.¬†standard error) is much smaller for the models without outliers.\n\n\n\n\n\n\nNote\n\n\n\nIt is often the case that by eliminating outliers, new outliers appear. This is simply because the ‚Äúoutlier‚Äù designation is usually based on a standardized residual: if you eliminate a couple of outliers, then the residual sums of squares decreases, i.e.¬†the ‚Äúaverage‚Äù (absolute) residual decreases. Thus, points which were not ‚Äúfar from the average‚Äù when the original average residual was comparatively large (i.e.¬†were not ‚Äúoutliers‚Äù), may now become so because the average residual has been decreased. Remember also that as you eliminate outliers, N decreases, and the increase in R2 may be more than compensated for by decreased power. So be wary of eliminating outliers!",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#comparing-model-fits",
    "href": "36-ancova_glm.html#comparing-model-fits",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.6 Comparing model fits",
    "text": "14.6 Comparing model fits\nAs we have just seen, the process of fitting models to data is usually an iterative one. That is, there are, more often than not, several competing models that may be used to fit the data and it is left to the analyst to decide which model best balances goodness of fit (which we are usually trying to maximize) and complexity (which we are usually trying to minimize). In general, the strategy to use in regression and anova is to choose a simpler model when doing so does not reduce the goodness-of-fit by a significant amount. R can compute an F-statistic to compare the fit of two models. The null hypothesis in this situation is that there is no difference in goodness of fit between the models.\n\n\n\n\n\n\nExercise\n\n\n\nWorking with the Anc1dat data set, compare the fit of the ANCOVA and common simple regression models::\n\n\n\nmodel.linear &lt;- lm(lfkl ~ lage, data = anc1dat)\nanova(model.ancova, model.linear)\n\nAnalysis of Variance Table\n\nModel 1: lfkl ~ sex + lage\nModel 2: lfkl ~ lage\n  Res.Df      RSS Df  Sum of Sq      F Pr(&gt;F)\n1     89 0.071623                            \n2     90 0.073113 -1 -0.0014899 1.8513 0.1771\n\n\nThe anova() function can compare the differences in sum of squares and degrees of freedom between the simpler and more complex models, takes the ratio of these two values to generate a mean square, and divides this by the mean square of the more complex model to generate an F-statistic. In the above case, there is insufficient evidence to reject the null hypothesis and we conclude that the simpler model, which is the simple linear regression, is the best model for these data. (Because these models differ by only the presence vs.¬†absence of a single factor (sex), the P-value is the same as the p-value for sex in model 1.)\n\n\n\n\n\n\nExercise\n\n\n\nRepeat the above procedure with the ANC3DAT data, rerunning the full ANCOVA with interaction ( lfkl ~ lage + locate + lage:locate ) and without interaction ( lfkl ~ lage + locate ), saving the model objects as you did above. Compare the fits of the two models. What do you conclude?\n\n\n\nmodel.full.anc3dat &lt;- lm(lfkl ~ lage + locate + lage:locate, data = anc3dat)\nmodel.ancova.anc3dat &lt;- lm(lfkl ~ lage + locate, data = anc3dat)\nanova(model.full.anc3dat, model.ancova.anc3dat)\n\nAnalysis of Variance Table\n\nModel 1: lfkl ~ lage + locate + lage:locate\nModel 2: lfkl ~ lage + locate\n  Res.Df      RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1     88 0.051358                                   \n2     89 0.060448 -1 -0.0090901 15.575 0.0001592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case there is sufficient evidence to reject the null hypothesis and conclude that the full model with interaction is the best model to fit to the Anc3dat data. This is as we expected, given the fact that we found the interaction to be significant the in original analysis of the data. While no new information is gain from this model comparison in this case, this approach can be more usefully employed to compared nested models that differ in more than one term.",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#bootstrap",
    "href": "36-ancova_glm.html#bootstrap",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.7 Bootstrap",
    "text": "14.7 Bootstrap\n\n############################################################\n######\n# Bootstrap analysis\n#\n# Bootstrap analysis BCa confidence intervals\n# Preferable when parameter distribution is far from normal\n# Bootstrap 95% BCa CI for regression coefficients\nlibrary(boot)\n\n# To simplify future modifications of the code in this file,\n# copy the data to a generic mydata dataframe\nmydata &lt;- anc3dat\n\n# create a myformula variable containing the formula for the model to be fitted\nmyformula &lt;- as.formula(lfkl ~ lage + locate + lage:locate)\n\n# function to obtain regression coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ]\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(data = mydata, statistic = bs, R = 1000, formula = myformula)\n\n# view results\nresults\nboot_res &lt;- summary(results)\nrownames(boot_res) &lt;- names(results$t0)\nboot_res\n\nop &lt;- par(ask = TRUE)\nfor (i in 1:length(results$t0)) {\n  plot(results, index = i)\n  title(names(results$t0)[i])\n}\npar(op)\n\n# get 95% confidence intervals\nfor (i in 1:length(results$t0)) {\n  cat(\"\\n\", names(results$t0)[i], \"\\n\")\n  print(boot.ci(results, type = \"bca\", index = i))\n}",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#permutation-test",
    "href": "36-ancova_glm.html#permutation-test",
    "title": "\n14¬† ANCOVA and general linear model\n",
    "section": "\n14.8 Permutation test",
    "text": "14.8 Permutation test\n\n############################################################\n##########\n# Permutation test\n#\n# using lmperm library\n# To simplify future modifications of the code in this file,\n# copy the data to a generic mydata dataframe\nmydata &lt;- anc3dat\n# create a myformula variable containing the formula for the\n# model to be fitted\nmyformula &lt;- as.formula(lfkl ~ lage + locate + lage:locate)\nlibrary(lmPerm)\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate p-values for each term by permutation\n# Note that lmp centers numeric variable by default, so to\n# get results that are\n# consistent with standard models, it is necessary to set\n# center=FALSE\nmymodelProb &lt;- lmp(myformula,\n  data = mydata, center = FALSE,\n  perm = \"Prob\"\n)\nsummary(mymodel)\nsummary(mymodelProb)",
    "crumbs": [
      "Data & Code",
      "Linear models",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>ANCOVA and general linear model</span>"
    ]
  },
  {
    "objectID": "41-glm.html",
    "href": "41-glm.html",
    "title": "\n15¬† Generalized linear model, glm\n",
    "section": "",
    "text": "15.1 Lecture\nDream pet dragon\nm1 &lt;- glm(fish ~ french_captain, data = dads_joke, family = poisson)",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Generalized linear model, `glm`</span>"
    ]
  },
  {
    "objectID": "41-glm.html#lecture",
    "href": "41-glm.html#lecture",
    "title": "\n15¬† Generalized linear model, glm\n",
    "section": "",
    "text": "15.1.1 Distributions\n\n15.1.1.1 Continuous linear\n\nGaussian\n\n15.1.1.2 Count data\n\npoisson\nnegative binomial\nquasi-poisson\ngeneralized poisson\nconway-maxwell poisson\n\n15.1.1.3 censored distribution\n\n15.1.1.4 zero-inflated / hurdle distribution\n\nzero-inflated/zero-truncated poisson\ncensored poisson\n\n15.1.1.5 zero-truncated distribution\n\n15.1.1.6 zero-one-inflated distribution\nsee https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html see alo MCMCglmm coursenotes\nfor help on description and to add some plots about those distribution",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Generalized linear model, `glm`</span>"
    ]
  },
  {
    "objectID": "41-glm.html#practical",
    "href": "41-glm.html#practical",
    "title": "\n15¬† Generalized linear model, glm\n",
    "section": "\n15.2 Practical",
    "text": "15.2 Practical\n\n\n\n\n\n\nWarning\n\n\n\nThis section need to be severely updated\n\n\n\n15.2.1 Logistic regression\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nlibrary(performance)\n\nmouflon &lt;- read.csv(\"data/mouflon.csv\")\nmouflonc &lt;- mouflon[order(mouflon$age),]\n\nmouflonc$reproduction &lt;- ifelse(mouflonc$age &lt; 13, mouflonc$reproduction, 0)\nmouflonc$reproduction &lt;- ifelse(mouflonc$age &gt; 4, mouflonc$reproduction, 1)\n\nplot(reproduction ~ age, mouflonc)\n\n\n\n\n\n\nplot(jitter(reproduction) ~ jitter(age), mouflonc)\n\n\n\n\n\n\nbubble &lt;- data.frame(age = rep(2:16, 2),\n                     reproduction = rep(0:1, each = 15),\n                     size = c(table(mouflonc$age, mouflonc$reproduction)))\nbubble$size &lt;- ifelse(bubble$size == 0 , NA, bubble$size)\n ggplot(data = bubble, aes(x = age, y = reproduction))+\n geom_point(aes(size = size*10))\n\nWarning: Removed 7 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nm1 &lt;- glm(reproduction ~ age,\n    data = mouflonc,\n    family = binomial)\nsummary(m1)\n\n\nCall:\nglm(formula = reproduction ~ age, family = binomial, data = mouflonc)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.19921    0.25417   12.59   &lt;2e-16 ***\nage         -0.36685    0.03287  -11.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 928.86  on 715  degrees of freedom\nResidual deviance: 767.51  on 714  degrees of freedom\n  (4 observations deleted due to missingness)\nAIC: 771.51\n\nNumber of Fisher Scoring iterations: 4\n\nsimulationOutput &lt;- simulateResiduals(m1)\nplot(simulationOutput)\n\n\n\n\n\n\n\nplotting the model prediction on the link (latent) scale\n\nmouflonc$logit_ypred &lt;- 3.19921 -0.36685 * mouflonc$age\nplot(logit_ypred ~  jitter(age), mouflonc)\npoints(mouflonc$age, mouflonc$logit_ypred, col=\"red\", type = \"l\", lwd = 2)\n\n\n\n\n\n\n\nplotting on the observed scale\n\nmouflonc$ypred &lt;- exp(mouflonc$logit_ypred) / (1 + exp(mouflonc$logit_ypred)) # inverse of logit \n\nplot(reproduction ~  jitter(age), mouflonc)\npoints(mouflonc$age, mouflonc$ypred, col=\"red\", type = \"l\", lwd = 2)\n\n\n\n\n\n\n\nEnfin, pour se simplifier la vie, il est aussi possible de r√©cup√©rer les valeurs pr√©dites de y directement\n\nplot(x,y)\nmyreg &lt;- glm(y~x, family=binomial(link=logit))\nypredit &lt;- myreg$fitted\no=order(x)\npoints(x[o],ypredit[o], col=\"red\", type=\"l\", lwd=2)\n\n\nm2 &lt;- glm(reproduction ~ age + mass_sept + as.factor(sex_lamb) + mass_gain + density + temp,\n    data = mouflon,\n    family = binomial)\n\nsummary(m2)\n\n\nCall:\nglm(formula = reproduction ~ age + mass_sept + as.factor(sex_lamb) + \n    mass_gain + density + temp, family = binomial, data = mouflon)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           1.622007   1.943242   0.835 0.403892    \nage                  -0.148567   0.033597  -4.422 9.78e-06 ***\nmass_sept             0.029878   0.016815   1.777 0.075590 .  \nas.factor(sex_lamb)1 -0.428169   0.166156  -2.577 0.009969 ** \nmass_gain            -0.094828   0.026516  -3.576 0.000348 ***\ndensity              -0.018132   0.003518  -5.154 2.55e-07 ***\ntemp                  0.037244   0.138712   0.269 0.788313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 916.06  on 674  degrees of freedom\nResidual deviance: 845.82  on 668  degrees of freedom\n  (45 observations deleted due to missingness)\nAIC: 859.82\n\nNumber of Fisher Scoring iterations: 4\n\ncheck_model(m2)\n\n\n\n\n\n\nsimulationOutput &lt;- simulateResiduals(m2)\nplot(simulationOutput)\n\n\n\n\n\n\n\n\n15.2.1.1 previous offspring sex effect\n\npred.data &lt;- data.frame(\n  age = mean(mouflon$age),\n  mass_sept = mean(mouflon$mass_sept),\n  sex_lamb = c(0,1),\n  mass_gain = mean(mouflon$mass_gain),\n  density = mean(mouflon$density),\n  temp = mean(mouflon$temp, na.rm =TRUE))\n\n  predict(m2, newdata = pred.data)\n\n        1         2 \n0.6225895 0.1944205 \n\n\n\n15.2.2 Poisson regression\ndata on galapagos islands species richness model of total number of species model of proportion of native model of density of species\nFit 3 models - model of total number of species - model of proportion of endemics to total - model of species density\n\n  hist(rpois(10000,3))\n\n\n\n\n\n\n#\n gala &lt;- read.delim2(\"data/gala.txt\")\n plot(Species ~ Area, gala)\n\n\n\n\n\n\n plot(Species ~ log(Area), gala)\n\n\n\n\n\n\n hist(gala$Species)\n\n\n\n\n\n\n modpl &lt;- glm(Species ~ Area + Elevation + Nearest, family=poisson, gala)\nres &lt;- simulateResiduals(modpl)\ntestDispersion(res)\n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 110.32, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\ntestZeroInflation(res)\n\n\n\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = NaN, p-value = 1\nalternative hypothesis: two.sided\n\n mean(gala$Species)\n\n[1] 85.23333\n\n var(gala$Species)\n\n[1] 13140.74\n\n hist(rpois(nrow(gala),mean(gala$Species)))\n\n\n\n\n\n\n plot(modpl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced\nWarning in sqrt(crit * p * (1 - hh)/hh): NaNs produced",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Generalized linear model, `glm`</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html",
    "href": "42-model_freq.html",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "",
    "text": "16.1 R packages and data\nFor this lab you need:",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#set-freq",
    "href": "42-model_freq.html#set-freq",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "",
    "text": "R packages:\n\nvcd\nvcdExtra\ncar\n\n\ndata files\n\nUSPopSurvey.csv\nloglin.csv\nsturgdat.csv",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#organizing-the-data-3-forms",
    "href": "42-model_freq.html#organizing-the-data-3-forms",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "\n16.2 Organizing the data: 3 forms",
    "text": "16.2 Organizing the data: 3 forms\nSome biological experiments yield count data, e.g., the number of plants infected by a plant pathogen under different exposure regimes, the number of male and female turtles hatched under different incubation temperature treatments (in turtles, sex is temperature dependent!), etc. Usually the statistical issue here is whether the proportion of individuals in different categories (e.g., infected versus uninfected, male versus female, etc.) differs significantly among treatments. To examine this question, we can to set up a data file that lists the number of individuals in each category. There are 3 ways to do this. You should be able to decide which one is appropriate, and how to convert between them with R.\nThe file USPopSurvey.csv contains the results of a 1980 U.S population survey of a mid-eastern town:\n\nUSPopSurvey &lt;- read.csv(\"data/USPopSurvey.csv\")\nUSPopSurvey\n\n   ageclass    sex frequency\n1       0-9 female     17619\n2     10-19 female     17947\n3     20-29 female     21344\n4     30-39 female     19138\n5     40-49 female     13135\n6     50-59 female     11617\n7     60-69 female     11053\n8     70-79 female      7712\n9       80+ female      4114\n10      0-9   male     17538\n11    10-19   male     18207\n12    20-29   male     21401\n13    30-39   male     18837\n14    40-49   male     12568\n15    50-59   male     10661\n16    60-69   male      9374\n17    70-79   male      5348\n18      80+   male      1926\n\n\nNote that there are 18 lines and 3 columns in this file. Each line lists the number of individuals (frequency) of a given sex and age class. There are (sum(USPopSurvey$frequency)) 239539 individuals that were classified into the 18 (2 sexes x 9 age classes) categories. This way of presenting data is the frequency form. It is a compact way to present the data when there are only categorical variables.\nWhen there are continuous variables, the frequency form can‚Äôt be utilized (or provides no gain since each observation could possibly have a different values for the continuous variable(s)). Data have therefore to be stored in case form where each observation (individual) represents one line in the data file, and each variable is a column. Conveniently, the vcdExtra üì¶ includes the expand.dft() function to convert from the frequency to case form. For example, to create a data frame with 239539 lines and 2 columns (sex and ageclass):\n\nUSPopSurvey.caseform &lt;- expand.dft(USPopSurvey, freq = \"frequency\")\nhead(USPopSurvey.caseform)\n\n  ageclass    sex\n1      0-9 female\n2      0-9 female\n3      0-9 female\n4      0-9 female\n5      0-9 female\n6      0-9 female\n\ntail(USPopSurvey.caseform)\n\n       ageclass  sex\n239534      80+ male\n239535      80+ male\n239536      80+ male\n239537      80+ male\n239538      80+ male\n239539      80+ male\n\n\nFinally, these data can also be represented in table form (contingency table) where each variable is represented by a dimension of the n-dimensional table (here, for example, rows could represent each age class, and columns each sex), and the cells of the resulting table contain the frequencies. The table form can be created from the case or frequency form by the xtabs() command with slightly different syntax:\n\n# convert case form to table form\nxtabs(~ ageclass + sex, USPopSurvey.caseform)\n\n        sex\nageclass female  male\n   0-9    17619 17538\n   10-19  17947 18207\n   20-29  21344 21401\n   30-39  19138 18837\n   40-49  13135 12568\n   50-59  11617 10661\n   60-69  11053  9374\n   70-79   7712  5348\n   80+     4114  1926\n\n# convert frequency form to table form\nxtabs(frequency ~ ageclass + sex, data = USPopSurvey)\n\n        sex\nageclass female  male\n   0-9    17619 17538\n   10-19  17947 18207\n   20-29  21344 21401\n   30-39  19138 18837\n   40-49  13135 12568\n   50-59  11617 10661\n   60-69  11053  9374\n   70-79   7712  5348\n   80+     4114  1926\n\n\n\n(#tab:unnamed-chunk-1)Tools for converting among different forms for categorical data.\n\n\n\n\n\n\n\nFrom (Row) \\ To (column)\nCase form\nFrequency form\nTable form\n\n\n\nCase form\n\nxtabs(~ A + B)\ntable(A, B)\n\n\nFrequency form\nexpand.dft(X)\n\nxtabs(count ~ A + B)\n\n\nTable form\nexpand.dft(X)\nas.data.frame(X)",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#graphs-for-contingency-tables-and-testing-for-independence",
    "href": "42-model_freq.html#graphs-for-contingency-tables-and-testing-for-independence",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "\n16.3 Graphs for contingency tables and testing for independence",
    "text": "16.3 Graphs for contingency tables and testing for independence\nContingency tables can be used to test for independence. By this we mean to answer the question: Is the classification of observations according to one variable (say, sex) independent from the classification by another variable (say, ageclass). In other words, is the proportion of males and females independent of age, or does it vary among age classes?\nThe vcd üì¶ includes a mosaic() function useful to graphically display contingency tables:\n\nlibrary(vcd)\nUSTable &lt;- xtabs(frequency ~ ageclass + sex, data = USPopSurvey) # save the table form as USTable dataframe\n# Mosaic plot of the contingency table\nmosaic(USTable)\n\n\n\nMosaic plot of sex classes per age\n\n\n\nMosaic plots represent the proportion of observations in each combination of categories (here there are 18 categories, 2 sexes x 9 age classes). Categories with a higher proportion of observations are represented by larger rectangles. Visually, one can see that males and females are approximately equal for young age classes, but that the proportion of females increases quite a bit amongst the elders.\nThe Chi square test can be used to test the null hypothesis that the proportion of males and females does not differ among age classes:\n\n# Test of independence\nchisq.test(USTable) # runs chi square test of independence of sex and age class\n\n\n    Pearson's Chi-squared test\n\ndata:  USTable\nX-squared = 1162.6, df = 8, p-value &lt; 2.2e-16\n\n\nFrom this we conclude there is ample evidence to reject the null hypothesis that ageclass and sex are independent, which isn‚Äôt particularly surprising.\nThe mosaic plot from the vcd üì¶ can be shaded to show the categories that contribute most to the lack of independence:\n\n# Mosaic plot of the contingency table with shading\nmosaic(USTable, shade = TRUE)\n\n\n\nMosaic plot of sex by age with colours\n\n\n\nThe shading of each rectangle is proportional to the extent that observed frequencies deviate from what would be expected if sex and age class were independent. The age classes 40-49 and 50-59 have a sex ratio about equal to the overall sex:ratio for the entire dataset, and appear in grey. There are more young males and old females than expected if sex ratio did not change with age, and these rectangles are coded in blue. On the other hand, there are fewer young females and old males than if sex ratio did not change with age, and these rectangles are red coded. Note that the p-value printed on the right of the graph is for the chi-square test that assumes that observations are independent.\nThe estimation of p-value associated with the chi square statistic is less than ideal when expected frequencies are small in some of the cells, particularly for 2x2 contingency tables. Two options are then preferred, depending on the number of observations. For large samples, like in this example with more than 200,000 cases(!), a Monte Carlo approach is suggested and can be obtained by adding simulate.p.value=TRUE as an argument to the chisq.test() function\n\n# Monte-carlo estimation of p value (better for small n)\nchisq.test(USTable, simulate.p.value = TRUE, B = 10000)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 10000\n    replicates)\n\ndata:  USTable\nX-squared = 1162.6, df = NA, p-value = 9.999e-05\n\n\nHere, the simulation was done B=10000 times, and the chi square value observed with the data was never exceeded so p is estimated as 1/10001=9.999e-05, which is much larger than the p-value estimated from the theoretical chi square distribution (p&lt; 2.2e-16). This difference in p-value is at least partly an artifact of the number of simulations. To estimate p values as small as 1e-16, at least 1016 simulations must be run. And I am not THAT patient. For small tables with relatively low expected frequencies, Fisher‚Äôs exact test can be run to test for independence. This result is unbiased if row and column totals are fixed, but is conservative (i.e.¬†it will incorrectly fail to reject the null more often than expected) if row and/ or column totals are not fixed.\nBut this test will fail for large samples, like in this example:\n\n# Fisher exact test for contingency tables (small samples and small tables)\nfisher.test(USTable) # fails here because too many observations\n\nError in fisher.test(USTable): FEXACT error 40.\nOut of workspace.\n\nfisher.test(USTable, simulate.p.value = TRUE, B = 10000)\n\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    10000 replicates)\n\ndata:  USTable\np-value = 9.999e-05\nalternative hypothesis: two.sided",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#log-linear-models-as-an-alternative-to-chi-square-test-for-contingency-tables",
    "href": "42-model_freq.html#log-linear-models-as-an-alternative-to-chi-square-test-for-contingency-tables",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "\n16.4 Log-linear models as an alternative to Chi-square test for contingency tables",
    "text": "16.4 Log-linear models as an alternative to Chi-square test for contingency tables\nBy now, hopefully, you have learned to appreciate the flexibility and generality of general linear models and you realize that the t-test is a special, simple, case of a linear model with one categorical independent variable. The analysis of contingency tables by chi square test can similarly be generalized. Indeed, generalized linear models for poisson distributed data can be used when the dependent variable are frequencies (count data) and the independent variables can be categorical only (like for contingency tables, these are also called log- linear models), continuous only (Poisson regression), or a combination of categorical and continuous independent variables (this, too is a Poisson regression, but with added categorical variables, analogous to an ANCOVA sensu largo).\nSuch models predict the natural log frequency of observations given the independent variables. Like for linear models assuming normality of residuals, one can assess the overall quality of the fit (by AIC for example), and the significance of terms (say by comparing the fit of models including or excluding particular terms). One can even, if desired, obtain estimates of the parameters for each model term, with confidence intervals and p-values for the null hypothesis that the value of the parameter is 0.\nThe glm() function with the option family=poisson() allows the estimation, by maximum likelihood, of linear models for count data. One ‚Äúpeculiarity‚Äù of fitting such models to contingency table data is that generally the only terms of interest are the interactions. Going back to the population survey data in frequency form, with sex and ageclass as independent variables, one can fit a glm model by:\n\nmymodel &lt;- glm(frequency ~ sex * ageclass, family = poisson(), data = USPopSurvey)\nsummary(mymodel)\n\n\nCall:\nglm(formula = frequency ~ sex * ageclass, family = poisson(), \n    data = USPopSurvey)\n\nCoefficients:\n                       Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)            9.776733   0.007534 1297.730  &lt; 2e-16 ***\nsexmale               -0.004608   0.010667   -0.432   0.6657    \nageclass10-19          0.018445   0.010605    1.739   0.0820 .  \nageclass20-29          0.191793   0.010179   18.842  &lt; 2e-16 ***\nageclass30-39          0.082698   0.010441    7.921 2.36e-15 ***\nageclass40-49         -0.293697   0.011528  -25.477  &lt; 2e-16 ***\nageclass50-59         -0.416508   0.011951  -34.850  &lt; 2e-16 ***\nageclass60-69         -0.466276   0.012134  -38.428  &lt; 2e-16 ***\nageclass70-79         -0.826200   0.013654  -60.511  &lt; 2e-16 ***\nageclass80+           -1.454582   0.017316  -84.004  &lt; 2e-16 ***\nsexmale:ageclass10-19  0.018991   0.014981    1.268   0.2049    \nsexmale:ageclass20-29  0.007275   0.014400    0.505   0.6134    \nsexmale:ageclass30-39 -0.011245   0.014803   -0.760   0.4475    \nsexmale:ageclass40-49 -0.039519   0.016416   -2.407   0.0161 *  \nsexmale:ageclass50-59 -0.081269   0.017136   -4.742 2.11e-06 ***\nsexmale:ageclass60-69 -0.160154   0.017633   -9.083  &lt; 2e-16 ***\nsexmale:ageclass70-79 -0.361447   0.020747  -17.422  &lt; 2e-16 ***\nsexmale:ageclass80+   -0.754343   0.029598  -25.486  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5.3611e+04  on 17  degrees of freedom\nResidual deviance: 6.5463e-12  on  0  degrees of freedom\nAIC: 237.31\n\nNumber of Fisher Scoring iterations: 2\n\n\nFitting the full model, with the sex:ageclass interaction, allows the proportion of males and females to vary among ageclass levels, and hence to estimate exactly the frequencies for each combination of sex and ageclass (note that the deviance residuals are all 0‚Äôs and that the Residual deviance is also approximately zero).\nA masochist can use the coefficient table to obtain the predicted values for sex and ageclass categories by summing the appropriate coefficients. The predicted values, like for multiway ANOVA model, are obtained by combining the coefficients. Remembering that the first level of a factor (alphabetically) is used as a reference, here the coefficient for the intercept (9.776733) is the predicted value for the natural log of the number of observations for females in the first alphabetical ageclass (0 to 9). Indeed e9.776733 is approximately equal to 17619, the observed number of females in that age class. For example, for males in the 80+ ageclass, calculate the antilog of the coefficient for the intercept (for female in the youngest age class) plus the coefficient for sexmale (equal to the difference between ln frequency of females and males overall), plus the coefficient for the ageclass 80+ corresponding in the difference in frequency on average between the oldest and reference ageclass, plus the coefficient for the interaction terms sexmale:ageclass80+ (corresponding to the difference in the proportion of male for this ageclass compared to the youngest ageclass), so \\(ln(frequency)=9.776733-0.004608-1.454582- 0.754343 = 7.5632\\), and the frequency is equal to e7.5632 = 1926\nAlthough there are numerous p values in this output, they are not really helpful. To test whether the effect of sex on observed frequency is the same across ageclass levels, i.e is sex and age are independent, one needs to fit a model where the interaction sex:ageclass is removed, and see how badly this affects the fit. The Anova() function of the car package provides a handy shortcut:\n\nAnova(mymodel, type = 3, test = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: frequency\n             LR Chisq Df Pr(&gt;Chisq)    \nsex               0.2  1     0.6657    \nageclass      21074.6  8     &lt;2e-16 ***\nsex:ageclass   1182.2  8     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe use of type=3 and test=‚ÄúLR‚Äù ensures that the test performed to compare the full and reduced models is the Likelihood Ratio Chi- Square using the Residual deviance, and that it is a partial test, not a sequential one.\nAccording to these tests, there is no main effect of sex (p=0.667) but there is a main effect of ageclass and a significant sex:ageclass interaction. The significant interaction means that the effect of sex on frequency varies with ageclass, or that the sex ratio varies with age. The main effect of ageclass means that the frequency of individuals varies with age (i.e some ageclass are more populous than others), The absence of a main effect of sex suggests that there are approximately the same frequency of males and females in this sample (although, since there is an interaction, you have to be careful in making this assertion. It is ‚Äútrue‚Äù overall, but appears incorrect for individual age categories).",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#testing-an-external-hypothesis",
    "href": "42-model_freq.html#testing-an-external-hypothesis",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "\n16.5 Testing an external hypothesis",
    "text": "16.5 Testing an external hypothesis\nThe above test of independence is that of an internal hypothesis because the proportions used to calculate the expected frequencies assuming independence of sex and ageclass come from the data (i.e.¬†the overall proportion of males in females in the entire dataset, and the proportions of individuals in each ageclass, males and females combined.\nTo test the (external) null hypothesis that the sex ratio is 1:1 for the youngest individuals (ageclass 0-9), one has to compute the 2 X 2 table of observed and expected frequencies. The expected frequencies are obtained simply by summing male and female frequencies and dividing by two.\nR program to create and analyze a 2x2 table to test an external hypothesis\n\n### Produce a table of obs vs exp for 0-9 age class\nPopn0.9 &lt;- rbind(c(17578, 17578), c(17619, 17538))\n### Run X2 test on above table\nchisq.test(Popn0.9, correct = F) ### X2 without Yates\nchisq.test(Popn0.9) ### X2 with Yates\n\n\n\n\n\n\n\nExercise\n\n\n\nTest the null hypothesis that the proportion of male and female at birth is equal. What is your conclusion? Do you think the data is appropriate to test this hypothesis?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\nchisq.test(Popn0.9, correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  Popn0.9\nX-squared = 0.093309, df = 1, p-value = 0.76\n\nchisq.test(Popn0.9)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  Popn0.9\nX-squared = 0.088758, df = 1, p-value = 0.7658\n\n\n\n\nIn the past, for 2 X 2 tables Yates‚Äôs correction was frequently employed (first test above, but it has since been shown to be overly conservative and is no longer recommended (although it doesn‚Äôt affect the results in this particular instance). Better is a Fisher‚Äôs exact test if the total number of cases is &lt;200 (which is not the case here), or a randomization. Given that we cannot use a Fisher‚Äôs exact test here we are using a Yate‚Äôs correction.\nThese data are not particularly good for testing the null hypothesis that the sex ratio at birth is 1:1 because the first age category is too coarse. It is entirely possible that at birth there is an unequal sex-ratio, but there is compensatory age-specific mortality (e.g.¬†more males at birth, but reduced survivorship among males in the first 9 years of life relative to females). In this case, the sex ratio at birth is NOT 1:1, but we still accept the null hypothesis based on age class 0-9.",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#poisson-regression-to-analyze-multi-way-tables",
    "href": "42-model_freq.html#poisson-regression-to-analyze-multi-way-tables",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "\n16.6 Poisson regression to analyze multi-way tables",
    "text": "16.6 Poisson regression to analyze multi-way tables\n\nloglin &lt;- read.csv(\"data/loglin.csv\")\n# Convert from frequency form to table form for mosaic plot\nloglinTable &lt;- xtabs(frequency ~ temperature + light + infected, data = loglin)\n# Create mosaic plot to look at data\nmosaic(loglinTable, shade = TRUE)\n\n\n\nProportion de plantes infect√©es en fonction de la temp√©rature er la lumi√®re\n\n\n\nThe principle of testing for independence through interactions can be extended to multi-way tables, that is, tables in which more than two criteria are used to classify observations. For example, suppose that we wanted to test the effect of temperature (two levels: high and low) and light (two levels: high irradiance and low irradiance) on the number of plants infected by a plant pathogen (two levels: infected and non-infected). In this case we would need a three-way table with three criteria (infection status, temperature, and light).\nFitting log linear models to frequency data involves testing of different models by comparing them with the full (saturated) model. A series of simplified models is produced, each model missing one of the interactions of interest, and the fit of each simplified model is compared to that of the full model. If the fit does not change much, then the term eliminated does not have much influence on the frequencies, whereas if the resulting model provides a significantly worse fit, then the term is important. As with two-way tables, the terms of interest are the interactions, not the main effects, if what we are testing for is independence of different factors.\nThe file loglin.csv contains the frequencies ( frequency ) of infected and non-infected plants ( infected ) at low and high temperature ( temperature) and low and high light ( light). To graph the data and determine if infected status depends on light and temperature, one can construct a mosaic plot and a loglinear model.\n\n# Convert from frequency form to table form for mosaic plot\nloglinTable &lt;- xtabs(frequency ~ temperature + light + infected, data = loglin)\n# Create mosaic plot to look at data\nlibrary(vcd)\nmosaic(loglinTable, shade = TRUE)\n\n\n\n\n\n\n\nThe symmetrical experimental design with the same number of observations made at the two levels of light and of temperature is apparent in the above plot in the overall equal area occupied by the observations in each of the four quadrants. What is of interest, the infected status, appears to vary among the quadrants (i.e.¬†levels of light and temperature). For example, the red rectangles in the lower left and upper right quadrants indicates that there were fewer infected plants at high light and low temperature (bottom left), and fewer uninfected plants at low light and high temperature than if the infected level was not affected by light and temperature. The p-value at the bottom of the color scale represents a test of independence equivalent to testing the full model against a reduced model including only the main effect of temperature, light, and infected status on the (ln) number of observations.\n\n# Fit full model\nfull.model &lt;- glm(frequency ~ temperature * light * infected, family = poisson(), data = loglin)\n# Test partial effect of terms in full model\nAnova(full.model, type = 3, test = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: frequency\n                           LR Chisq Df Pr(&gt;Chisq)    \ntemperature                  9.1786  1  0.0024487 ** \nlight                       13.2829  1  0.0002678 ***\ninfected                     0.0000  1  0.9999999    \ntemperature:light            5.6758  1  0.0172008 *  \ntemperature:infected        29.0612  1  7.013e-08 ***\nlight:infected              20.2687  1  6.729e-06 ***\ntemperature:light:infected   1.0840  1  0.2978126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe probabilities associated with each term in the full model are here calculated by comparing the fit of the full model to that of a model with this particular term removed. As is typical in log-linear model analyses, many of the tests here are not interesting. If the biological question is about how infected status varies with other conditions, then the only informative terms are the interaction terms involving infected status.\nThere are therefore only 3 terms of interest:\n\n\ntemperature:infected significant interaction implies that infection status is not independent of temperature. Indeed the mosaic plot shows that the proportion of infected cases is higher at high temperature.\n\nlight:infected significant interaction implies that infection status is not independent of light. The mosaic plot also indicates that the proportion of infected plants is larger at low light levels.\n\ntemperature:light:infected 3 way-interaction is not significant. This implies that the previous 2 effects do not vary between levels of the third variable. So there is no evidence that the effect of light on infection status varies at the two temperatures, or that the effect of temperature on infection status varies between the two light levels. We should therefore drop this term and refit before evaluating the 2-way interactions (small increase in power).",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "42-model_freq.html#ex-glm",
    "href": "42-model_freq.html#ex-glm",
    "title": "\n16¬† Frequency data and Poisson Regression\n",
    "section": "\n16.7 Exercice",
    "text": "16.7 Exercice\nWe will now work with the sturgdat data set to test the hypothesis that number of fish caught is independent of location, year, and gender. Before the analysis, the data will have to be reshaped to be in suitable format for fitting a log-linear model.\n\n\n\n\n\n\nExercise\n\n\n\nOpen sturgdat.csv , then use the table() function to summarize the data according to number of individuals by sex , location , and year . Save this object as sturgdat.table . Make a mosaic plot of the data.\n\n\n\nsturgdat &lt;- read.csv(\"data/sturgdat.csv\")\n# Reorganize data from case form to table form\nsturgdat.table &lt;- with(sturgdat, table(sex, year, location))\n# display the table\nsturgdat.table\n\n, , location = CUMBERLAND  \n\n              year\nsex            1978 1979 1980\n  FEMALE         10   30   11\n  MALE           14   14    6\n\n, , location = THE_PAS     \n\n              year\nsex            1978 1979 1980\n  FEMALE          5   12   38\n  MALE           16   12   18\n\n# Create data frame while converting from table form to frequency form\nsturgdat.freq &lt;- as.data.frame(sturgdat.table)\n# display data frame\nsturgdat.freq\n\n            sex year     location Freq\n1  FEMALE       1978 CUMBERLAND     10\n2  MALE         1978 CUMBERLAND     14\n3  FEMALE       1979 CUMBERLAND     30\n4  MALE         1979 CUMBERLAND     14\n5  FEMALE       1980 CUMBERLAND     11\n6  MALE         1980 CUMBERLAND      6\n7  FEMALE       1978 THE_PAS         5\n8  MALE         1978 THE_PAS        16\n9  FEMALE       1979 THE_PAS        12\n10 MALE         1979 THE_PAS        12\n11 FEMALE       1980 THE_PAS        38\n12 MALE         1980 THE_PAS        18\n\n# Look at the data as mosaic plot\n# mosaic using the table created above\nmosaic(sturgdat.table, shade = TRUE)\n\n\n\nFrequency of female and male sturgeon as a function of year and location\n\n\n\ncallout-caution # Exercise Using the frequency form of the table, fit the full log-linear model just as we did with the loglin data set and produce the anova table with chi square statistics for the terms in the model. Is the 3-way interaction significant ( location:year:sex )? Does sex ratio change between locations or among years? ::\n\n# Fit full model\nfull.model &lt;- glm(Freq ~ sex * year * location, data = sturgdat.freq, family = \"poisson\")\nsummary(full.model)\n\n\nCall:\nglm(formula = Freq ~ sex * year * location, family = \"poisson\", \n    data = sturgdat.freq)\n\nCoefficients:\n                                              Estimate Std. Error z value\n(Intercept)                                    2.30259    0.31623   7.281\nsexMALE                                        0.33647    0.41404   0.813\nyear1979                                       1.09861    0.36515   3.009\nyear1980                                       0.09531    0.43693   0.218\nlocationTHE_PAS                               -0.69315    0.54772  -1.266\nsexMALE        :year1979                      -1.09861    0.52554  -2.090\nsexMALE        :year1980                      -0.94261    0.65498  -1.439\nsexMALE        :locationTHE_PAS                0.82668    0.65873   1.255\nyear1979:locationTHE_PAS                      -0.22314    0.64550  -0.346\nyear1980:locationTHE_PAS                       1.93284    0.64593   2.992\nsexMALE        :year1979:locationTHE_PAS      -0.06454    0.83986  -0.077\nsexMALE        :year1980:locationTHE_PAS      -0.96776    0.87942  -1.100\n                                              Pr(&gt;|z|)    \n(Intercept)                                    3.3e-13 ***\nsexMALE                                        0.41641    \nyear1979                                       0.00262 ** \nyear1980                                       0.82732    \nlocationTHE_PAS                                0.20569    \nsexMALE        :year1979                       0.03658 *  \nsexMALE        :year1980                       0.15011    \nsexMALE        :locationTHE_PAS                0.20950    \nyear1979:locationTHE_PAS                       0.72957    \nyear1980:locationTHE_PAS                       0.00277 ** \nsexMALE        :year1979:locationTHE_PAS       0.93875    \nsexMALE        :year1980:locationTHE_PAS       0.27114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  5.7176e+01  on 11  degrees of freedom\nResidual deviance: -2.6645e-15  on  0  degrees of freedom\nAIC: 77.28\n\nNumber of Fisher Scoring iterations: 3\n\nAnova(full.model, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n                  LR Chisq Df Pr(&gt;Chisq)    \nsex                 0.6698  1  0.4131256    \nyear               13.8895  2  0.0009637 ***\nlocation            1.6990  1  0.1924201    \nsex:year            4.6930  2  0.0957024 .  \nsex:location        1.6323  1  0.2013888    \nyear:location      25.2580  2  3.276e-06 ***\nsex:year:location   1.6677  2  0.4343666    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is a three-way table, with three factors: sex, location and year . Thus, the ‚Äústaturated‚Äù or ‚Äúfull‚Äù loglinear model includes 7 terms: the three main effects ( sex, location and year ), the three 2-way interactions ( sex:year, sex:location and year: location ) and the one 3-way interaction ( sex:year:location ). The null deviance is 57.17574, the residual deviance of the full model is, not surprisingly, 0. The deviance explained by the three-way interaction, 1.66773 (which is, in fact, a chi square statistic with two degrees of freedom), is not significant, and we are therefore justified in fitting the model without this term.\nWhat does this mean? It means that, if there are any 2-way interactions, they do not depend on the level of the third variable. For example, if indeed the sex ratio of sturgeon varies among years (a sex:year interaction), that it varies in the same manner at the two locations. This in turn means that in testing for two-way interactions, we are (statistically) justified in pooling (summing) over the levels of the third variable. This is analog to what can be done in multiway ANOVA when high order interactions are not significant. For example, in testing for a sex:location effect, we can pool over year , to produce a 2 X 2 table whose cell counts are the total number of sturgeon of a given sex at a given location captured over the three years 1978-1980. By increasing cell counts, we increase statistical power, which is desirable.\n\nIf we adjust the model without the 3-way interaction, we get:\n\n\n\n\n\n\n\nSolution\n\n\n\n\no2int.model &lt;- glm(Freq ~ sex + year + location + sex:year + sex:location + year:location, data = sturgdat.freq, family = \"poisson\")\nAnova(o2int.model, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n              LR Chisq Df Pr(&gt;Chisq)    \nsex             1.8691  1  0.1715807    \nyear           15.1289  2  0.0005186 ***\nlocation        1.5444  1  0.2139568    \nsex:year       15.5847  2  0.0004129 ***\nsex:location    2.1762  1  0.1401583    \nyear:location  28.3499  2  6.981e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWe can see that the sex:location interaction does not explain a significant portion of the deviance, whereas the two others do. Sex ratio does not vary among locations, but it does among years. The year:location is also significant (see below for its meaning).\nShould you try to simplify the model further? Real statisticians are divided on this question. All agree that keeping insignificant terms in the model may cost some power. On the other hand, removing non significant interactions can lead to difficulty interpreting answers when observations are not well balanced (i.e.¬†there is colinearity among model terms).\n\nRefit the model, this time excluding the sex:location interaction.\n\n\n\n\n\n\n\nSolution\n\n\n\n\no2int.model2 &lt;- glm(Freq ~ sex + year + location + sex:year + year:location, data = sturgdat.freq, family = \"poisson\")\nAnova(o2int.model2, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n              LR Chisq Df Pr(&gt;Chisq)    \nsex             5.0970  1  0.0239677 *  \nyear           16.1226  2  0.0003155 ***\nlocation        0.2001  1  0.6546011    \nsex:year       13.9883  2  0.0009173 ***\nyear:location  26.7534  2  1.551e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNow the remaining two interactions are significant. It looks as though this is the ‚Äúbest‚Äù model. On the basis of the above analysis, the simplest model is:\n\\[ln[f_{(ijk)} ] = location + sex + year + sex:year + location:year\\]\nHow are these effects interpreted biologically? Remember, as in tests of independence, we are not interested in main effects, only the interactions. For example, the main effect location tells us that the total number of sturgeon caught (pooled over both sexes and all years 1978-1980) varied between the two locations. This is not surprising and uninteresting given that we have no information on the sampling effort. However, the sex:year interaction tells us that over the 3 year period, the sex-ratio of the harvest changed, and it changed in more or less the same fashion in the two locations, which is a rather interesting result. The location:year effect tells us that the total number of sturgeon harvested not only changed over the years, but that this change varied between locations. This could be caused by a difeerent fishing effort at one station over the years, or to a negative impact at one station only on one year. Whatever the cause, it affected males and females similarly since the 3 way interaction is not significant.",
    "crumbs": [
      "Data & Code",
      "Generalized linear models",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Frequency data and Poisson Regression</span>"
    ]
  },
  {
    "objectID": "51-lmm.html",
    "href": "51-lmm.html",
    "title": "\n17¬† Introduction to linear mixed models\n",
    "section": "",
    "text": "17.1 Lecture",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to linear mixed models</span>"
    ]
  },
  {
    "objectID": "51-lmm.html#lecture",
    "href": "51-lmm.html#lecture",
    "title": "\n17¬† Introduction to linear mixed models\n",
    "section": "",
    "text": "17.1.1 Testing fixed effects\nmaking a note that LRT on fixed effects should not be the preferred method and more inportantly should eb done using ML and not REML Fitsee pinheiro & Bates 2000 p76\n\n17.1.2 Shrinkage\nThe following is an example of shrinkage, sometimes called partial-pooling, as it occurs in mixed effects models. \nIt is often the case that we have data such that observations are clustered in some way (e.g.¬†repeated observations for units over time, students within schools, etc.). In mixed models, we obtain cluster-specific effects in addition to those for standard coefficients of our regression model. The former are called random effects, while the latter are typically referred to as fixed effects or population-average effects.\nIn other circumstances, we could ignore the clustering, and run a basic regression model. Unfortunately this assumes that all observations behave in the same way, i.e.¬†that there are no cluster-specific effects, which would often be an untenable assumption. Another approach would be to run separate models for each cluster. However, aside from being problematic due to potentially small cluster sizes in common data settings, this ignores the fact that clusters are not isolated and potentially have some commonality.\nMixed models provide an alternative where we have cluster specific effects, but ‚Äòborrow strength‚Äô from the population-average effects. In general, this borrowing is more apparent for what would otherwise be more extreme clusters, and those that have less data. The following will demonstrate how shrinkage arises in different data situations.\n\n17.1.2.1 Analysis\nFor the following we run a basic mixed model with a random intercept and random slopes for a single predictor variable. There are a number of ways to write such models, and the following does so for a single cluster \\(c\\) and observation \\(i\\). \\(y\\) is a function of the covariate \\(x\\), and otherwise we have a basic linear regression model. In this formulation, the random effects for a given cluster (\\(u_{* c}\\)) are added to each fixed effect (intercept \\(b_0\\) and the effect of \\(x\\), \\(b_1\\)). The random effects are multivariate normally distributed with some covariance. The per observation noise \\(\\sigma\\) is assumed constant across observations.\n\\[\\mu_{ic} = (b_0 + \\mathrm{u}_{0c})+ (b_1+\\mathrm{u}_{1c}) * x_{ic}\\] \\[\\mathrm{u}_{0}, \\mathrm{u}_{1} \\sim \\mathcal{N}(0, \\Sigma)\\] \\[y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\]\nSuch models are highly flexible and have many extensions, but this simple model is enough for our purposes.\n\n17.1.2.2 Data\nDefault settings for data creation are as follows:\n\n\nobs_per_cluster (observations per cluster) = 10\n\nn_cluster (number of clusters) = 100\n\nintercept (intercept) = 1\n\nbeta (coefficient for x) = .5\n\nsigma (observation level standard deviation) = 1\n\nsd_int (standard deviation for intercept random effect)= .5\n\nsd_slope (standard deviation for x random effect)= .25\n\ncor (correlation of random effect) = 0\n\nbalanced (fraction of overall sample size) = 1\n\nseed (for reproducibility) = 1024\n\nIn this setting, \\(x\\) is a standardized variable with mean zero and standard deviation of 1. Unless a fraction is provided for balanced, the \\(N\\), i.e.¬†the total sample size, is equal to n_cluster * obs_per_cluster. The following is the function that will be used to create the data, which tries to follow the model depiction above. It requires the tidyverse package to work.\n\n17.1.2.3 Run the baseline model\nWe will use lme4 to run the analysis. We can see that the model recovers the parameters fairly well, even with the default of only 1000 observations.\n\ndf &lt;- create_data()\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nmod &lt;- lmer(y ~ x + (x | cluster), df)\nsummary(mod, cor = F)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (x | cluster)\n   Data: df\n\nREML criterion at convergence: 3012.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9392 -0.6352 -0.0061  0.6156  2.8721 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n cluster  (Intercept) 0.29138  0.5398       \n          x           0.05986  0.2447   0.30\n Residual             0.99244  0.9962       \nNumber of obs: 1000, groups:  cluster, 100\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.93647    0.06282   14.91\nx            0.54405    0.04270   12.74\n\n\n\n\n17.1.2.4 Visualize the baseline model\nNow it is time to visualize the results. We will use gganimate to bring the shrinkage into focus. We start with the estimates that would be obtained by a ‚Äòregression-by-cluster‚Äô approach or a linear regression for each cluster. The movement shown will be of those cluster-specific estimates toward the mixed model estimates. On the x axis is the estimate for the intercepts, on the y axis are the estimated slopes of the x covariate.\n\n\n\n\n\n\n\n\nWe see more clearly what the mixed model does. The general result is that cluster-specific effects (lighter color) are shrunk back toward the population-average effects (the ‚Äòblack hole‚Äô), as the imposed normal distribution for the random effects makes the extreme values less probable. Likewise, those more extreme cluster-specific effects, some of which are not displayed as they are so far from the population average, will generally have the most shrinkage imposed. In terms of prediction, it is akin to introducing bias for the cluster specific effects while lowering variance for prediction of new data, and allows us to make predictions on new categories we have not previously seen - we just assume an ‚Äòaverage‚Äô cluster effect, i.e.¬†a random effect of 0.\n\n17.1.2.5 Summary\nMixed models incorporate some amount of shrinkage for cluster-specific effects. Data nuances will determine the relative amount of ‚Äòstrength borrowed‚Äô, but in general, such models provide a good way for the data to speak for itself when it should, and reflect an ‚Äòaverage‚Äô when there is little information. An additional benefit is that thinking about models in this way can be seen as a precursor to Bayesian approaches, which can allow for even more flexibility via priors, and more control over how shrinkage is added to the model.",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to linear mixed models</span>"
    ]
  },
  {
    "objectID": "51-lmm.html#practical",
    "href": "51-lmm.html#practical",
    "title": "\n17¬† Introduction to linear mixed models\n",
    "section": "\n17.2 Practical",
    "text": "17.2 Practical\n\n17.2.1 Overview\nThis practical is intended to get you started fitting some simple mixed models with so called random intercepts. The tutorial is derived from one that accompanied the paper (Houslay and Wilson 2017), ‚ÄúAvoiding the misuse of BLUP in behavioral ecology‚Äù. Here, you will be working through a simplified version in which I have taken more time to cover the basic mixed models and don‚Äôt cover multivariate models which were really the main point of that paper. So if you find this material interesting don‚Äôt worry we will go through a more advanced version of the original paper on multivariate models in chapter XX. The original version will be worth a work through to help you break into multivariate mixed models anyway! Here we will:\n\nLearn how to fit - and interpret the results of - a simple univariate mixed effect model\nSee how to add fixed and random effects to your model, and to test their significance in the normal frequentists sense\n\nWe are going to use the üì¶ lme4 (Bates et al. 2015) which is widely used and great for simple mixed models. However, since, for philosophical reasons, lme4 does not provide any p-values for either fixed or random effects, we are going to use the üì¶ lmerTest (Kuznetsova et al. 2017), which add a bunch a nice goodies to lme4 For slightly more complex models, including multivariate ones, generalised models, and random effects of things like shared space, pedigree, phylogeny I tend to use different üì¶ like MCMCglmm (Hadfield 2010) (which is Bayesian, look at Jarrod Hadfield‚Äôs excellent course notes (Hadfield 2010)) or ASReml-R (The VSNi Team 2023) (which is likelihood based/frequentist but sadly is not free).\n\n\n17.2.2 R packages needed\nFirst we load required libraries\n\nlibrary(lmerTest)\nlibrary(performance)\nlibrary(tidyverse)\nlibrary(rptR)\n\n\n17.2.3 The superb wild unicorns of the Scottish Highlands\nUnicorns, a legendary animal and also symbol or Scotland, are frequently described as extremely wild woodland creature but also a symbol of purity and grace. Here is one of most accurate representation of the lengendary animal.\n\n\n\n\nThe superb unicorn of the Scottish Highlands\n\n\n\nDespite their image of purity and grace, unicorns (Unicornus legendaricus) are raging fighter when it comes to compete for the best sweets you can find at the bottom of rainbows (unicorn favourite source of food).\nWe want to know:\n\nIf aggressiveness differs among individuals\nIf aggressive behaviour is plastic (change with the environment)\nIf aggressive behaviour depends on body condition of focal animal \n\n\nWith respect to plasticity, we will focus on rival size as an ‚Äòenvironment‚Äô. Common sense, and animal-contest theory, suggest a small animal would be wise not to escalate an aggressive contest against a larger, stronger rival. However, there are reports in the legendary beasty literature that they get more aggressive as rival size increases. Those reports are based on small sample sizes and uncontrolled field observations by foreigners Munro baggers enjoying their whisky after a long day in the hills.\n\n17.2.3.1 Experimental design\nHere, we have measured aggression in a population of wild unicorns. We brought some (n=80) individual into the lab, tagged them so they were individually identifiable, then repeatedly observed their aggression when presented with model ‚Äòintruders‚Äô (animal care committe approved). There were three models; one of average unicorn (calculated as the population mean body length), one that was build to be 1 standard deviation below the population mean, and one that was 1 standard deviation above.\nData were collected on all individuals in two block of lab work. Within each block, each animal was tested 3 times, once against an ‚Äòintruder‚Äô of each size. The test order in which each animal experienced the three instruder sizes was randomised in each block. The body size of all focal individuals was measured at the beginning of each block so we know that too (and have two separate measures per individual).\n\n17.2.3.2 looking at the data\nLet‚Äôs load the data file unicorns_aggression.csv in a R object named unicorns and make sure we understand what it contains\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nunicorns &lt;- read.csv(\"data/unicorns_aggression.csv\")\n\nYou can use summary(unicorns) to get an overview of the data and/or str(unicorns) to see the structure in the first few lines. This data frame has 6 variables:\n\nstr(unicorns)\n\n'data.frame':   480 obs. of  6 variables:\n $ ID        : chr  \"ID_1\" \"ID_1\" \"ID_1\" \"ID_1\" ...\n $ block     : num  -0.5 -0.5 -0.5 0.5 0.5 0.5 -0.5 -0.5 -0.5 0.5 ...\n $ assay_rep : int  1 2 3 1 2 3 1 2 3 1 ...\n $ opp_size  : int  -1 1 0 0 1 -1 1 -1 0 1 ...\n $ aggression: num  7.02 10.67 10.22 8.95 10.51 ...\n $ body_size : num  206 206 206 207 207 ...\n\nsummary(unicorns)\n\n      ID                block        assay_rep    opp_size    aggression    \n Length:480         Min.   :-0.5   Min.   :1   Min.   :-1   Min.   : 5.900  \n Class :character   1st Qu.:-0.5   1st Qu.:1   1st Qu.:-1   1st Qu.: 8.158  \n Mode  :character   Median : 0.0   Median :2   Median : 0   Median : 8.950  \n                    Mean   : 0.0   Mean   :2   Mean   : 0   Mean   : 9.002  \n                    3rd Qu.: 0.5   3rd Qu.:3   3rd Qu.: 1   3rd Qu.: 9.822  \n                    Max.   : 0.5   Max.   :3   Max.   : 1   Max.   :12.170  \n   body_size    \n Min.   :192.0  \n 1st Qu.:229.7  \n Median :250.0  \n Mean   :252.5  \n 3rd Qu.:272.0  \n Max.   :345.2  \n\n\n\n\n\nSo the different columns in the data set are:\n\nIndividual ID\n\nExperimental Block, denoted for now as a continuous variable with possible values of -0.5 (first block) or +0.5 (second block)\nIndividual body_size, as measured at the start of each block in kg\nThe repeat number for each behavioural test, assay_rep\n\nOpponent size (opp_size), in standard deviations from the mean (i.e., -1,0,1)\n\naggression, our behavioural trait, measured 6 times in total per individual (2 blocks of 3 tests)\n\nmaybe add something on how to look at data structure closely using tables\n\n17.2.4 Do unicorns differ in aggressiveness? Your first mixed model\nFit a first mixed model with lmer that have only individual identity as a random effect and only a population mean.\nWhy, so simple? Because we simply want to partition variance around the mean into a component that among-individual variance and one that is within-individual variance.\n\n\n\n\n\n\nImportant\n\n\n\nWe are going to use the function lmer() from the üì¶ lme4 package. The notation of the model formula is similar as the notation for a linear model but now we also add random effects using the notation (1 | r_effect) which indicates that we want to fit the variable r_effect as a random effect for the intercept. Thus, in lmer notation a simploe model would be :\nlmer(Y ~ x1 + x2 + (1 | r_effect), data = data)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA sensible researcher would probably take the time to do some exploratory data plots here. So let‚Äôs write a mixed model. This one is going to have no fixed effects except the mean, and just one random effect - individual identity.\n\nm_1 &lt;- lmer(aggression ~ 1 + (1 | ID), data = unicorns)\n\nboundary (singular) fit: see help('isSingular')\n\n\nThere is a warning‚Ä¶ something about ‚Äúsingularities‚Äù. Ignore that for a moment.\n\n\n\nNow you need to get the model output. By that I just mean use summary(model_name).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(m_1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: aggression ~ 1 + (1 | ID)\n   Data: unicorns\n\nREML criterion at convergence: 1503.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.68530 -0.73094 -0.04486  0.71048  2.74276 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.000    0.000   \n Residual             1.334    1.155   \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   9.00181    0.05272 479.00000   170.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n\n\n\nIn the summary you will find a table of fixed effects.\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   9.00181    0.05272 479.00000   170.7   &lt;2e-16 ***\nThe intercept (here the mean) is about 9 and is significantly &gt;0 - fine, but not very interesting to us.\nYou will also find a random effect table that contains estimates of the among individual (ID) and residual variances.\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.000    0.000   \n Residual             1.334    1.155   \nNumber of obs: 480, groups:  ID, 80\nThe among individual (ID) is estimated as zero. In fact this is what the cryptic warning was about: in most situations the idea of a random effect explaining less than zero variance is not sensible (strangely there are exception!). So by default the variance estimates are constrained to lie in positive parameter space. Here in trying to find the maximum likelihood solution for among-individual variance, our model has run up against this constraint.\n\n17.2.4.1 Testing for random effects\nWe can test the statistical significance of the random effect using the ranova() command in lmerTest. This function is actually doing a likelihood ratio test (LRT) of the random effect. The premise of which is that twice the difference in log-likelihood of the full and reduced (i.e.¬†with the random effect dropped) is itself distributed as \\(\\chi^2\\)$ with DF equal to the number of parameters dropped (here 1). Actually, there is a good argument that this is too conservative, but we can discuss that later. So let‚Äôs do the LRT for the random effect using ranova()\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nranova(m_1)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\naggression ~ (1 | ID)\n         npar  logLik    AIC LRT Df Pr(&gt;Chisq)\n&lt;none&gt;      3 -751.83 1509.7                  \n(1 | ID)    2 -751.83 1507.7   0  1          1\n\n\n\n\n\nThere is apparently no among-individual variance in aggressiveness.\nSo this is a fairly rubbish and underwhelming model. Let‚Äôs improve it.\n\n17.2.5 Do unicorns differ in aggressiveness? A better mixed model\nThe answer we got from our first model is not wrong, it estimated the parameters we asked for and that might be informative or not and that might be representative or not of the true biology. Anyway all models are wrong but as models go this one is fairly rubbish. In fact we have explained no variation at all as we have no fixed effects (except the mean) and our random effect variance is zero. We woud have seen just how pointless this model was if we‚Äôd plotted it\n\nplot(m_1)\n\n\n\nFitted values vs residuals for a simple mixed model of unicorn aggression\n\n\n\nSo we can probably do better at modelling the data, which may or may not change our view on whether there is any real variation among unicorns in aggressiveness.\nFor instance, we can (and should have started with) an initial plot of the phenotypic data against opponent size indicates to have a look at our prediction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe code below uses the excellent üì¶ ggplot2 but the same figure can be done using base R code.\n\nggplot(unicorns, aes(x = opp_size, y = aggression)) +\n  geom_jitter(\n    alpha = 0.5,\n    width = 0.05\n  ) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  labs(\n    x = \"Opponent size (SD)\",\n    y = \"Aggression\"\n  ) +\n  theme_classic()\n\n\n\n\n\nggplot(unicorns, aes(x = opp_size, y = aggression)) +\n  geom_jitter(\n    alpha = 0.5,\n    width = 0.05\n  ) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  labs(\n    x = \"Opponent size (SD)\",\n    y = \"Aggression\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\nFigure¬†17.1: Unicorn aggressivity as a function of opponent size when fighting for sweets\n\n\n\n\nAs predicted, there is a general increase in aggression with opponent size (points are lightly jittered on the x-axis to show the spread of data a little better)\nYou can see the same thing from a quick look at the population means for aggression at opponent size. Here we do it with the kable function that makes nice tables in rmarkdown documents.\n\nunicorns %&gt;%\n  group_by(opp_size) %&gt;%\n  summarise(mean_aggr = mean(aggression)) %&gt;%\n  knitr::kable(digits = 2)\n\n\n\nopp_size\nmean_aggr\n\n\n\n-1\n8.00\n\n\n0\n8.91\n\n\n1\n10.09\n\n\n\n\n\nSo, there does appear to be plasticity of aggression with changing size of the model opponent. But other things may explain variation in aggressiveness too - what about block for instance? Block effects may not be the subject of any biologically interesting hypotheses, but accounting for any differences between blocks could remove noise.\nThere may also be systematic change in behaviour as an individual experiences more repeat observations (i.e.¬†exposure to the model). Do they get sensitised or habituated to the model intruder for example?\nSo let‚Äôs run a mixed model with the same random effect of individual, but with a fixed effects of opponent size (our predictor of interest) and experimental block.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nm_2 &lt;- lmer(aggression ~ opp_size + block + (1 | ID), data = unicorns)\n\n\n\n\n\n17.2.5.1 Diagnostic plots\nRun a few diagnostic plots before we look at the answers. In diagnostic plots, we want to check the condition of applications of the linear mixed model which are the same 4 as the linear model plus 2 extra:\n\nLinearity of the relation between covariates and the response\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nDone with data exploration graph (i.e.¬†just plot the data see if it is linear) - see previous graph Figure¬†17.1.\n\n\n\n\nNo error on measurement of covariates\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nassumed to be correct if measurement error is lower than 10% of variance in the variable - I know this sounds pretty bad\n\n\n\n\nResidual have a Gaussian distribution\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nusing quantile-quantile plot or histogram of residuals\n\npar(mfrow = c(1, 2)) # multiple graphs in a window\nqqnorm(residuals(m_2)) # a q-q plot\nqqline(residuals(m_2))\nhist(resid(m_2)) # are the residuals roughly Gaussian?\n\n\n\nChecking residuals have Gaussian distribution\n\n\n\n\n\n\n\nHomoscedasticty (variance of residuals is constant across covariates)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsing plot of residuals by fitted values\n\nplot(m_2)\n\n\n\nResiduals by fitted values for model m_2 to check homoscedasticity\n\n\n\n\n\n\n\nRandom effects have a Gaussian distribution\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nhistogram of the predictions for the random effects (BLUPs)\n\n# extracting blups\nr1 &lt;- as.data.frame(ranef(m_2, condVar = TRUE))\npar(mfrow = c(1, 2))\nhist(r1$condval)\nqqnorm(r1$condval)\nqqline(r1$condval)\n\n\n\nChecking random effects are gaussian\n\n\n\n\n\n\n\nResidual variance is constant across all levels of a random effect\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo straightforward solution to deal with that. We can just do a plot is absolutely not-informative for that problem but I always like to look at. It is the plot of the sorted BLUPs with their associated errors.\n\nr1 &lt;- r1[order(r1$condval), ] # sorting the BLUPs\nggplot(r1, aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nHere is a great magic trick üéá because 3-5 and more can be done in one step\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou need to use the function check_model() from the üì¶ performance package.\n\ncheck_model(m_2)\n\n\n\nGraphical check of model assumptions\n\n\n\n\n\n\n\n17.2.5.2 Inferences\nNow summarise this model. We will pause here for you to think about and discuss a few things: * What can you take from the fixed effect table? * How do you interpret the intercept now that there are other effects in the model? * What would happen if we scaled our fixed covariates differently? Why?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(m_2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: aggression ~ opp_size + block + (1 | ID)\n   Data: unicorns\n\nREML criterion at convergence: 1129.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.79296 -0.64761  0.00155  0.67586  2.71456 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.02478  0.1574  \n Residual             0.58166  0.7627  \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   9.00181    0.03901  79.00000 230.778   &lt;2e-16 ***\nopp_size      1.04562    0.04263 398.00000  24.525   &lt;2e-16 ***\nblock        -0.02179    0.06962 398.00000  -0.313    0.754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) opp_sz\nopp_size 0.000        \nblock    0.000  0.000 \n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry tweaking the fixed part of your model:\n\nWhat happens if you add more fixed effects? Try it!\nCould focal body size also matter? If so, should you rescale before adding it to the model?\nShould you add interactions (e.g.¬†block:opp_size)?\nShould you drop non-significant fixed effects?\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHaving changed the fixed part of your model, do the variance estimates change at all?\n\nIs among-individual variance always estimated as zero regardless of fixed effects?\nIs among-individual variance significant with some fixed effets structures but not others?\n\n\n\n\n17.2.6 What is the repeatability?\nAs a reminder, repeatability is the proportion of variance explained by a random effect and it is estimate as the ratio of the variance associated to a random effect by the total variance, or the sum of the residual variance and the different variance compoentn associated with the random effects. In our first model among-individual variance was zero, so R was zero. If we have a different model of aggression and get a non-zero value of the random effect variance, we can obviously calculate a repeatability estimate (R). So we are all working from the same starting point, let‚Äôs all stick with a common set of fixed effects from here on:\n\nm_3 &lt;- lmer(\n  aggression ~ opp_size + scale(body_size, center = TRUE, scale = TRUE)\n    + scale(assay_rep, scale = FALSE) + block\n    + (1 | ID),\n  data = unicorns\n)\nsummary(m_3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \naggression ~ opp_size + scale(body_size, center = TRUE, scale = TRUE) +  \n    scale(assay_rep, scale = FALSE) + block + (1 | ID)\n   Data: unicorns\n\nREML criterion at convergence: 1136.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.85473 -0.62831  0.02545  0.68998  2.74064 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.02538  0.1593  \n Residual             0.58048  0.7619  \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n                                               Estimate Std. Error        df\n(Intercept)                                     9.00181    0.03907  78.07315\nopp_size                                        1.05141    0.04281 396.99857\nscale(body_size, center = TRUE, scale = TRUE)   0.03310    0.03896  84.21144\nscale(assay_rep, scale = FALSE)                -0.05783    0.04281 396.99857\nblock                                          -0.02166    0.06955 397.00209\n                                              t value Pr(&gt;|t|)    \n(Intercept)                                   230.395   &lt;2e-16 ***\nopp_size                                       24.562   &lt;2e-16 ***\nscale(body_size, center = TRUE, scale = TRUE)   0.850    0.398    \nscale(assay_rep, scale = FALSE)                -1.351    0.177    \nblock                                          -0.311    0.756    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) opp_sz sc=Ts=T s(_s=F\nopp_size     0.000                      \ns(_,c=TRs=T  0.000  0.000               \ns(_,s=FALSE  0.000 -0.100  0.000        \nblock        0.000  0.000  0.002   0.000\n\n\nSo we‚Äôd probably calculate R using the individual and residual variance simply as:\n\n0.02538 / (0.02538 + 0.58048)\n\n[1] 0.04189087\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDo you see where I took the numbers ?\n\n\nWe can use some more fancy coding to extract the estimates and plugged them in a formula to estimate the repeatbility\n\nv_id &lt;- VarCorr(m_3)$ID[1, 1]\nv_r &lt;- attr(VarCorr(m_3), \"sc\")^2\nr_man &lt;- v_id / (v_id + v_r)\nr_man\n\n[1] 0.04188879\n\n\nWhich yields an estimate of approximately R=4%. Strictly speaking we should make clear this a conditional repeatability estimate.\nConditional on what you might ask‚Ä¶ on the fixed effects in your model. So our best estimate of 4% refers to the proportion of variance in aggressiveness not explained by fixed effects that is explained by individual identity. This isn‚Äôt much and still won‚Äôt be significant, but illustrates the point that conditional repeatabilities often have a tendency to go up as people explain more of the residual variance by adding fixed effects. This is fine and proper, but can mislead the unwary reader. It also means that decisions about which fixed effects to include in your model need to be based on how you want to interpret R not just on, for instance, whether fixed effects are deemed significant.\n\n17.2.7 A quick note on uncertainty\nUsing lmer in the üì¶ lme4 üì¶ there isn‚Äôt a really simple way to put some measure of uncertainty (SE or CI) on derived parameters like repeatabilities. This is a bit annoying. Such things are more easily done with other mixed model üì¶ like MCMCglmm and asreml which are a bit more specialist. If you are using lmer for models you want to publish then you could look into the üì¶ rptR (Stoffel et al. 2017). This acts as a ‚Äòwrapper‚Äô for lmer models and adds some nice functionality including options to boostrap confidence intervals. Regardless, of how you do it, if you want to put a repeatability in one of your papers as a key result - it really should be accompanied by a measure of uncertainty just like any other effect size estimate.\nHere I am estimating the repeatability and using bootstrap to estimate a confidence interval and a probability associated with the repeatability with the rptR üì¶. For more information about the use of the package and the theory behind it suggest the excellent paper associated with the package (Stoffel et al. 2017)\n\nr_rpt &lt;- rptGaussian(\n  aggression ~ opp_size + block + (1 | ID),\n  grname = \"ID\", data = unicorns\n)\n\nBootstrap Progress:\n\nr_rpt\n\n\n\nRepeatability estimation using the lmm method \n\nRepeatability for ID\nR  = 0.041\nSE = 0.03\nCI = [0, 0.103]\nP  = 0.0966 [LRT]\n     NA [Permutation]\n\n\n\n17.2.8 An easy way to mess up your mixed models\nWe will try some more advanced mixed models in a moment to explore plasticity in aggressiveness a bit more. First let‚Äôs quickly look for among-individual variance in focal body size. Why not? We have the data handy, everyone says morphological traits are very repeatable and - lets be honest - who wouldn‚Äôt like to see a small P value after striking out with aggressiveness.\nInclude a random effect of ID as before and maybe a fixed effect of block, just to see if the beasties were growing a bit between data collection periods.\n\nlmer_size &lt;- lmer(body_size ~ block + (1 | ID),\n  data = unicorns\n)\n\nSummarise and test the random effect.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(lmer_size)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: body_size ~ block + (1 | ID)\n   Data: unicorns\n\nREML criterion at convergence: 3460.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.80452 -0.71319  0.00718  0.70280  1.81747 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 936.01   30.594  \n Residual              34.32    5.858  \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept) 252.5031     3.4310  79.0000  73.595   &lt;2e-16 ***\nblock        -0.1188     0.5348 399.0000  -0.222    0.824    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nblock 0.000 \n\nranova(lmer_size)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\nbody_size ~ block + (1 | ID)\n         npar  logLik    AIC    LRT Df Pr(&gt;Chisq)    \n&lt;none&gt;      4 -1730.4 3468.7                         \n(1 | ID)    3 -2325.6 4657.1 1190.4  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat might you conclude, and why would this be foolish?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHopefully you spotted the problem here. You have fed in a data set with 6 records per individual (with 2 sets of 3 identical values per unicorns), when you know size was only measured twice in reality. This means you‚Äôd expect to get a (potentially very) upwardly biased estimate of R and a (potentially very) downwardly biased P value when testing among-individual variance.\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow can we do it properly?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can prune the data to the two actual observations per unicorns by just selecting the first assay in each block.\n\nunicorns2 &lt;- unicorns[unicorns$assay_rep == 1, ]\n\nlmer_size2 &lt;- lmer(body_size ~ block + (1 | ID),\n  data = unicorns2\n)\nsummary(lmer_size2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: body_size ~ block + (1 | ID)\n   Data: unicorns2\n\nREML criterion at convergence: 1373.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.54633 -0.56198  0.01319  0.56094  1.42095 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 912.84   30.213  \n Residual              57.78    7.601  \nNumber of obs: 160, groups:  ID, 80\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept) 252.5031     3.4310  79.0000  73.595   &lt;2e-16 ***\nblock        -0.1188     1.2019  79.0000  -0.099    0.922    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr)\nblock 0.000 \n\nranova(lmer_size2)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\nbody_size ~ block + (1 | ID)\n         npar  logLik    AIC    LRT Df Pr(&gt;Chisq)    \n&lt;none&gt;      4 -686.68 1381.3                         \n(1 | ID)    3 -771.93 1549.9 170.51  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSummarise and test your random effect and you‚Äôll see the qualitative conclusions will actually be very similar using the pruned data set. Of course this won‚Äôt generallty but be true, so just be careful. Mixed models are intended to help you model repeated measures data with non-independence, but they won‚Äôt get you out of trouble if you mis-represent the true structure of observations on your dependent variable.\n\n\n\n\n17.2.9 Happy mixed-modelling\n\n\n\n\nThe superb unicorn\n\n\n\n\n\n\n\nBates, D., M. M√§chler, B. Bolker, and S. Walker. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67:1‚Äì48.\n\n\nHadfield, J. D. 2010. MCMC methods for multi-response generalized linear mixed models: The MCMCglmm R package. Journal of Statistical Software 33:1‚Äì22.\n\n\nHouslay, T. M., and A. J. Wilson. 2017. Avoiding the misuse of BLUP in behavioural ecology. Behavioral Ecology 28:948‚Äì952.\n\n\nKuznetsova, A., P. B. Brockhoff, and R. H. B. Christensen. 2017. lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software 82:1‚Äì26.\n\n\nStoffel, M. A., S. Nakagawa, and H. Schielzeth. 2017. rptR: Repeatability estimation and variance decomposition by generalized linear mixed-effects models. Methods in Ecology and Evolution 8:1639???1644.\n\n\nThe VSNi Team. 2023. asreml: Fits linear mixed models using REML.",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Introduction to linear mixed models</span>"
    ]
  },
  {
    "objectID": "52-intro_glmm.html",
    "href": "52-intro_glmm.html",
    "title": "\n18¬† Introduction to GLMM\n",
    "section": "",
    "text": "18.1 Lecture\ntheoretical intro to glmm and introduce DHarma package to evaluate fit of glmm\nDream pet dragon",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to `GLMM`</span>"
    ]
  },
  {
    "objectID": "52-intro_glmm.html#practical",
    "href": "52-intro_glmm.html#practical",
    "title": "\n18¬† Introduction to GLMM\n",
    "section": "\n18.2 Practical",
    "text": "18.2 Practical\nThis is an adapted version largely inspired by the tutorial in (Bolker et al. 2009). Spatial variation in nutrient availability and herbivory is likely to cause population differentiation and maintain genetic diversity in plant populations.Here we measure the extent to which mouse-ear cress (Arabidopsis thaliana)exhibits population and genotypic variation in their responses to these im-portant environmental factors. We are particularly interested in whether these populations exhibit nutrient mediated compensation, where higher nutrient levels allow genotypes to better tolerate herbivory (Banta et al. 2010). We use GLMMs to estimate the effect of nutrient levels, simulated herbivory, and their interaction on fruit production in Arabidopsis thaliana(fixed effects), and the extent to which populations vary in their responses(random effects, or variance components)\n\n18.2.1 Packages and functions\nYou need to download the ‚Äúextra_funs.R‚Äù script for some functions used in the Practical\n\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lattice)\nlibrary(DHARMa)\nsource(\"code/extra_funs.R\")\n\n\n18.2.2 The data set\nIn this data set, the response variable is the number of fruits (i.e.¬†seed capsules) per plant. The number of fruits produced by an individual plant(the experimental unit) was hypothesized to be a function of fixed effects,including nutrient levels (low vs.¬†high), simulated herbivory (none vs.¬†apical meristem damage), region (Sweden, Netherlands, Spain), and interactions among these. Fruit number was also a function of random effects including both the population and individual genotype. Because Arabidopsis is highly selfing, seeds of a single individual served as replicates of that individual.There were also nuisance variables, including the placement of the plant in the greenhouse, and the method used to germinate seeds. These were estimated as fixed effects but interactions were excluded.\n\n\nX observation number (we will use this observation number later, when we are accounting for overdispersion)\n\nreg a factor for region (Netherlands, Spain, Sweden).\n\npopu a factor with a level for each population.\n\ngen a factor with a level for each genotype.\n\nrack a nuisance factor for one of two greenhouse racks.\n\nnutrient a factor with levels for minimal or additional nutrients.\n\namd a factor with levels for no damage or simulated herbivory (apical meristem damage; we will sometimes refer to this as ‚Äúclipping‚Äù)\n\nstatus a nuisance factor for germination method.\n\ntotal.fruits the response; an integer count of the number of fruits per plant.\n\n18.2.3 Specifying fixed and random Effects\nHere we need to select a realistic full model, based on the scientific questions and the data actually at hand. We first load the data set and make sure that each variable is appropriately designated as numeric or factor (i.e.categorical variable).\n\ndat_tf &lt;- read.csv(\"data/Banta_TotalFruits.csv\")\nstr(dat_tf)\n\n'data.frame':   625 obs. of  9 variables:\n $ X           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ reg         : chr  \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ popu        : chr  \"3.NL\" \"3.NL\" \"3.NL\" \"3.NL\" ...\n $ gen         : int  4 4 4 4 4 4 4 4 4 5 ...\n $ rack        : int  2 1 1 2 2 2 2 1 2 1 ...\n $ nutrient    : int  1 1 1 1 8 1 1 1 8 1 ...\n $ amd         : chr  \"clipped\" \"clipped\" \"clipped\" \"clipped\" ...\n $ status      : chr  \"Transplant\" \"Petri.Plate\" \"Normal\" \"Normal\" ...\n $ total.fruits: int  0 0 0 0 0 0 0 3 2 0 ...\n\n\nThe X, gen, rack and nutrient variables are coded as integers, but we want them to be factors. ¬à We use mutate() dplyr üì¶, which operates within the data set, to avoid typing lots of commands like dat_tf$rack &lt;- factor(dat_tf$rack) ¬à At the same time, we reorder the clipping variable so that \"unclipped\" is the reference level (we could also have used relevel(amd,\"unclipped\")).\n\ndat_tf &lt;- mutate(\n  dat_tf,\n  X = factor(X),\n  gen = factor(gen),\n  rack = factor(rack),\n  amd = factor(amd, levels = c(\"unclipped\", \"clipped\")),\n  nutrient = factor(nutrient, label = c(\"Low\", \"High\"))\n)\n\nNow we check replication for each genotype (columns) within each population (rows).\n\n(reptab &lt;- with(dat_tf, table(popu, gen)))\n\n      gen\npopu    4  5  6 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 27 28 30 34 35 36\n  1.SP  0  0  0  0  0 39 26 35  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  1.SW  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 20  0  0  0  0  0\n  2.SW  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18 14  0  0  0\n  3.NL 31 11 13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  5.NL  0  0  0 35 26  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  5.SP  0  0  0  0  0  0  0  0 43 22 12  0  0  0  0  0  0  0  0  0  0  0  0  0\n  6.SP  0  0  0  0  0  0  0  0  0  0  0 13 24 14  0  0  0  0  0  0  0  0  0  0\n  7.SW  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 45 47 45\n  8.SP  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 16 35  0  0  0  0  0  0  0\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExercise: this mode of inspection is OK for this data set but might fail for much larger data sets or for more levels of nesting. See if you can think of some other numerical or graphical methods for inspecting the structure of data sets.\n\nplot(reptab) gives a mosaic plot of the two-way table; examine this, see if you can figure out how to interpret it, and decide whether you think it might be useful\ntry the commands colSums(reptab&gt;0) (and the equivalent for rowSums) and figure out what they are telling you.\nUsing this recipe, how would you compute the range of number of genotypes per treatment combination?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nDo you find the mosaic plot you obtained ugly and super hard to read? Me too üòÜ\n\n\nplot(reptab)\n\n\n\nA truly useless plot no one can understand\n\n\n\n\n\ncolSums() do the sum of all the rows for each columns of a table. So colSums(reptab&gt;0) gives you for each genotype the number of populations (lines) where you have at least 1 observations.\n\n\ncolSums(reptab &gt; 0)\n\n 4  5  6 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 27 28 30 34 35 36 \n 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 \n\nrowSums(reptab &gt; 0)\n\n1.SP 1.SW 2.SW 3.NL 5.NL 5.SP 6.SP 7.SW 8.SP \n   3    2    2    3    2    3    3    3    3 \n\n\n\nYou firts need to create a new table of number of observations per treatment and genotypes\n\n\nreptab2 &lt;- with(dat_tf, table(paste(amd, nutrient, sep = \"_\"), gen))\nrange(reptab2)\n\n[1]  2 13\n\n\n\n\n\nThis reveals that we have only 2‚Äì4 populations per region and 2‚Äì3 genotypes per population. However, we also have 2‚Äì13 replicates per genotype for each treatment combination (four unique treatment combinations: 2 levels of nutrients by 2 levels of simulated herbivory). Thus, even though this was a reasonably large experiment (625 plants), there were a very small number of replicates with which to estimate variance components, and many more potential interactions than our data can support. Therefore, judicious selection of model terms, based on both biology and the data, is warranted. We note that we don‚Äôt really have enough levels per random effect, nor enough replication per unique treatment combination. Therefore, we decide to omit the fixed effect of ‚Äúregion‚Äù, although we recognize that populations in different regions are widely geographically separated.\nHowever, as in all GLMMs where the scale parameter is treated as fixed and deviations from the fixed scale parameter would be identifiable (i.e.¬†Poisson and binomial (N &gt; 1), but not binary, models) we may have to deal with overdispersion.\n\n18.2.4 Look at overall patterns in data\nI usually like to start with a relatively simple overall plot of the data, disregarding the random factors, just to see what‚Äôs going on. For reasons to be discussed below, we choose to look at the data on the log (or log(1 + x) scale. Let‚Äôs plot either box-and-whisker plots (useful summaries) or dot plots (more detailed, good for seeing if we missed anything).\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\nNumber of fruits (log + 1) as a function of treatments\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nGenerate these plots and figure out how they work before continuing. Try conditioning/faceting on population rather than region: for facet_wrap you might want to take out the nrow = 1 specification. If you want try reorder the subplots by overall mean fruit set and/or colour the points according to the region they come from.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\np1 &lt;- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf, geom = \"boxplot\") +\n  facet_wrap(~reg, nrow = 1) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Boxplot\")\np2 &lt;- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf) +\n  facet_wrap(~reg, nrow = 1) +\n  stat_sum() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Dot plot\")\np1 + p2\n\n\n\n\n\n18.2.5 Choose an error distribution\nThe data are non-normal in principle (i.e., count data, so our first guess would be a Poisson distribution). If we transform total fruits with the canonical link function (log), we hope to see relatively homogeneous variances across categories and groups.\nFirst we define a new factor that represents every combination of genotype and treatment (nutrient √ó clipping) treatment, and sort it in order of increasing mean fruit set.\n\ndat_tf &lt;- dat_tf %&gt;%\n  mutate(\n    gna = reorder(interaction(gen, nutrient, amd), total.fruits, mean)\n  )\n\nNow time to plot it\n\nggplot(dat_tf, aes(x = gna, y = log(1 + total.fruits))) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\nBoxplot of total fruits (log + 1) per genotypes and treatments\n\n\n\nWe could also calculate the variance for each genotype √ó treatment combination and provide a statistical summary of these variances. This reveals substantial variation among the sample variances on the transformed data. In addition to heterogeneous variances across groups, Figure 1 reveals many zeroes in groups, and some groups with a mean and variance of zero, further suggesting we need a non-normal error distribution, and perhaps something other than a Poisson distribution.\nWe could calculate Œª(mean) for each genotype √ó treatment combination and provide a statistical summary of each group‚Äôs Œª.\n\ngrp_means &lt;- with(dat_tf, tapply(total.fruits, list(gna), mean))\nsummary(grp_means)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   11.35   23.16   31.86   49.74  122.40 \n\n\nA core property of the Poisson distribution is that the variance is equal to the mean. A simple diagnostic is a plot of the group variances against the group means:\n\nPoisson-distributed data will result in a linear pattern with slope = 1\nas long as the variance is generally greater than the mean, we call the data overdispersed. Overdispersion comes in various forms:\n\na linear mean-variance relationship with Var = œÜ¬µ (a line through the origin) with œÜ &gt; 1 is called a quasi-Poisson pattern (this term describes the mean-variance relationship, not any particular proability distribution); we can implement it statistically via quasilikelihood (Venables and Ripley, 2002) or by using a particular parameterization of the negative binomial distribution (‚ÄúNB1‚Äù inthe terminology of Hardin and Hilbe (2007))\na semi-quadratic pattern, Var = ¬µ(1 + Œ±¬µ) or ¬µ(1 + ¬µ/k), is characteristic of overdispersed data that is driven by underlying heterogeneity among samples, either the negative binomial (gamma-Poisson) or the lognormal-Poisson (Elston et al. 2001)\n\n\n\n\nWe‚Äôve already calculated the group (genotype √ó treatment) means, we calculate the variances in the same way.\n\ngrp_vars &lt;- with(\n  dat_tf,\n  tapply(\n    total.fruits,\n    list(gna), var\n  )\n)\n\nWe can get approximate estimates of the quasi-Poisson (linear) and negative binomial (linear/quadratic) pattern using lm.\n\nlm1 &lt;- lm(grp_vars ~ grp_means - 1) ## `quasi-Poisson' fit\nphi_fit &lt;- coef(lm1)\nlm2 &lt;- lm((grp_vars - grp_means) ~ I(grp_means^2) - 1)\nk_fit &lt;- 1 / coef(lm2)\n\nNow we can plot them.\n\nplot(grp_vars ~ grp_means, xlab = \"group means\", ylab = \"group variances\")\nabline(c(0, 1), lty = 2)\ntext(105, 500, \"Poisson\")\ncurve(phi_fit * x, col = 2, add = TRUE)\n## bquote() is used to substitute numeric values\n## in equations with symbols\ntext(110, 3900,\n  bquote(paste(\"QP: \", sigma^2 == .(round(phi_fit, 1)) * mu)),\n  col = 2\n)\ncurve(x * (1 + x / k_fit), col = 4, add = TRUE)\ntext(104, 7200, paste(\"NB: k=\", round(k_fit, 1), sep = \"\"), col = 4)\nl_fit &lt;- loess(grp_vars ~ grp_means)\nmvec &lt;- 0:120\nlines(mvec, predict(l_fit, mvec), col = 5)\ntext(100, 2500, \"loess\", col = 5)\n\n\n\nGraphical evaluation of distribution to use\n\n\n\nSame with ggplot\n\nggplot(\n  data.frame(grp_means, grp_vars),\n  aes(x = grp_means, y = grp_vars)) +\n  geom_point() +\n  geom_smooth(\n    aes(colour = \"Loess\"), se = FALSE) +\n  geom_smooth(\n    method = \"lm\", formula = y ~ x - 1, se = FALSE,\n    aes(colour = \"Q_Pois\")) +\n  stat_function(\n    fun = function(x) x * (1 + x / k_fit),\n    aes(colour = \"Neg_bin\")\n  ) +\n  geom_abline(\n    aes(intercept = 0, slope = 1, colour = \"Poisson\")) +\n  scale_colour_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\", \"red\")) +\n  scale_fill_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\", \"red\")) +\n  guides(fill = FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nGraphical evaluation of distribution to use with ggplot\n\n\n\n\nThese fits are not rigorous statistical tests ‚Äî they violate a variety of assumptions of linear regression (e.g.¬†constant variance, independence), but they are good enough to give us an initial guess about what distributions we should use.\nExercise\n\ncompare a simple quadratic fit to the data (i.e., without the linear part) with the negative binomial and quasipoisson fits \n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlm3 &lt;- lm(grp_vars ~ I(grp_means)^2 - 1) ## quadratic fit\nquad_fit &lt;- coef(lm3)\n\nggplot(\n  data.frame(grp_means, grp_vars),\n  aes(x = grp_means, y = grp_vars)) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\", formula = y ~ x - 1, se = FALSE,\n    aes(colour = \"Q_Pois\")) +\n  stat_function(\n    fun = function(x) x * (1 + x / k_fit),\n    aes(colour = \"Neg_bin\")\n  ) +\n  geom_smooth(\n    method = \"lm\", formula = y ~ I(x^2) - 1, se = FALSE,\n    aes(colour = \"Quad\")) +\n  scale_colour_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\")) +\n  scale_fill_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\")) +\n  guides(fill = FALSE)\n\n\n\nGraphical evaluation of distribution to use including quadratic effect\n\n\n\n\n\n\n\n18.2.5.1 Plotting the response vs treatments\nJust to avoid surprises\n\nggplot(dat_tf, aes(x = amd, y = log(total.fruits + 1), colour = nutrient)) +\n  geom_point() +\n  ## need to use as.numeric(amd) to get lines\n  stat_summary(aes(x = as.numeric(amd)), fun = mean, geom = \"line\") +\n  theme_bw() +\n  theme(panel.spacing = unit(0, \"lines\")) +\n  facet_wrap(~popu)\n\n\n\nFruit production by treatments by population\n\n\n\n\nggplot(dat_tf, aes(x = amd, y = log(total.fruits + 1), colour = gen)) +\n  geom_point() +\n  stat_summary(aes(x = as.numeric(amd)), fun = mean, geom = \"line\") +\n  theme_bw() +\n  ## label_both adds variable name ('nutrient') to facet labels\n  facet_grid(. ~ nutrient, labeller = label_both)\n\n\n\nFruit production by genotype by treatments\n\n\n\n\n18.2.6 Fitting group-wise GLM\nAnother general starting approach is to fit GLMs to each group of data separately, equivalent to treating the grouping variables as fixed effects. This should result in reasonable variation among treatment effects. We first fit the models, and then examine the coefficients.\n\nglm_lis &lt;- lmList(\n  total.fruits ~ nutrient * amd | gen,\n  data = dat_tf,\n  family = \"poisson\")\nplot.lmList(glm_lis)\n\nLoading required package: reshape2\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nUsing grp as id variables\n\n\n\n\nModel coefficients for GLM fits on each genotype\n\n\n\nThree genotypes (5, 6, 34) have extreme coefficients (Fig. 5). A mixed model assumes that the underlying random effects are normally distributed, although we shouldn‚Äôt take these outliers too seriously at this point ‚Äî we are not actually plotting the random effects, or even estimates of random effects (which are not themselves guaranteed to be normally distributed), but rather separate estimates for each group. Create a plotting function for Q-Q plots of these coefficients to visualize the departure from normality.\n\nqqmath.lmList(glm_lis)\n\nNo id variables; using all as measure variables\n\n\n\n\nQ-Q plots of model coefficients for GLM fits on each genotype\n\n\n\nWe see that these extreme coefficients fall far outside a normal error distribution. We shouldn‚Äôt take these outliers too seriously at this point ‚Äî we are not actually plotting the random effects, or even estimates of random effects, but rather separate estimates for each group. Especially if these groups have relatively small sample sizes, the estimates may eventually be ‚Äúshrunk‚Äù closer to the mean when we do the mixed model. We should nonetheless take care to see if the coefficients for these genotypes from the GLMM are still outliers, and take the same precautions as we usually do for outliers. For example, we can look back at the original data to see if there is something weird about the way those genotypes were collected, or try re-running the analysis without those genotypes to see if the results are robust.\n\n18.2.7 Fitting and evaluating GLMMs\nNow we (try to) build and fit a full model, using glmer in the emoji::emoji(\"pacakage\") lme4. This model has random effects for all genotype and population √ó treatment random effects, and for the nuisance variables for the rack and germination method (status). (Given the mean-variance relationship we saw it‚Äôs pretty clear that we are going to have to proceed eventually to a model with overdispersion, but we fit the Poisson model first for illustration.)\n\nmp1 &lt;- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (amd * nutrient | popu) +\n  (amd * nutrient | gen),\ndata = dat_tf, family = \"poisson\"\n)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0135718 (tol = 0.002, component 1)\n\noverdisp_fun(mp1)\n\n      chisq       ratio           p \n13909.47073    23.25998     0.00000 \n\n\n\nThe overdisp_fun() is described [here] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-for-overdispersioncomputing-overdispersion-factor) on the absolutely fantastic FAQ about GLMMs by Ben Bolker https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\nWe can ignore the model convergence for the moment. This shows that the data are (extremely) over-dispersed, given the model.\nWe can also use the excellent DHARMa üì¶ (Hartig 2022) to evaluate fit of glm and glmm. So instead of using the function overdisp_fun(), we can simply use the function testDispersion().\n\ntestDispersion(mp1)\n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 1.3003, p-value = 0.384\nalternative hypothesis: two.sided\n\n\nAs you can see, DHARMa suggests that there is no overdispersion based on the distribution of residuals from simulated data. We are going to consider that we have overdispersion and adjust the model accordingly.\nNow we add the observation-level random effect to the model to account for overdispersion (Elston et al. 2001).\n\nmp2 &lt;- update(mp1, . ~ . + (1 | X))\n\nWarning in (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, :\nfailure to converge in 10000 evaluations\n\n\nWarning in optwrap(optimizer, devfun, start, rho$lower, control = control, :\nconvergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.173075 (tol = 0.002, component 1)\n\n\nThe model takes much longer to fit (and gives warnings). We look just at the variance components. In particular, if we look at the correlation matrix among the genotype random effects, we see a perfect correlation.\n\nattr(VarCorr(mp2)$gen, \"correlation\")\n\n                        (Intercept) amdclipped nutrientHigh\n(Intercept)               1.0000000 -0.9979733   -0.9861333\namdclipped               -0.9979733  1.0000000    0.9883406\nnutrientHigh             -0.9861333  0.9883406    1.0000000\namdclipped:nutrientHigh   0.8199582 -0.8326069   -0.9033829\n                        amdclipped:nutrientHigh\n(Intercept)                           0.8199582\namdclipped                           -0.8326069\nnutrientHigh                         -0.9033829\namdclipped:nutrientHigh               1.0000000\n\n\nWe‚Äôll try getting rid of the correlations between clipping (amd) and nutrients, using amd+nutrient instead of amd*nutrient in the random effects specification (here it seems easier to re-do the model rather than using update to add and subtract terms).\n\nmp3 &lt;- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (amd + nutrient | popu) +\n  (amd + nutrient | gen) + (1 | X),\ndata = dat_tf, family = \"poisson\"\n)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.225224 (tol = 0.002, component 1)\n\nattr(VarCorr(mp3)$gen, \"correlation\")\n\n             (Intercept) amdclipped nutrientHigh\n(Intercept)    1.0000000 -0.9118731   -0.9966458\namdclipped    -0.9118731  1.0000000    0.9123919\nnutrientHigh  -0.9966458  0.9123919    1.0000000\n\nattr(VarCorr(mp3)$popu, \"correlation\")\n\n             (Intercept) amdclipped nutrientHigh\n(Intercept)    1.0000000  0.9947027    0.9969663\namdclipped     0.9947027  1.0000000    0.9909861\nnutrientHigh   0.9969663  0.9909861    1.0000000\n\n\nUnfortunately, we still have perfect correlations among the random effects terms. For some models (e.g.¬†random-slope models), it is possible to fit random effects models in such a way that the correlation between the different parameters (intercept and slope in the case of random-slope models) is constrained to be zero, by fitting a model like (1|f)+(0+x|f); unfortunately, because of the way lme4 is set up, this is considerably more difficult with categorical predictors (factors).\nWe have to reduce the model further in some way in order not to overfit (i.e., in order to not have perfect ¬±1 correlations among random effects). It looks like we can‚Äôt allow both nutrients and clipping in the random effect model at either the population or the genotype level. However, it‚Äôs hard to know whether we should proceed with amd or nutrient, both, or neither in the model.\nA convenient way to proceed if we are going to try fitting several different combinations of random effects is to fit the model with all the fixed effects but only observation-level random effects, and then to use update to add various components to it.\n\nmp_obs &lt;- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (1 | X),\ndata = dat_tf, family = \"poisson\"\n)\n\nNow, for example, update(mp_obs,.~.+(1|gen)+(amd|popu)) fits the model with intercept random effects at the genotype level and variation in clipping effects across populations.\n\n\n\n\n\n\nExercise\n\n\n\nExercise using update, fit the models with\n\nclipping variation at both genotype and population levels;\nnutrient variation at both genotype and populations; convince yourself that trying to fit variation in either clipping or nutrients leads to overfitting (perfect correlations).\nFit the model with only intercept variation at the population and genotype levels, saving it as mp4; show that there is non-zero variance estimated\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\nmpcli &lt;- update(mp_obs, . ~ . + (amd | gen) + (amd | popu))\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0833404 (tol = 0.002, component 1)\n\nVarCorr(mpcli)\n\n Groups Name        Std.Dev. Corr  \n X      (Intercept) 1.431394       \n gen    (Intercept) 0.293549       \n        amdclipped  0.032813 -0.944\n popu   (Intercept) 0.750993       \n        amdclipped  0.125908 0.998 \n\n\n\n\n\n\nmpnut &lt;- update(mp_obs, . ~ . + (nutrient | gen) + (nutrient | popu))\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0161168 (tol = 0.002, component 1)\n\nVarCorr(mpnut)\n\n Groups Name         Std.Dev. Corr  \n X      (Intercept)  1.41918        \n gen    (Intercept)  0.47719        \n        nutrientHigh 0.32402  -1.000\n popu   (Intercept)  0.74716        \n        nutrientHigh 0.12001  1.000 \n\n\n\n\n\n\nmp4 &lt;- update(mp_obs, . ~ . + (1 | gen) + (1 | popu))\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0212496 (tol = 0.002, component 1)\n\nVarCorr(mp4)\n\n Groups Name        Std.Dev.\n X      (Intercept) 1.43199 \n gen    (Intercept) 0.28669 \n popu   (Intercept) 0.80575 \n\n\n\n\n\nIn other words, while it‚Äôs biologically plausible that there is some variation in the nutrient or clipping effect at the genotype or population levels, with this modeling approach we really don‚Äôt have enough data to speak confidently about these effects. Let‚Äôs check that mp4 no longer incorporates overdispersion (the observationlevel random effect should have taken care of it):\n\noverdisp_fun(mp4)\n\n      chisq       ratio           p \n177.3714518   0.2884089   1.0000000 \n\n\nUsing the DHARMa üì¶, we will also check the model. To do so we first need to simulate some data and get the scaled residuals following the DHARMa notation. Then we can check the distributional properties of the scaled residuals and see if they follow the classic assumption using the different functions provided.\n\nscaled_res &lt;- simulateResiduals(mp4)\nplot(scaled_res)\n\n\n\n\n\n\ntestZeroInflation(mp4, plot = TRUE)\n\n\n\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.9823, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n# note about overdispersion\nsum(dat_tf$total.fruits == 0)\n\n[1] 126\n\na &lt;- predict(mp4, type = \"response\")\nb &lt;- rep(0, 500)\nfor (j in 1:500) {\n  b[j] &lt;- sum(sapply(seq(nrow(dat_tf)), function(i) rpois(1, a[i])) == 0)\n}\nhist(b)\n\n\n\n\n\n\n\n\n18.2.8 Inference\n\n18.2.8.1 Random effects\nglmer (lmer) does not return information about the standard errors or confidence intervals of the variance components.\n\nVarCorr(mp4)\n\n Groups Name        Std.Dev.\n X      (Intercept) 1.43199 \n gen    (Intercept) 0.28669 \n popu   (Intercept) 0.80575 \n\n\n\n18.2.8.1.1 Testing for random Effects\nIf we want to test the significance of the random effects we can fit reduced models and run likelihood ratio tests via anova, keeping in mind that in this case (testing a null hypothesis of zero variance, where the parameter is on the boundary of its feasible region) the reported p value is approximately twice what it should be.\n\nmp4v1 &lt;- update(mp_obs, . ~ . + (1 | popu)) ## popu only (drop gen)\nmp4v2 &lt;- update(mp_obs, . ~ . + (1 | gen)) ## gen only (drop popu)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 2 negative eigenvalues\n\nanova(mp4, mp4v1)\n\nData: dat_tf\nModels:\nmp4v1: total.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | popu) + nutrient:amd\nmp4: total.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | gen) + (1 | popu) + nutrient:amd\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)  \nmp4v1    9 5017.4 5057.4 -2499.7   4999.4                       \nmp4     10 5015.4 5059.8 -2497.7   4995.4 4.0631  1    0.04383 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mp4, mp4v2)\n\nData: dat_tf\nModels:\nmp4v2: total.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | gen) + nutrient:amd\nmp4: total.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | gen) + (1 | popu) + nutrient:amd\n      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmp4v2    9 5031.6 5071.5 -2506.8   5013.6                         \nmp4     10 5015.4 5059.8 -2497.7   4995.4 18.212  1  1.977e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor various forms of linear mixed models, the RLRsim package can do efficient simulation-based hypothesis testing of variance components ‚Äî un- fortunately, that doesn‚Äôt include GLMMs. If we are sufficiently patient we can do hypothesis testing via brute-force parametric bootstrapping where we repeatedly simulate data from the reduced (null) model, fit both the re- duced and full models to the simulated data, and compute the distribution of the deviance (change in -2 log likelihood). The code below took about half an hour on a reasonably modern desktop computer.\n\nsimdev &lt;- function() {\n  newdat &lt;- simulate(mp4v1)\n  reduced &lt;- lme4::refit(mp4v1, newdat)\n  full &lt;- lme4::refit(mp4, newdat)\n  2 * (c(logLik(full) - logLik(reduced)))\n}\n\nset.seed(101)\nnulldist0 &lt;- replicate(2, simdev())\n## zero spurious (small) negative values\nnulldist[nulldist &lt; 0 & abs(nulldist) &lt; 1e-5] &lt;- 0\nobsdev &lt;- 2 * c(logLik(mp4) - logLik(mp4v1))\n\n\nmean(c(nulldist, obsdev) &gt;= obsdev)\n\n[1] 0.01492537\n\n\nThe true p-value is actually closer to 0.05 than 0.02. In other words, here the deviations from the original statistical model from that for which the original ‚Äúp value is inflated by 2‚Äù rule of thumb was derived ‚Äî fitting a GLMM instead of a LMM, and using a moderate-sized rather than an arbitrarily large (asymptotic) data set ‚Äî have made the likelihood ratio test liberal (increased type I error) rather than conservative (decreased type I error).\nWe can also inspect the random effects estimates themselves (in proper statistical jargon, these might be considered ‚Äúpredictions‚Äù rather than ‚Äúestimates‚Äù (Robinson, 1991)). We use the built-in dotplot method for the random effects extracted from glmer fits (i.e.¬†ranef(model,condVar=TRUE)), which returns a list of plots, one for each random effect level in the model.\n\nr1 &lt;- as.data.frame(ranef(mp4, condVar = TRUE, whichel = c(\"gen\", \"popu\")))\np1 &lt;- ggplot(subset(r1, grpvar == \"gen\"), aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\np2 &lt;- ggplot(subset(r1, grpvar == \"popu\"), aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\np1 + p2\n\n\n\nDistribution of BLUPs for genotypes and populations\n\n\n\nAs expected from the similarity of the variance estimates, the population-level estimates (the only shared component) do not differ much between the two models. There is a hint of regional differentiation ‚Äî the Spanish populations have higher fruit sets than the Swedish and Dutch populations. Genotype 34 again looks a little bit unusual.\n\n18.2.8.2 Fixed effects\nNow we want to do inference on the fixed effects. We use the drop1 func- tion to assess both the AIC difference and the likelihood ratio test between models. (In glmm_funs.R we define a convenience function dfun to convert the AIC tables returned by drop1 (which we will create momentarily) into ‚àÜAIC tables.) Although the likelihood ratio test (and the AIC) are asymptotic tests, comparing fits between full and reduced models is still more accurate than the Wald (curvature-based) tests shown in the summary tables for glmer fits.\n\n(dd_aic &lt;- dfun(drop1(mp4)))\n\nSingle term deletions\n\nModel:\ntotal.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | \n    gen) + (1 | popu) + nutrient:amd\n             npar   dAIC\n&lt;none&gt;             0.000\nrack            1 55.082\nstatus          2  1.611\nnutrient:amd    1  1.443\n\n(dd_lrt &lt;- drop1(mp4, test = \"Chisq\"))\n\nSingle term deletions\n\nModel:\ntotal.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | \n    gen) + (1 | popu) + nutrient:amd\n             npar    AIC    LRT   Pr(Chi)    \n&lt;none&gt;            5015.4                     \nrack            1 5070.5 57.082 4.181e-14 ***\nstatus          2 5017.0  5.611   0.06046 .  \nnutrient:amd    1 5016.8  3.443   0.06352 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn the basis of these comparisons, there appears to be a very strong effect of rack and weak effects of status and of the interaction term. Dropping the nutrient:amd interaction gives a (slightly) increased AIC (‚àÜAIC = 1.4), so the full model has the best expected predictive capability (by a small margin). On the other hand, the p-value is slightly above 0.05 (p = 0.06). At this point we remove the non-significant interaction term so we can test the main effects. (We don‚Äôt worry about removing status because it measures an aspect of experimental design that we want to leave in the model whether it is significant or not.) Once we have fitted the reduced model, we can run the LRT via anova.\n\nmp5 &lt;- update(mp4, . ~ . - amd:nutrient)\nanova(mp5, mp4)\n\nData: dat_tf\nModels:\nmp5: total.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | gen) + (1 | popu)\nmp4: total.fruits ~ nutrient + amd + rack + status + (1 | X) + (1 | gen) + (1 | popu) + nutrient:amd\n    npar    AIC    BIC  logLik deviance Chisq Df Pr(&gt;Chisq)  \nmp5    9 5016.8 5056.8 -2499.4   4998.8                      \nmp4   10 5015.4 5059.8 -2497.7   4995.4 3.443  1    0.06352 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nExercise Test now the reduced model.\nIn the reduced model, we find that both nutrients and clipping have strong effects, whether measured by AIC or LRT. If we wanted to be still more careful about our interpretation, we would try to relax the asymptotic assumption. In classical linear models, we would do this by doing F tests with the appropriate denominator degrees of freedom. In ‚Äúmodern‚Äù mixed model approaches, we might try to use denominator-degree-of-freedom approximations such as the Kenward-Roger (despite the controversy over these approximations, they are actually available in lmerTest, but they do not apply to GLMMs. We can use a parametric bootstrap comparison between nested models to test fixed effects, as we did above for random effects, with the caveat that is computationally slow.\nIn addition, we can check the normality of the random effects and find they are reasonable (Fig. 10).\n\nr5 &lt;- as.data.frame(ranef(mp5))\nggplot(data = r5, aes(sample = condval)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~ grpvar) +\n  theme_classic()\n\n\n\nQ-Q plot of BLUPs from model mp5\n\n\n\nChecking everything with DHARMa also\n\nscaled_res &lt;- simulateResiduals(mp5)\nplot(scaled_res)\n\n\n\n\n\n\ntestZeroInflation(mp5, plot = TRUE)\n\n\n\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.9918, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nIt is better than before but not perfect. I think this is completely OK and that it will extremely rarely be perfect. You need to learn what is acceptable (by that I mean you find acceptable) and be happy to justify and discuss your decisions.\n\n18.2.9 Conclusions\nOur final model includes fixed effects of nutrients and clipping, as well as the nuisance variables rack and status; observation-level random effects to ac- count for overdispersion; and variation in overall fruit set at the population and genotype levels. However, we don‚Äôt (apparently) have quite enough in- formation to estimate the variation in clipping and nutrient effects, or their interaction, at the genotype or population levels. There is a strong overall positive effect of nutrients and a slightly weaker negative effect of clipping. The interaction between clipping and nutrients is only weakly supported (i.e.¬†the p-value is not very small), but it is positive and about the same magnitude as the clipping effect, which is consistent with the statement that ‚Äúnutrients cancel out the effect of herbivory‚Äù.\n\n\n\n\n\n\nExercise\n\n\n\nExercise\n\nRe-do the analysis with region as a fixed effect.\nRe-do the analysis with a one-way layout as suggested above\n\n\n\n\n18.2.10 Happy generalized mixed-modelling\n\n\n\n\nA GLMM character\n\n\n\n\n\n\n\nBanta, J. A., M. H. H. Stevens, and M. Pigliucci. 2010. A comprehensive test of the ‚Äúlimiting resources‚Äù framework applied to plant tolerance to apical meristem damage. Oikos 119:359‚Äì369.\n\n\nBolker, B. M., M. E. Brooks, C. J. Clark, S. W. Geange, J. R. Poulsen, M. H. H. Stevens, and J.-S. S. White. 2009. Generalized linear mixed models: A practical guide for ecology and evolution. Trends in Ecology and Evolution 24:127‚Äì135.\n\n\nElston, D. A., R. Moss, T. Boulinier, C. Arrowsmith, and X. Lambin. 2001. Analysis of aggregation, a worked example: Numbers of ticks on red grouse chicks. Parasitology 122:563‚Äì569.\n\n\nHartig, F. 2022. DHARMa: Residual diagnostics for hierarchical (multi-level / mixed) regression models.",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Introduction to `GLMM`</span>"
    ]
  },
  {
    "objectID": "53-randreg.html",
    "href": "53-randreg.html",
    "title": "\n19¬† Random regression and character state approaches\n",
    "section": "",
    "text": "19.1 Lecture\nAnd here there would be dragons\nDream pet dragon",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Random regression and character state approaches</span>"
    ]
  },
  {
    "objectID": "53-randreg.html#practical",
    "href": "53-randreg.html#practical",
    "title": "\n19¬† Random regression and character state approaches\n",
    "section": "\n19.2 Practical",
    "text": "19.2 Practical\nIn this practical, we will revisit our analysis on unicorn aggressivity. Honestly, we can use any other data with repeated measures for this exercise but I just ‚ù§Ô∏è unicorns.\n\n19.2.1 R packages needed\nFirst we load required libraries\n\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(asreml)\nlibrary(MCMCglmm)\nlibrary(bayesplot)\nlibrary(patchwork)\n\n\n19.2.2 Refresher on unicorn aggression\nIn the previous, practical on linear mixed models, we simply explored the differences among individuals in their mean aggression (Intercept), but we assumed that the response to the change in aggression with the opponent size (i.e.¬†plasticity) was the same for all individuals. However, this plastic responses can also vary amon individuals. This is called IxE, or individual by environment interaction. To test if individuals differ in their plasticity we can use a random regression, whcih is simply a mixed-model where we fit both a random intercept and a random slope effect.\nFollowing analysis from the previous pratical, our model of interest using scaled covariate was:\naggression ~ opp_size + body_size_sc + assay_rep_sc + block\n              + (1 | ID)\nWe should start by loading the data and refitting the model using lmer().\n\nunicorns &lt;- read.csv(\"data/unicorns_aggression.csv\")\nunicorns &lt;- unicorns %&gt;%\n  mutate(\n    body_size_sc = scale(body_size),\n    assay_rep_sc = scale(assay_rep, scale = FALSE)\n  )\n\nm_mer &lt;- lmer(\n    aggression ~ opp_size + body_size_sc + assay_rep_sc + block\n      + (1 | ID),\n    data = unicorns\n)\nsummary(m_mer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: aggression ~ opp_size + body_size_sc + assay_rep_sc + block +  \n    (1 | ID)\n   Data: unicorns\n\nREML criterion at convergence: 1136.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.85473 -0.62831  0.02545  0.68998  2.74064 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.02538  0.1593  \n Residual             0.58048  0.7619  \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)   9.00181    0.03907 230.395\nopp_size      1.05141    0.04281  24.562\nbody_size_sc  0.03310    0.03896   0.850\nassay_rep_sc -0.05783    0.04281  -1.351\nblock        -0.02166    0.06955  -0.311\n\nCorrelation of Fixed Effects:\n            (Intr) opp_sz bdy_s_ assy__\nopp_size     0.000                     \nbody_siz_sc  0.000  0.000              \nassay_rp_sc  0.000 -0.100  0.000       \nblock        0.000  0.000  0.002  0.000\n\n\nWe can now plot the predictions for each of our observations and plot for the observed and the fitted data for each individuals. Todo so we will use the augment() function from the üì¶ broom.mixed.\nBelow, we plot the raw data for each individual in one panel, with the fitted slopes in a second panel. Because we have 2 blocks of data, and block is fitted as a fixed effect, for ease of presentation we need to either select only 1 block for representation, take teh avaerage over the block effect or do a more complex graph with the two blocks. Here I have selected only one of the blocks for this plot\n\npred_m_mer &lt;- augment(m_mer) %&gt;%\n  select(ID, block, opp_size, .fitted, aggression) %&gt;%\n  filter(block == -0.5) %&gt;%\n  gather(\n    type, aggression,\n    `.fitted`:aggression\n  )\nggplot(pred_m_mer, aes(x = opp_size, y = aggression, group = ID)) +\n  geom_line(alpha = 0.3) +\n  theme_classic() +\n  facet_grid(. ~ type)\n\n\n\nPredicted (from m_mer) and observed value of aggression as a function of opponent size in unicorns\n\n\n\nThis illustrates the importance of using model predictions to see whether the model actually fits the individual-level data well or not ‚Äî while the diagnostic plots looked fine, and the model captures mean plasticity, here we can see that the model really doesn‚Äôt fit the actual data very well at all.\n\n\n19.2.3 Random regression\n\n19.2.3.1 with lme4\n\n\nrr_mer &lt;- lmer(\n  aggression ~ opp_size + body_size_sc + assay_rep_sc + block\n  + (1 + opp_size | ID),\n  data = unicorns\n)\n\n\npred_rr_mer &lt;- augment(rr_mer) %&gt;%\n  select(ID, block, opp_size, .fitted, aggression) %&gt;%\n  filter(block == -0.5) %&gt;%\n  gather(type,aggression, `.fitted`:aggression)\nggplot(pred_rr_mer, aes(x = opp_size, y = aggression, group = ID)) +\n  geom_line(alpha = 0.3) +\n  theme_classic() +\n  facet_grid(. ~ type)\n\n\n\n\n\n\n\nWe can test the improvement of the model fit using the overloaded anova function in R to perform a likelihood ratio test (LRT):\n\nanova(rr_mer, m_mer, refit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\nm_mer\n7\n1150.477\n1179.693\n-568.2383\n1136.477\nNA\nNA\nNA\n\n\nrr_mer\n9\n1092.356\n1129.920\n-537.1780\n1074.356\n62.1206\n2\n0\n\n\n\n\n\nWe can see here that the LRT uses a chi-square test with 2 degrees of freedom, and indicates that the random slopes model shows a statistically significant improvement in model fit. The 2df are because there are two additional (co)variance terms estimated in the random regression model: a variance term for individual slopes, and the covariance (or correlation) between the slopes and intercepts. Let‚Äôs look at those values, and also the fixed effects parameters, via the model summary:\n\nsummary(rr_mer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: aggression ~ opp_size + body_size_sc + assay_rep_sc + block +  \n    (1 + opp_size | ID)\n   Data: unicorns\n\nREML criterion at convergence: 1074.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.04932 -0.59780 -0.02002  0.59574  2.68010 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n ID       (Intercept) 0.05043  0.2246       \n          opp_size    0.19167  0.4378   0.96\n Residual             0.42816  0.6543       \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)   9.00181    0.03902 230.707\nopp_size      1.05033    0.06123  17.153\nbody_size_sc  0.02725    0.03377   0.807\nassay_rep_sc -0.04702    0.03945  -1.192\nblock        -0.02169    0.05973  -0.363\n\nCorrelation of Fixed Effects:\n            (Intr) opp_sz bdy_s_ assy__\nopp_size     0.495                     \nbody_siz_sc  0.000  0.000              \nassay_rp_sc  0.000 -0.064 -0.006       \nblock        0.000  0.000  0.002  0.000\n\n\n\n19.2.3.2 with asreml\n\n\nunicorns &lt;- unicorns %&gt;%\n  mutate( ID = as.factor(ID))\nrr_asr &lt;- asreml(\n  aggression ~ opp_size + body_size_sc + assay_rep_sc + block,\n  random = ~str(~ ID + ID:opp_size, ~us(2):id(ID)),\n  residual = ~ units,\n  data = unicorns,\n  maxiter = 200\n)\n\nASReml Version 4.2 19/09/2024 20:54:07\n          LogLik        Sigma2     DF     wall\n  1     -109.4261     0.4632316    475   20:54:07\n  2     -105.0501     0.4545934    475   20:54:07\n  3     -101.8142     0.4436619    475   20:54:07\n  4     -100.8141     0.4338731    475   20:54:07\n  5     -100.6827     0.4285963    475   20:54:07\n  6     -100.6821     0.4281695    475   20:54:07\n\n\n\nplot(rr_asr)\n\n\n\n\n\n\n\n\nsummary(rr_asr, coef = TRUE)$coef.fixed\n\n                solution  std error     z.ratio\n(Intercept)   9.00181250 0.03901766 230.7112239\nopp_size      1.05032703 0.06123110  17.1534907\nbody_size_sc  0.02725092 0.03377443   0.8068506\nassay_rep_sc -0.04702032 0.03944594  -1.1920191\nblock        -0.02168725 0.05973354  -0.3630665\n\nwa &lt;- wald(rr_asr, ssType = \"conditional\", denDF = \"numeric\")\n\nASReml Version 4.2 19/09/2024 20:54:08\n          LogLik        Sigma2     DF     wall\n  1     -100.6821     0.4281680    475   20:54:08\n  2     -100.6821     0.4281680    475   20:54:08\n\nattr(wa$Wald, \"heading\") &lt;- NULL\nwa\n\n$Wald\n\n             Df denDF F.inc F.con Margin      Pr\n(Intercept)   1  78.3 65490 53230        0.00000\nopp_size      1  79.5   293   294      A 0.00000\nbody_size_sc  1  84.3     1     1      A 0.42202\nassay_rep_sc  1 387.6     1     1      A 0.23398\nblock         1 318.1     0     0      A 0.71680\n\n$stratumVariances\n                                df  Variance ID+ID:opp_size!us(2)_1:1\nID+ID:opp_size!us(2)_1:1  78.00483 0.4790737                 5.216311\nID+ID:opp_size!us(2)_2:1   0.00000 0.0000000                 0.000000\nID+ID:opp_size!us(2)_2:2  78.94046 1.1937287                 0.000000\nunits!R                  318.05470 0.4281680                 0.000000\n                         ID+ID:opp_size!us(2)_2:1 ID+ID:opp_size!us(2)_2:2\nID+ID:opp_size!us(2)_1:1                -3.301137                0.5221955\nID+ID:opp_size!us(2)_2:1                 0.000000                0.0000000\nID+ID:opp_size!us(2)_2:2                 0.000000                3.9943993\nunits!R                                  0.000000                0.0000000\n                         units!R\nID+ID:opp_size!us(2)_1:1       1\nID+ID:opp_size!us(2)_2:1       1\nID+ID:opp_size!us(2)_2:2       1\nunits!R                        1\n\n\n\nsummary(rr_asr)$varcomp\n\n                          component  std.error   z.ratio bound %ch\nID+ID:opp_size!us(2)_1:1 0.05042932 0.02027564  2.487187     P   0\nID+ID:opp_size!us(2)_2:1 0.09458336 0.02400745  3.939751     P   0\nID+ID:opp_size!us(2)_2:2 0.19165924 0.04832059  3.966409     P   0\nunits!R                  0.42816954 0.03395320 12.610582     P   0\n\n\n\nrio_asr &lt;- asreml(\n  aggression ~ opp_size + body_size_sc + assay_rep_sc + block,\n  random = ~ ID,\n  residual = ~units,\n  data = unicorns,\n  maxiter = 200\n)\n\nASReml Version 4.2 19/09/2024 20:54:08\n          LogLik        Sigma2     DF     wall\n  1     -132.6114     0.5603527    475   20:54:08\n  2     -132.1061     0.5670427    475   20:54:08\n  3     -131.7956     0.5751571    475   20:54:08\n  4     -131.7426     0.5807624    475   20:54:08\n  5     -131.7425     0.5804802    475   20:54:08\n\n\n\npchisq(2 * (rr_asr$loglik - rio_asr$loglik), 2,\n  lower.tail = FALSE\n)\n\n[1] 3.241026e-14\n\n\n\nvpredict(rr_asr, cor_is ~ V2 / (sqrt(V1) * sqrt(V3)))\n\n        Estimate        SE\ncor_is 0.9620736 0.1773965\n\n\n\npred_rr_asr &lt;- as.data.frame(predict(rr_asr,\n  classify = \"opp_size:ID\",\n  levels = list(\n    \"opp_size\" =\n      c(opp_size = -1:1)\n  )\n)$pvals)\n\nASReml Version 4.2 19/09/2024 20:54:08\n          LogLik        Sigma2     DF     wall\n  1     -100.6821     0.4281680    475   20:54:08\n  2     -100.6821     0.4281680    475   20:54:08\n 3     -100.6821     0.4281680    475   20:54:08\n\np_rr &lt;- ggplot(pred_rr_asr, aes(\n  x = opp_size,\n  y = predicted.value,\n  group = ID\n)) +\n  geom_line(alpha = 0.2) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  labs(\n    x = \"Opponent size (SDU)\",\n    y = \"Aggression\"\n  ) +\n  theme_classic()\np_rr\n\n\n\n\n\n\n\n\n19.2.3.3 with MCMCglmm\n\n\nprior_RR &lt;- list(\n  R = list(V = 1, nu = 0.002),\n  G = list(\n    G1 = list(V = diag(2)*0.02, nu = 3,\nalpha.mu = rep(0, 2),\nalpha.V= diag(1000, 2, 2))))\nrr_mcmc &lt;- MCMCglmm(\n  aggression ~ opp_size + assay_rep_sc + body_size_sc + block,\n  random = ~ us(1 + opp_size):ID,\n  rcov = ~ units,\nfamily = \"gaussian\",\nprior = prior_RR,\nnitt=750000,\nburnin=50000,\nthin=350,\nverbose = FALSE,\ndata = unicorns,\npr = TRUE,\nsaveX = TRUE, saveZ = TRUE)\n\n\nomar &lt;- par()\npar(mar = c(4, 2, 1.5, 2))\nplot(rr_mcmc$VCV)\n\n\n\n\n\n\n\n\n\n\n\n\npar(omar)\n\nWarning in par(omar): graphical parameter \"cin\" cannot be set\n\n\nWarning in par(omar): graphical parameter \"cra\" cannot be set\n\n\nWarning in par(omar): graphical parameter \"csi\" cannot be set\n\n\nWarning in par(omar): graphical parameter \"cxy\" cannot be set\n\n\nWarning in par(omar): graphical parameter \"din\" cannot be set\n\n\nWarning in par(omar): graphical parameter \"page\" cannot be set\n\n\n\nposterior.mode(rr_mcmc$VCV[, \"opp_size:opp_size.ID\"]) # mean\n\n     var1 \n0.1730222 \n\nHPDinterval(rr_mcmc$VCV[, \"opp_size:opp_size.ID\"])\n\n        lower    upper\nvar1 0.115247 0.302485\nattr(,\"Probability\")\n[1] 0.95\n\n\n\nrr_cor_mcmc &lt;- rr_mcmc$VCV[, \"opp_size:(Intercept).ID\"] /\n  (sqrt(rr_mcmc$VCV[, \"(Intercept):(Intercept).ID\"]) *\n    sqrt(rr_mcmc$VCV[, \"opp_size:opp_size.ID\"]))\nposterior.mode(rr_cor_mcmc)\n\n     var1 \n0.8138931 \n\nHPDinterval(rr_cor_mcmc)\n\n         lower     upper\nvar1 0.5129586 0.9721811\nattr(,\"Probability\")\n[1] 0.95\n\n\n\ndf_rand &lt;- cbind(unicorns,\n  rr_fit = predict(rr_mcmc, marginal = NULL)\n) %&gt;%\n  select(ID, opp_size, rr_fit, aggression) %&gt;%\n  group_by(ID, opp_size) %&gt;%\n  summarise(\n    rr_fit = mean(rr_fit),\n    aggression = mean(aggression)\n  ) %&gt;%\n  gather(\n    Type, Value,\n    rr_fit:aggression\n  )\n\n`summarise()` has grouped output by 'ID'. You can override using the `.groups`\nargument.\n\n# Plot separate panels for individual lines of each type\nggplot(df_rand, aes(x = opp_size, y = Value, group = ID)) +\n  geom_line(alpha = 0.3) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  theme_classic() +\n  facet_grid(. ~ Type)\n\n\n\n\n\n\n\n\n\n\nVariance estimated from random regression models using 3 different softwares\n\nMethod\nv_int\ncov\nv_sl\nv_r\n\n\n\nlmer\n0.0504347\n0.0945863\n0.1916653\n0.4281625\n\n\nasreml\n0.0504293\n0.0945834\n0.1916592\n0.4281695\n\n\nMCMCglmm\n0.0475577\n0.0718380\n0.1730222\n0.4127275\n\n\n\n\n\n\n19.2.4 Character-State approach\nNeed to pivot to a wider format\n\nunicorns_cs &lt;- unicorns %&gt;%\n  select(ID, body_size, assay_rep, block, aggression, opp_size) %&gt;%\n  mutate(\n    opp_size = recode(as.character(opp_size), \"-1\" = \"s\", \"0\" = \"m\", \"1\" = \"l\")\n  ) %&gt;%\n  dplyr::rename(agg = aggression) %&gt;%\n  pivot_wider(names_from = opp_size, values_from = c(agg, assay_rep)) %&gt;%\n  mutate(\n    body_size_sc = scale(body_size),\n    opp_order = as.factor(paste(assay_rep_s, assay_rep_m, assay_rep_l, sep = \"_\"))\n  )\nstr(unicorns_cs)\n\ntibble [160 √ó 11] (S3: tbl_df/tbl/data.frame)\n $ ID          : Factor w/ 80 levels \"ID_1\",\"ID_10\",..: 1 1 2 2 3 3 4 4 5 5 ...\n $ body_size   : num [1:160] 206 207 283 288 229 ...\n $ block       : num [1:160] -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 ...\n $ agg_s       : num [1:160] 7.02 8.44 7.73 8.08 8.06 8.16 8.16 8.51 7.59 6.67 ...\n $ agg_l       : num [1:160] 10.67 10.51 10.81 10.67 9.77 ...\n $ agg_m       : num [1:160] 10.22 8.95 9.43 9.46 7.63 ...\n $ assay_rep_s : int [1:160] 1 3 2 2 1 1 3 3 1 1 ...\n $ assay_rep_l : int [1:160] 2 2 1 1 2 2 2 1 2 2 ...\n $ assay_rep_m : int [1:160] 3 1 3 3 3 3 1 2 3 3 ...\n $ body_size_sc: num [1:160, 1] -1.504 -1.456 0.988 1.143 -0.76 ...\n  ..- attr(*, \"scaled:center\")= num 253\n  ..- attr(*, \"scaled:scale\")= num 31.1\n $ opp_order   : Factor w/ 6 levels \"1_2_3\",\"1_3_2\",..: 2 5 4 4 2 2 5 6 2 2 ...\n\nhead(unicorns_cs)\n\n# A tibble: 6 √ó 11\n  ID    body_size block agg_s agg_l agg_m assay_rep_s assay_rep_l assay_rep_m\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;       &lt;int&gt;       &lt;int&gt;\n1 ID_1       206.  -0.5  7.02 10.7  10.2            1           2           3\n2 ID_1       207.   0.5  8.44 10.5   8.95           3           2           1\n3 ID_10      283.  -0.5  7.73 10.8   9.43           2           1           3\n4 ID_10      288    0.5  8.08 10.7   9.46           2           1           3\n5 ID_11      229.  -0.5  8.06  9.77  7.63           1           2           3\n6 ID_11      236.   0.5  8.16 10.8   8.23           1           2           3\n# ‚Ñπ 2 more variables: body_size_sc &lt;dbl[,1]&gt;, opp_order &lt;fct&gt;\n\n\n\ncs_asr &lt;- asreml(\n  cbind(agg_s, agg_m, agg_l) ~ trait + trait:body_size_sc +\n    trait:block +\n    trait:opp_order,\n  random =~ ID:us(trait),\n  residual =~ units:us(trait),\n  data = unicorns_cs,\n  maxiter = 200\n)\n\nASReml Version 4.2 19/09/2024 20:56:33\n          LogLik        Sigma2     DF     wall\n  1     -150.1721           1.0    456   20:56:33\n  2     -129.6584           1.0    456   20:56:33\n  3     -110.4540           1.0    456   20:56:33\n  4     -101.8792           1.0    456   20:56:33\n  5     -100.0917           1.0    456   20:56:33\n  6     -100.0545           1.0    456   20:56:33\n  7     -100.0544           1.0    456   20:56:33\n\nplot(residuals(cs_asr) ~ fitted(cs_asr))\n\n\n\n\n\n\nqqnorm(residuals(cs_asr))\nqqline(residuals(cs_asr))\n\n\n\n\n\n\nhist(residuals(cs_asr))\n\n\n\n\n\n\n\n\nsummary(cs_asr, all = T)$coef.fixed\n\nNULL\n\nwa &lt;- wald(cs_asr, ssType = \"conditional\", denDF = \"numeric\")\n\nASReml Version 4.2 19/09/2024 20:56:34\n          LogLik        Sigma2     DF     wall\n  1     -100.0544           1.0    456   20:56:34\n  2     -100.0544           1.0    456   20:56:34\n\nattr(wa$Wald, \"heading\") &lt;- NULL\nwa\n\n$Wald\n\n                   Df denDF   F.inc   F.con Margin      Pr\ntrait               3  73.2 21080.0 21080.0        0.00000\ntrait:body_size_sc  3  86.6     0.4     0.5      B 0.68324\ntrait:block         3  75.2     0.6     0.3      B 0.82418\ntrait:opp_order    15 240.5     1.3     1.3      B 0.23282\n\n$stratumVariances\nNULL\n\n\n\nsummary(cs_asr)$varcomp[, c(\"component\", \"std.error\")]\n\n                                 component  std.error\nID:trait!trait_agg_s:agg_s     0.192959991 0.06321872\nID:trait!trait_agg_m:agg_s    -0.168519644 0.05085583\nID:trait!trait_agg_m:agg_m     0.245594370 0.07096325\nID:trait!trait_agg_l:agg_s    -0.151990204 0.05660748\nID:trait!trait_agg_l:agg_m     0.158418588 0.06374995\nID:trait!trait_agg_l:agg_l     0.312548090 0.09125168\nunits:trait!R                  1.000000000         NA\nunits:trait!trait_agg_s:agg_s  0.318089965 0.05198135\nunits:trait!trait_agg_m:agg_s  0.010362390 0.03695483\nunits:trait!trait_agg_m:agg_m  0.322379911 0.05248291\nunits:trait!trait_agg_l:agg_s -0.009311656 0.04168455\nunits:trait!trait_agg_l:agg_m  0.159240476 0.04569305\nunits:trait!trait_agg_l:agg_l  0.405942147 0.06679700\n\n\n\ncs_idh_asr &lt;- asreml(\n  cbind(agg_s, agg_m, agg_l) ~ trait + trait:body_size_sc +\n    trait:block +\n    trait:opp_order,\n  random = ~ ID:idh(trait),\n  residual = ~ units:us(trait),\n  data = unicorns_cs,\n  maxiter = 200\n)\n\nASReml Version 4.2 19/09/2024 20:56:34\n          LogLik        Sigma2     DF     wall\n  1     -147.0682           1.0    456   20:56:34\n  2     -131.2680           1.0    456   20:56:34\n  3     -116.9080           1.0    456   20:56:34\n  4     -110.9955           1.0    456   20:56:34\n  5     -109.9048           1.0    456   20:56:34\n  6     -109.8659           1.0    456   20:56:34\n  7     -109.8626           1.0    456   20:56:34\n\n\n\npchisq(2 * (cs_asr$loglik - cs_idh_asr$loglik), 3,\n  lower.tail = FALSE\n)\n\n[1] 0.0002038324\n\n\n\nvpredict(cs_asr, cor_S_M ~ V2 / (sqrt(V1) * sqrt(V3)))\n\n          Estimate        SE\ncor_S_M -0.7741189 0.1869789\n\nvpredict(cs_asr, cor_M_L ~ V5 / (sqrt(V3) * sqrt(V6)))\n\n         Estimate        SE\ncor_M_L 0.5717926 0.1469504\n\nvpredict(cs_asr, cor_S_L ~ V4 / (sqrt(V1) * sqrt(V6)))\n\n          Estimate        SE\ncor_S_L -0.6189044 0.1912133\n\n\n\nvpredict(cs_asr, prop_S ~ V1 / (V1 + V8))\n\n        Estimate         SE\nprop_S 0.3775756 0.09950306\n\nvpredict(cs_asr, prop_M ~ V3 / (V3 + V10))\n\n       Estimate        SE\nprop_M 0.432404 0.0934477\n\nvpredict(cs_asr, prop_L ~ V6 / (V6 + V13))\n\n        Estimate         SE\nprop_L 0.4350067 0.09498512\n\n\n\ninit_CS_cor1_tri &lt;- c(\n  0.999,\n  0.999, 0.999,\n  1, 1, 1\n)\nnames(init_CS_cor1_tri) &lt;- c(\n  \"F\",\n  \"F\", \"F\",\n  \"U\", \"U\", \"U\"\n)\ncs_asr_cor1_tri &lt;- asreml(\n  cbind(agg_s, agg_m, agg_l) ~ trait + trait:body_size_sc +\n    trait:block +\n    trait:opp_order,\n  random = ~ ID:corgh(trait, init = init_CS_cor1_tri),\nresidual = ~ units:us(trait),\ndata = unicorns_cs,\nmaxiter = 500\n)\n\nASReml Version 4.2 19/09/2024 20:56:34\n          LogLik        Sigma2     DF     wall\n  1     -228.0158           1.0    456   20:56:34  (  3 restrained)\n  2     -150.0138           1.0    456   20:56:34\n  3     -129.5803           1.0    456   20:56:34\n  4     -119.9924           1.0    456   20:56:34  (  1 restrained)\n  5     -116.9067           1.0    456   20:56:34  (  1 restrained)\n  6     -115.7721           1.0    456   20:56:34\n  7     -115.6466           1.0    456   20:56:34\n  8     -115.5882           1.0    456   20:56:34\n  9     -115.5334           1.0    456   20:56:34\n 10     -115.4795           1.0    456   20:56:34\n 11     -115.4273           1.0    456   20:56:34\n 12     -115.3777           1.0    456   20:56:34\n 13     -115.3314           1.0    456   20:56:34\n 14     -115.2892           1.0    456   20:56:34\n 15     -115.2511           1.0    456   20:56:34\n 16     -115.2174           1.0    456   20:56:34\n 17     -115.1879           1.0    456   20:56:34\n 18     -115.1624           1.0    456   20:56:34\n 19     -115.1406           1.0    456   20:56:34\n 20     -115.1221           1.0    456   20:56:34\n 21     -115.1065           1.0    456   20:56:34\n 22     -115.0934           1.0    456   20:56:34\n 23     -115.0825           1.0    456   20:56:34\n 24     -115.0731           1.0    456   20:56:34  (  1 restrained)\n 25     -115.0640           1.0    456   20:56:34\n 26     -115.0637           1.0    456   20:56:34\n\npchisq(2 * (cs_asr$loglik - cs_asr_cor1_tri$loglik),\n  3,\n  lower.tail = FALSE\n)\n\n[1] 1.367792e-06\n\n\n\ndf_CS_pred &lt;- as.data.frame(predict(cs_asr,\n  classify = \"trait:ID\"\n)$pvals)\n\nASReml Version 4.2 19/09/2024 20:56:34\n          LogLik        Sigma2     DF     wall\n  1     -100.0544           1.0    456   20:56:35\n  2     -100.0544           1.0    456   20:56:35\n 3     -100.0544           1.0    456   20:56:35\n\n# Add numeric variable for easier plotting\n# of opponent size\ndf_CS_pred &lt;- df_CS_pred %&gt;%\n  mutate(sizeNum = ifelse(trait == \"agg_s\", -1,\n    ifelse(trait == \"agg_m\", 0, 1)\n  ))\np_cs &lt;- ggplot(df_CS_pred, aes(\n  x = sizeNum,\n  y = predicted.value,\n  group = ID\n)) +\n  geom_line(alpha = 0.2) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  labs(\n    x = \"Opponent size (SDU)\",\n    y = \"Aggression\"\n  ) +\n  theme_classic()\np_cs\n\n\n\n\n\n\n\n\nunicorns &lt;- arrange(unicorns, opp_size, by_group = ID)\np_obs &lt;- ggplot(unicorns[unicorns$block==-0.5,], aes(x = opp_size, y = aggression, group = ID)) +\n  geom_line(alpha = 0.3) +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  labs(\n    x = \"Opponent size (SDU)\",\n    y = \"Aggression\"\n  ) +\n  ggtitle(\"Observed\") +\n  ylim(5.9, 12) +\n  theme_classic()\n\np_rr &lt;- p_rr + ggtitle(\"Random regression\") + ylim(5.9, 12)\np_cs &lt;- p_cs + ggtitle(\"Character-State\") + ylim(5.9, 12)\np_obs + p_rr + p_cs\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n19.2.5 From random regression to character-state\n\nvar_mat_asr &lt;- function(model, var_names, pos){\n  size &lt;- length(var_names)\n  v_out &lt;- matrix(NA, ncol = size, nrow = size)\n  rownames(v_out) &lt;- var_names\n  colnames(v_out) &lt;- var_names\n  v_out[upper.tri(v_out, diag = TRUE)] &lt;- summary(model)$varcomp[pos, 1]\n  v_out &lt;- forceSymmetric(v_out, uplo = \"U\")\n  as.matrix(v_out)\n}\nv_id_rr &lt;- var_mat_asr(rr_asr, c(\"v_int\", \"v_sl\"), 1:3)\nknitr::kable(v_id_rr, digits = 3)\n\n\n\n\nv_int\nv_sl\n\n\n\nv_int\n0.050\n0.095\n\n\nv_sl\n0.095\n0.192\n\n\n\n\n\n\nv_id_cs &lt;- var_mat_asr(cs_asr, c(\"v_s\", \"v_m\", \"v_l\"), 1:6)\nknitr::kable(v_id_cs, digits = 3)\n\n\n\n\nv_s\nv_m\nv_l\n\n\n\nv_s\n0.193\n-0.169\n-0.152\n\n\nv_m\n-0.169\n0.246\n0.158\n\n\nv_l\n-0.152\n0.158\n0.313\n\n\n\n\n\nWe also need to make a second matrix, let‚Äôs call it Q (no particular reason, pick something else if you want). This is going to contain the values needed to turn an individual‚Äôs intercept (mean) and slope (plasticity) deviations into estimates of environment-specific individual merit in a character state model.\nWhat do we mean by this? Well if an individual i has an intercept deviation of IDint(i) and a slope deviation of IDslp(i) for a given value of the environment opp_size we might be interested in:\nIDi = (1 x IDint(i)) + (opp_size x IDslp(i))\nWe want to look at character states representing the three observed values of opp_size here so\n\nQ &lt;- as.matrix(cbind(c(1, 1, 1),\n                    c(-1, 0, 1)))\n\nThen we can generate our among-individual covariance matrix environment specific aggresiveness, which we can call ID_cs_rr by matrix multiplication:\n\nID_cs_rr&lt;- Q %*% v_id_rr %*%t(Q)    #where t(Q) is the transpose of Q\n                               #and %*% is matrix multiplication\n\nID_cs_rr  #rows and columns correspond to aggressiveness at opp_size=-1,0,1 in that order\n\n            [,1]        [,2]       [,3]\n[1,]  0.05292184 -0.04415404 -0.1412299\n[2,] -0.04415404  0.05042932  0.1450127\n[3,] -0.14122993  0.14501267  0.4312553\n\ncov2cor(ID_cs_rr)   #Converting to a correlation scale\n\n           [,1]       [,2]       [,3]\n[1,]  1.0000000 -0.8546956 -0.9348503\n[2,] -0.8546956  1.0000000  0.9833253\n[3,] -0.9348503  0.9833253  1.0000000\n\ncov2cor(v_id_cs)\n\n           v_s        v_m        v_l\nv_s  1.0000000 -0.7741189 -0.6189044\nv_m -0.7741189  1.0000000  0.5717926\nv_l -0.6189044  0.5717926  1.0000000\n\n\n\n19.2.6 Conclusions\n\n19.2.7 Happy multivariate models\n\n\n\n\nA female blue dragon of the West",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Random regression and character state approaches</span>"
    ]
  },
  {
    "objectID": "54-multivar.html",
    "href": "54-multivar.html",
    "title": "\n20¬† Multivariate mixed models\n",
    "section": "",
    "text": "20.1 Lecture\nAmazing beasties and crazy animals\nDream pet dragon\nadd a comparison of lrt",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Multivariate mixed models</span>"
    ]
  },
  {
    "objectID": "54-multivar.html#practical",
    "href": "54-multivar.html#practical",
    "title": "\n20¬† Multivariate mixed models\n",
    "section": "\n20.2 Practical",
    "text": "20.2 Practical\nIn this practical, we have collected data on the amazing blue dragon of the East that roam the sky at night.\nWe will use two different üì¶ to fit more complex models that are not possible with lmer() from lme4 üì¶ (Bates et al. 2015). We will use:\n\n\nasreml-R which is a commercial software developed by VSNi (The VSNi Team 2023). ASReml fit models using a maximum likelihood approach, is quite flexible and fast.\n\nMCMCglmm which is free and open-source and fit model using a Bayesian approach (Hadfield 2010). It is super flexible and allow to fit a wide diversity of distribution.\n\nThe aims of the practical are to learn:\n\nHow to phrase questions of interest in terms of variances and covariances (or derived correlations or regressions);\nHow to incorporate more advanced model structures, such as:\n\nFixed effects that apply only to a subset of the response traits;\nTraits which are measured a different number of times (e.g., repeated measures of behaviour and a single value of breeding success);\n\n\nHypothesis testing using likelihood ratio tests.\n\n\n20.2.1 R packages needed\nFirst we load required libraries\n\nlibrary(lmerTest)\nlibrary(tidyverse)\nlibrary(asreml)\nlibrary(MCMCglmm)\nlibrary(nadiv)\n\n\n20.2.2 The blue dragon of the East\nFor this practical, we have collected data on the amazing blue dragon of the East that roam the sky at night.\n\n\n\n\nBlue dragon male\n\n\n\nWe tagged all dragons individually when they hatch from their eggs. Here, we concentrate on female dragon that produce a single clucth of eggs per mating seasons. Adult femlae blue dragons need to explore vast amount of land to find a compatible male. We thus hypothesized that maximum flight speed as well as exploration are key traits to determine fitness. We were able to obtain repeated measures of flying speed and exploration on 80 adult females during one mating season and also measure the number of egg layed at the end of the season.\nEach females was capture 4 times during the season and each time we measured the maximum flying speed using a wind tunnel and exploration using a openfield test.\nThe data frame has 6 variables:\n\nID: Individual identity\nassay_rep: the repeat number of the behavioural assay\nmax_speed: maximum flying speed\nexploration:\neggs: measure of reproductive succes measured only once per individual\nbody_size: individual body size measured on the day of the test\n\n\ndf_dragons &lt;- read.csv(\"data/dragons.csv\")\nstr(df_dragons)\n\n'data.frame':   320 obs. of  6 variables:\n $ ID         : chr  \"S_1\" \"S_1\" \"S_1\" \"S_1\" ...\n $ assay_rep  : int  1 2 3 4 1 2 3 4 1 2 ...\n $ max_speed  : num  58.7 57.9 64.3 61.4 65.5 ...\n $ exploration: num  126 125 127 127 125 ...\n $ eggs       : int  39 NA NA NA 56 NA NA NA 51 NA ...\n $ body_size  : num  21.7 21.5 21.3 20.8 25.7 ...\n\n\nTo help with convergence of the model, and also help with parameter interpretation, we will first scale our covariates.\n\ndf_dragons &lt;- df_dragons %&gt;%\n  mutate(\n    body_size_sc = scale(body_size),\n    assay_rep_sc = scale(assay_rep, scale = FALSE)\n  )\n\n\n20.2.3 Multiple univariate models\nWe first use the lme4 üì¶ to determine the proportion of phenotypic variation (adjusted for fixed effects) that is due to differences among individuals, separately for each trait with repeated measures.\n\n20.2.3.1 Flying speed\nOur model includes fixed effects of the assay repeat number (centred) and individual body size (centred and scaled to standard deviation units), as we wish to control for any systematic effects of these variables on individual behaviour. Be aware that controlling variables are at your discretion ‚Äî for example, while we want to characterise among-individual variance in flying speed after controlling for size effects in this study, others may wish to characterise among-individual variance in flying speed without such control. Using techniques shown later in the practical, it would be entirely possible to characterise both among-individual variance in flying speed and in size, and the among-individual covariance between these measurements.\n\nlmer_f &lt;- lmer(max_speed ~ assay_rep_sc + body_size_sc + (1 | ID),\n  data = df_dragons\n)\npar(mfrow = c(1, 3))\nplot(resid(lmer_f, type = \"pearson\") ~ fitted(lmer_f))\nqqnorm(residuals(lmer_f))\nqqline(residuals(lmer_f))\nhist(residuals(lmer_f))\n\n\n\nChecking assumptions of model lmer_f\n\n\nsummary(lmer_f)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: max_speed ~ assay_rep_sc + body_size_sc + (1 | ID)\n   Data: df_dragons\n\nREML criterion at convergence: 1791.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.3645 -0.6496 -0.1154  0.6463  2.6894 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept)  6.951   2.636   \n Residual             11.682   3.418   \nNumber of obs: 320, groups:  ID, 80\n\nFixed effects:\n             Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   63.5344     0.3513  78.0954 180.870   &lt;2e-16 ***\nassay_rep_sc  -0.1519     0.1709 238.9807  -0.889    0.375    \nbody_size_sc   0.4468     0.3445  88.0328   1.297    0.198    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) assy__\nassay_rp_sc  0.000       \nbody_siz_sc  0.000 -0.002\n\n\nHaving examined diagnostic plots of the model fit, we can check the model summary. We are interested in the random effects section of the lme4 model output (specifically the variance component ‚Äî note that the standard deviation here is simply the square root of the variance). Evidence for ‚Äòanimal personality‚Äô (or ‚Äòconsistent among-individual differences in behaviour‚Äô) in the literature is largely taken from the repeatability of behaviorual traits: we can compute this repeatability (also known as the intraclass correlation coefficient) by dividing the variance in the trait due to differences among individuals (\\(V_{ID}\\)) by the total phenotypic variance after accounting for the fixed effects (\\(V_{ID} + V_{residual}\\) ).\n\nrep_flying &lt;- as.data.frame(VarCorr(lmer_f)) %&gt;%\n  select(grp, vcov) %&gt;%\n  spread(grp, vcov) %&gt;%\n  mutate(repeatability = ID / (ID + Residual))\nrep_flying\n\n\n\n\nVariance components and repeatbility for the maximum flying speed of blue dragons\n\nID\nResidual\nrepeatability\n\n\n6.951\n11.682\n0.373\n\n\n\n\nSo we can see that 37.31% of the phenotypic variation in boldness (having controlled for body size and assay repeat number) is due to differences among individuals.\n\n20.2.3.2 Exploration\n\nlmer_e &lt;- lmer(exploration ~ assay_rep_sc + body_size_sc + (1 | ID),\n  data = df_dragons\n)\npar(mfrow = c(1, 3))\nplot(resid(lmer_e, type = \"pearson\") ~ fitted(lmer_e))\nqqnorm(residuals(lmer_e))\nqqline(residuals(lmer_e))\nhist(residuals(lmer_e))\n\n\n\nChecking assumptions of model lmer_e\n\n\nsummary(lmer_e)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: exploration ~ assay_rep_sc + body_size_sc + (1 | ID)\n   Data: df_dragons\n\nREML criterion at convergence: 1691.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.73290 -0.62520  0.01635  0.55523  2.95896 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 3.623    1.903   \n Residual             9.091    3.015   \nNumber of obs: 320, groups:  ID, 80\n\nFixed effects:\n              Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  127.22524    0.27148  78.08871 468.639   &lt;2e-16 ***\nassay_rep_sc  -0.07811    0.15076 238.99943  -0.518    0.605    \nbody_size_sc   0.26114    0.26806  85.68180   0.974    0.333    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) assy__\nassay_rp_sc  0.000       \nbody_siz_sc  0.000 -0.002\n\n\nSo the model looks good and we can see our estimates for both fixed and random effects. We can now estimate the repeatbility of exploration.\n\nrep_expl &lt;- as.data.frame(VarCorr(lmer_e)) %&gt;%\n  select(grp, vcov) %&gt;%\n  spread(grp, vcov) %&gt;%\n  mutate(repeatability = ID / (ID + Residual))\nrep_expl\n\n\n\n\nVariance components and repeatability for exploration behaviour of blue dragons\n\nID\nResidual\nrepeatability\n\n\n3.623\n9.091\n0.285\n\n\n\n\nBoth of traits of interest are repeatable at the among-individual level. So, the remaining question is estimating the relation between these two traits. Are individuals that are consistently faster than average also more exploratory than average (and vice versa)?\n\n20.2.3.3 Correlation using BLUPs\nUsing BLUPs to estimate correlations between traits or to further investigate biological associations can lead to spurious results and anticonservative hypothesis tests and narrow confidence intervals. Hadfield et al. (2010) discuss the problem as well as present some alternative method to avoid the problem using Bayesian methods. However, it is always preferable to use multivariate models when possible.\nWe need to create a data frame that contain the BLUPs from both univariate models.\n\ndf_blups_fe &lt;- merge(\n  as.data.frame(ranef(lmer_f)),\n  as.data.frame(ranef(lmer_e)),\n  by = \"grp\"\n) %&gt;%\n  mutate(\n    speed = condval.x,\n    exploration = condval.y\n  )\n\nWe can now test the correlation among-individual between flying speed and exploration.\n\n(cor_blups &lt;- with(df_blups_fe, cor.test(speed, exploration)))\n\n\n    Pearson's product-moment correlation\n\ndata:  speed and exploration\nt = 3.2131, df = 78, p-value = 0.00191\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1320924 0.5223645\nsample estimates:\n      cor \n0.3418867 \n\nggplot(df_blups_fe, aes(x = exploration, y = speed)) +\n  geom_point() +\n  labs(xlab = \"Exploration (BLUP)\", ylab = \"Flying speed (BLUP)\") +\n  theme_classic()\n\n\n\nRelation between exploration and flying speed using BLUPs from univariate models\n\n\n\nAs you can see, we get a positive correlation with a very small p-value (P = 0.00191), indicating that these traits are involved in a behavioural syndrome. While the correlation itself is fairly weak ($r = 0.342), it appears to be highly significant, and suggests that individuals that are faster than average also tend to be more exploratory than average. However, as discussed in Hadfield et al. (2010) and Houslay and Wilson (2017), using BLUPs in this way leads to anticonservative significance tests. This is because the error inherent in their prediction is not carried forward from the lmer models to the subsequent analysis (in this case, a correlation test). To illustrate this point quickly, below we plot the individual estimates along with their associated standard errors.\n\nggplot(df_blups_fe, aes(x = exploration, y = speed)) +\n  geom_point() +\n  geom_linerange(aes(\n    xmin = exploration - condsd.x,\n    xmax = exploration + condsd.x\n  )) +\n  geom_linerange(aes(\n    ymin = speed - condsd.y,\n    ymax = speed + condsd.y\n  )) +\n  labs(\n    xlab = \"Exploration (BLUP +/- SE)\",\n    ylab = \"Flying speed (BLUP +/- SE)\"\n  ) +\n  theme_classic()\n\n\n\nRelation between exploration and flying speed using BLUPs from univariate models including +/- SE as error bars\n\n\n\n\n20.2.4 Multivariate approach\n\n20.2.4.1 Based on ASRemlR\nThe correct approach for testing the hypothesised relation between speed and exploration uses both response variables in a two-trait (‚Äòbivariate‚Äô) mixed model. This model estimates the among-individual variance for each response variable (and the covariance between them). Separate (co)variances are also fitted for the residual variation. The bivariate model also allows for fixed effects to be fitted on both response variables. We set up our model using the asreml function call, with our bivariate response variable being exploration and flying speed bound together using cbind. You will also note that we scale our response variables, meaning that each is centred at their mean value and standardised to units of 1 standard deviation. This is not essential, but simply makes it easier for the model to be fit. Scaling the response variables also aids our understanding of the output, as both flying speed and exploration are now on the same scale.\nasreml can be a bit specific sometime and random effects should absolutely be factor and not character or integer\n\ndf_dragons &lt;- df_dragons %&gt;%\n  mutate(\n    ID = as.factor(ID),\n    speed_sc = scale(max_speed),\n    exploration_sc = scale(exploration)\n  )\n\nasr_us &lt;- asreml(\n  cbind(speed_sc, exploration_sc) ~ trait +\n    trait:assay_rep_sc + trait:body_size_sc,\n  random = ~ ID:us(trait),\n  residual = ~ units:us(trait),\n  data = df_dragons,\n  maxiter = 100\n)\n\nASReml Version 4.2 19/09/2024 21:01:56\n          LogLik        Sigma2     DF     wall\n  1     -333.1053           1.0    634   21:01:56\n  2     -303.6372           1.0    634   21:01:56\n  3     -274.8492           1.0    634   21:01:56\n  4     -260.2431           1.0    634   21:01:56\n  5     -256.1178           1.0    634   21:01:56\n  6     -255.8906           1.0    634   21:01:56\n  7     -255.8893           1.0    634   21:01:56\n\n\nOn the right hand side of our model formula, we use the trait keyword to specify that this is a multivariate model ‚Äî trait itself tells the model to give us the intercept for each trait. We then interact trait with the fixed effects, assay_rep_sc and body_size_sc, so that we get estimates for the effect of these variables on each of teh 2 traits. The random effects structure starts with the random effects, where we tell the model to fit an unstructured (us) covariance matrix for the grouping variable ID. This means that the variance in exploration due to differences among individuals, the variance in boldness due to differences among individuals, and the covariance between these variances will be estimated. Next, we set a structure for the residual variation (residual), which is also sometimes known as the within-individual variation. As we have repeated measures for both traits at the individual level, we also set an unstructured covariance matrix, which estimates the residual variance for each trait and also allows the residuals to covary across the two traits. Finally, we provide the name of the data frame, and a maximum number of iterations for ASReml to attempt to fit the model. After the model has been fit by ASReml, we can check the fit using the same type of model diagnostic plots as we use for lme4:\n\npar(mfrow = c(1, 3))\nplot(residuals(asr_us) ~ fitted(asr_us))\nqqnorm(residuals(asr_us))\nqqline(residuals(asr_us))\nhist(residuals(asr_us))\n\n\n\nChecking assumptions of model asr_us\n\n\n\nThe summary part of the ASReml model fit contains a large amount of information, so it is best to look only at certain parts of it at a single time. While we are not particularly interested in the fixed effects for current purposes, you can inspect these using the following code to check whether there were any large effects of assay repeat or body size on either trait:\n\nsummary(asr_us, coef = TRUE)$coef.fixed\n\n                                       solution  std error       z.ratio\ntrait_speed_sc                    -7.150609e-17 0.08140684 -8.783795e-16\ntrait_exploration_sc              -2.138998e-16 0.07631479 -2.802861e-15\ntrait_speed_sc:assay_rep_sc       -3.521261e-02 0.03960492 -8.890967e-01\ntrait_exploration_sc:assay_rep_sc -2.195541e-02 0.04238056 -5.180538e-01\ntrait_speed_sc:body_size_sc        1.040579e-01 0.07972962  1.305135e+00\ntrait_exploration_sc:body_size_sc  7.269022e-02 0.07533421  9.649033e-01\n\nwa &lt;- wald(asr_us, ssType = \"conditional\", denDF = \"numeric\")\n\nASReml Version 4.2 19/09/2024 21:01:56\n          LogLik        Sigma2     DF     wall\n  1     -255.8893           1.0    634   21:01:56\n  2     -255.8893           1.0    634   21:01:56\n\nattr(wa$Wald, \"heading\") &lt;- NULL\nwa\n\n$Wald\n\n                   Df denDF  F.inc  F.con Margin      Pr\ntrait               2  77.1 0.0000 0.0000        1.00000\ntrait:assay_rep_sc  2 237.9 0.3955 0.3984      B 0.67184\ntrait:body_size_sc  2  86.6 0.9871 0.9871      B 0.37679\n\n$stratumVariances\nNULL\n\n\nWe can see that there is a separate intercept for both personality traits (no surprise that these are very close to zero, given that we mean-centred and scaled each trait before fitting the model), and an estimate of the effect of assay repeat and body size on both traits. None of these appear to be large effects, so let‚Äôs move on to the more interesting parts ‚Äî the random effects estimates:\n\nsummary(asr_us)$varcomp\n\n                                                 component  std.error   z.ratio\nID:trait!trait_speed_sc:speed_sc                0.37333063 0.08607123  4.337461\nID:trait!trait_exploration_sc:speed_sc          0.08838639 0.06067006  1.456837\nID:trait!trait_exploration_sc:exploration_sc    0.28631012 0.07637247  3.748865\nunits:trait!R                                   1.00000000         NA        NA\nunits:trait!trait_speed_sc:speed_sc             0.62741689 0.05740281 10.930073\nunits:trait!trait_exploration_sc:speed_sc       0.32632113 0.04829175  6.757286\nunits:trait!trait_exploration_sc:exploration_sc 0.71844189 0.06572780 10.930563\n                                                bound %ch\nID:trait!trait_speed_sc:speed_sc                    P   0\nID:trait!trait_exploration_sc:speed_sc              P   0\nID:trait!trait_exploration_sc:exploration_sc        P   0\nunits:trait!R                                       F   0\nunits:trait!trait_speed_sc:speed_sc                 P   0\nunits:trait!trait_exploration_sc:speed_sc           P   0\nunits:trait!trait_exploration_sc:exploration_sc     P   0\n\n\nIn the above summary table, we have the among-individual (co)variances listed first (starting with ID), then the residual (or within-individual) (co)variances (starting with R). You will notice that the variance estimates here are actually close to the lme4 repeatability estimates, because our response variables were scaled to phenotypic standard deviations. We can also find the ‚Äòadjusted repeatability‚Äô (i.e., the repeatability conditional on the fixed effects) for each trait by dividing its among-individual variance estimate by the sum of its among-individual and residual variances. Here, we use the vpredict function to estimate the repeatability and its standard error for each trait, conditional on the effects of assay repeat and body size. For this function, we provide the name of the model object, followed by a name that we want to give the estimate being returned, and a formula for the calculation. Each ‚ÄòV‚Äô term in the formula refers to a variance component, using its position in the model summary shown above.\n\nvpredict(asr_us, rep_speed ~ V1 / (V1 + V5))\n\n           Estimate         SE\nrep_speed 0.3730518 0.06124032\n\nvpredict(asr_us, rep_expl ~ V3 / (V3 + V7))\n\n         Estimate         SE\nrep_expl 0.284956 0.06113539\n\n\nWe can also use this function to calculate the estimate and standard error of the correlation from our model (co)variances. We do this by specifying the formula for the correlation:\n\n(cor_fe &lt;- vpredict(asr_us, cor_expl_speed ~ V2 / (sqrt(V1 * V3))))\n\n                Estimate        SE\ncor_expl_speed 0.2703462 0.1594097\n\n\nIn this case, the estimate is similar (here, slightly lower) than our correlation estimate using BLUPs. However, if we consider confidence intervals as +/- 1.96 SE around the estimate, the lower bound of the confidence interval would actually be -0.0421. With confidence intervals straddling zero, we would conclude that this correlation is likely non-significant. As the use of standard errors in this way is only approximate, we should also test our hypothesis formally using likelihood ratio tests.\n\n20.2.4.1.1 Hypothesis testing\nWe can now test the statistical significance of this correlation directly, by fitting a second model without the among-individual covariance between our two traits, and then using a likelihood ratio test to determine whether the model with the covariance produces a better fit. Here, we use the idh structure for our random effects. This stands for ‚Äòidentity matrix‚Äô (i.e., with 0s on the off-diagonals) with heterogeneous variances (i.e., the variance components for our two response traits are allowed to be different from one another). The rest of the model is identical to the previous version.\n\nasr_idh &lt;- asreml(\n  cbind(speed_sc, exploration_sc) ~ trait +\n    trait:assay_rep_sc + trait:body_size_sc,\n  random = ~ ID:idh(trait),\n  residual = ~ units:us(trait),\n  data = df_dragons,\n  maxiter = 100\n)\n\nASReml Version 4.2 19/09/2024 21:01:56\n          LogLik        Sigma2     DF     wall\n  1     -327.0510           1.0    634   21:01:56\n  2     -299.8739           1.0    634   21:01:56\n  3     -273.6894           1.0    634   21:01:56\n  4     -260.8385           1.0    634   21:01:56\n  5     -257.3307           1.0    634   21:01:56\n  6     -257.1202           1.0    634   21:01:56\n  7     -257.1176           1.0    634   21:01:56\n\n\nThe likelihood ratio test is calculated as twice the difference between model log-likelihoods, on a single degree of freedom (the covariance term):\n\n(p_biv &lt;- pchisq(2 * (asr_us$loglik - asr_idh$loglik),\n  df = 1,\n  lower.tail = FALSE\n))\n\n[1] 0.1170385\n\n\nIn sharp contrast to the highly-significant P-value given by a correlation test using BLUPs, here we find no evidence for a correlation between flying speed and exploration. To better understand why BLUPs produce an anticonservative p-value in comparison to multivariate models, we should plot the correlation estimates and their confidence intervals. The confidence intervals are taken directly from the cor.test function for BLUPs, and for ASReml they are calculated as 1.96 times the standard error from the vpredict function.\n\ndf_cor &lt;- data.frame(\n  Method = c(\"ASReml\", \"BLUPs\"),\n  Correlation = c(as.numeric(cor_fe[1]), cor_blups$estimate),\n  low = c(as.numeric(cor_fe[1] - 1.96 * cor_fe[2]), cor_blups$conf.int[1]),\n  high = c(as.numeric(cor_fe[1] + 1.96 * cor_fe[2]), cor_blups$conf.int[2])\n)\nggplot(df_cor, aes(x = Method, y = Correlation)) +\n  geom_point() +\n  geom_linerange(aes(ymin = low, ymax = high)) +\n  ylim(-1, 1) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  theme_classic()\n\n\n\nCorrelation estimates (with CI) using 2 different methods\n\n\n\nHere we can clearly see that the BLUPs method - having failed to carry through the error around the predictions of individual-level estimates - is anticonservative, with small confidence intervals and a correspondingly small P-value (P = 0.00191). Testing the syndrome directly in a bivariate model that retains all the data, by comparison, enables us to capture the true uncertainty about the estimate of the correlation. This is reflected in the larger confidence intervals and, in this case, the non-significant P-value (P = 0.117).\n\n\n20.2.4.1.2 Conclusions\nTo conclude, then: we found that the correlation between flying speed and exploration tends to be positive among female blue dragon. This correlation is not statistically significant, and thus does not provide strong evidence. However, inappropriate analysis of BLUP extracted from univariate models would lead to a different (erroneous) conclusion.\n\n20.2.4.2 Using MCMCglmm\nIn this section I present the code needed to fit the model and explain only the specific aspect of fittign and evaluating the models with MCMCglmm.\nTo be completed. with more details\nFirst, we need to create a ‚Äòprior‚Äô for our model. We recommend reading up on the use of priors (see the course notes of MCMCglmm Hadfield 2010); briefly, we use a parameter-expanded prior here that should be uninformative for our model. One of the model diagnostic steps that should be used later is to check that the model is robust to multiple prior specifications.\n\nprior_1ex &lt;- list(\n  R = list(V = diag(2), nu = 0.002),\n  G = list(G1 = list(\n    V = diag(2) * 0.02, nu = 3,\n    alpha.mu = rep(0, 2),\n    alpha.V = diag(1000, 2, 2)\n  ))\n)\n\n\nmcmc_us &lt;- MCMCglmm(cbind(speed_sc, exploration_sc) ~ trait - 1 +\n  trait:assay_rep_sc +\n  trait:body_size_sc,\nrandom = ~ us(trait):ID,\nrcov = ~ us(trait):units,\nfamily = c(\"gaussian\", \"gaussian\"),\nprior = prior_1ex,\nnitt = 420000,\nburnin = 20000,\nthin = 100,\nverbose = FALSE,\ndata = df_dragons\n)\n\n\nomar &lt;- par()\npar(mar = c(4, 2, 1.5, 2))\nplot(mcmc_us$VCV[, c(1, 2, 4)])\n\n\n\nMCMC trace and Posterior distribution of the (co)variance estimates of model mcmc_us\n\n\nplot(mcmc_us$VCV[, c(5, 6, 8)])\n\n\n\nMCMC trace and Posterior distribution of the (co)variance estimates of model mcmc_us\n\n\npar(omar)\n\n\nsummary(mcmc_us)\n\n\n Iterations = 20001:419901\n Thinning interval  = 100\n Sample size  = 4000 \n\n DIC: 1596.525 \n\n G-structure:  ~us(trait):ID\n\n                                           post.mean l-95% CI u-95% CI eff.samp\ntraitspeed_sc:traitspeed_sc.ID               0.38634  0.22071   0.5691     4000\ntraitexploration_sc:traitspeed_sc.ID         0.08162 -0.03901   0.1986     4000\ntraitspeed_sc:traitexploration_sc.ID         0.08162 -0.03901   0.1986     4000\ntraitexploration_sc:traitexploration_sc.ID   0.29400  0.14511   0.4575     4000\n\n R-structure:  ~us(trait):units\n\n                                              post.mean l-95% CI u-95% CI\ntraitspeed_sc:traitspeed_sc.units                0.6392   0.5234   0.7553\ntraitexploration_sc:traitspeed_sc.units          0.3337   0.2363   0.4304\ntraitspeed_sc:traitexploration_sc.units          0.3337   0.2363   0.4304\ntraitexploration_sc:traitexploration_sc.units    0.7326   0.6074   0.8673\n                                              eff.samp\ntraitspeed_sc:traitspeed_sc.units                 4358\ntraitexploration_sc:traitspeed_sc.units           4000\ntraitspeed_sc:traitexploration_sc.units           4000\ntraitexploration_sc:traitexploration_sc.units     3666\n\n Location effects: cbind(speed_sc, exploration_sc) ~ trait - 1 + trait:assay_rep_sc + trait:body_size_sc \n\n                                 post.mean  l-95% CI  u-95% CI eff.samp pMCMC\ntraitspeed_sc                     0.004181 -0.154250  0.162628     4461 0.945\ntraitexploration_sc               0.001157 -0.154881  0.151179     4000 1.000\ntraitspeed_sc:assay_rep_sc       -0.036040 -0.115559  0.039994     4000 0.367\ntraitexploration_sc:assay_rep_sc -0.022076 -0.112238  0.057978     4000 0.605\ntraitspeed_sc:body_size_sc        0.103102 -0.053037  0.267543     4398 0.206\ntraitexploration_sc:body_size_sc  0.072509 -0.074982  0.221208     4000 0.337\n\n\n\nmcmc_prop_f &lt;- mcmc_us$VCV[, 1] /\n  (mcmc_us$VCV[, 1] + mcmc_us$VCV[, 5])\nplot(mcmc_prop_f)\n\n\n\nPosterior trace and distribution of the repeatability in flying speed\n\n\n\n\nposterior.mode(mcmc_prop_f)\n\n     var1 \n0.3699824 \n\nHPDinterval(mcmc_prop_f)\n\n         lower     upper\nvar1 0.2539071 0.4908642\nattr(,\"Probability\")\n[1] 0.95\n\n\n\nmcmc_prop_e &lt;- mcmc_us$VCV[, 4] /\n  (mcmc_us$VCV[, 4] + mcmc_us$VCV[, 8])\nplot(mcmc_prop_e)\n\n\n\nPosterior trace and distribution of the repeatbility of exploration\n\n\nposterior.mode(mcmc_prop_e)\n\n    var1 \n0.282816 \n\nHPDinterval(mcmc_prop_e)\n\n         lower     upper\nvar1 0.1618704 0.4049431\nattr(,\"Probability\")\n[1] 0.95\n\n\n\nmcmc_cor_fe &lt;- mcmc_us$VCV[, 2] /\n  sqrt(mcmc_us$VCV[, 1] * mcmc_us$VCV[, 4])\nplot(mcmc_cor_fe)\n\n\n\nPosterior trace and distribution of the correlation between flying speed and exploration\n\n\nposterior.mode(mcmc_cor_fe)\n\n     var1 \n0.2586689 \n\nHPDinterval(mcmc_cor_fe)\n\n           lower     upper\nvar1 -0.08046508 0.5263442\nattr(,\"Probability\")\n[1] 0.95\n\n\n\ndf_cor[3, 1] &lt;- \"MCMCglmm\"\ndf_cor[3, -1] &lt;- c(posterior.mode(mcmc_cor_fe), HPDinterval(mcmc_cor_fe))\nrownames(df_cor) &lt;- NULL\n\nggplot(df_cor, aes(x = Method, y = Correlation)) +\n  geom_point() +\n  geom_linerange(aes(ymin = low, ymax = high)) +\n  ylim(-1, 1) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  theme_classic()\n\n\n\nCorrelation estimates (with CI) using 3 different methods\n\n\n\n\n\n\nCorrelation (with 95% intervals) between flying speed and exploration estimated with 3 different methods\n\nMethod\nCorrelation\nlow\nhigh\n\n\n\nASReml\n0.270\n-0.042\n0.583\n\n\nBLUPs\n0.342\n0.132\n0.522\n\n\nMCMCglmm\n0.259\n-0.080\n0.526\n\n\n\n\n\n\n20.2.5 Happy multivariate models\n\n\n\n\nA female blue dragon of the West\n\n\n\n\n\n\n\nBates, D., M. M√§chler, B. Bolker, and S. Walker. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67:1‚Äì48.\n\n\nHadfield, J. D. 2010. MCMC methods for multi-response generalized linear mixed models: The MCMCglmm R package. Journal of Statistical Software 33:1‚Äì22.\n\n\nHadfield, J. D., A. J. Wilson, D. Garant, B. C. Sheldon, and L. E. Kruuk. 2010. The Misuse of BLUP in Ecology and Evolution. American Naturalist 175:116‚Äì125.\n\n\nHouslay, T. M., and A. J. Wilson. 2017. Avoiding the misuse of BLUP in behavioural ecology. Behavioral Ecology 28:948‚Äì952.\n\n\nThe VSNi Team. 2023. asreml: Fits linear mixed models using REML.",
    "crumbs": [
      "Data & Code",
      "Mixed models",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Multivariate mixed models</span>"
    ]
  },
  {
    "objectID": "81-beyond_p.html",
    "href": "81-beyond_p.html",
    "title": "\n21¬† Beyond P < 0.05\n",
    "section": "",
    "text": "cite a bunch a must read paper on the subject and maybe summarize the big point of Do and Don‚Äôt\n\n\n\n\n\n\n\nFigure¬†21.1: Reflection on the meaning of probabilities in biology",
    "crumbs": [
      "Data & Code",
      "Bayesian approach",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>Beyond *P < 0.05*</span>"
    ]
  },
  {
    "objectID": "82-intro_bayesian.html",
    "href": "82-intro_bayesian.html",
    "title": "\n22¬† Introduction to Bayesian Inference\n",
    "section": "",
    "text": "22.1 Lecture\nAmazing beasties and crazy animals\nDream pet dragon\nneed to add stuff here",
    "crumbs": [
      "Data & Code",
      "Bayesian approach",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Introduction to Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "82-intro_bayesian.html#lecture",
    "href": "82-intro_bayesian.html#lecture",
    "title": "\n22¬† Introduction to Bayesian Inference\n",
    "section": "",
    "text": "22.1.1 Bayes‚Äô theorem\nFirst, let‚Äôs review the theorem. Mathematically, it says how to convert one conditional probability into another one.\n\\[ P(B \\mid A) = \\frac{ P(A \\mid B) * P(B)}{P(A)} \\]\nThe formula becomes more interesting in the context of statistical modeling. We have some model that describes a data-generating process and we have some observed data, but we want to estimate some unknown model parameters. In that case, the formula reads like:\n\\[ P(\\text{hypothesis} \\mid \\text{data}) = \\frac{ P(\\text{data} \\mid \\text{hypothesis}) * P(\\text{hypothesis})}{P(\\text{data})} \\]\nThese terms have conventional names:\n\\[ \\text{posterior} = \\frac{ \\text{likelihood} * \\text{prior}}{\\text{evidence}} \\]\nPrior and posterior describe when information is obtained: what we know pre-data is our prior information, and what we learn post-data is the updated information (‚Äúposterior‚Äù).\nThe likelihood in the equation says how likely the data is given the model parameters. I think of it as fit: How well do the parameters fit the data? Classical regression‚Äôs line of best fit is the maximum likelihood line. The likelihood also encompasses the data-generating process behind the model. For example, if we assume that the observed data is normally distributed, then we evaluate the likelihood by using the normal probability density function. You don‚Äôt need to know what that last sentence means. What‚Äôs important is that the likelihood contains our built-in assumptions about how the data is distributed.\nThe evidence (sometimes called average likelihood) is hareder to grasp. I am not sure how to describe it in an intuitive way. It‚Äôs there to make sure the math works out so that the posterior probabilities sum to 1. Some presentations of Bayes‚Äô theorem gloss over it and I am not the exception üòÑ. The important thing to note is that the posterior is proportional to the likelihood and prior information.\n\\[\n\\text{posterior information} \\propto\n  \\text{likelihood of data} * \\text{prior information}\n\\]\nSo simply put, you update your prior information in proportion to how well it fits the observed data. So essentially you are doing that on a daily basis for everything except when you ar doing frequentist stats üòÑ.\n\n\n\n\nBayesian Triptych\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA word of encouragement! The prior is an intimidating part of Bayesian statistics. It seems highly subjective, as though we are pulling numbers from thin air, and it can be overwhelming for complex models. But if we are familiar with the kind of data we are modeling, we have prior information. We can have the model simulate new observations using the prior distribution and then plot the hypothetical data. Does anything look wrong or implausible about the simulated data? If so, then we have some prior information that we can include in our model. Note that we do not evaluate the plausibility of the simulated data based on the data we have in hand (the data we want to model); that‚Äôs not\n\n\n\n22.1.2 Intro to MCMC\nWe will now walk through a simple example coded in R to illustrate how an MCMC algorithm works.\nSuppose you are interested in the mean heart rate is of students when asked a question in a stat course. You are not sure what the exact mean value is, but you know the values are normally distributed with a standard deviation of 15. You have observed 5 individuals to have heart rate of 104, 120,160,90,130. You could use MCMC sampling to draw samples from the target distribution. We need to specify:\n\nthe starting value for the chain.\nthe length of the chain. In general, more iterations will give you more accurate output.\n\n\nlibrary(coda)\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nset.seed(170)\nhr_obs &lt;- c(104, 112, 132, 115, 110)\n\nstart_value &lt;- 250\n\nn_iter &lt;- 2500 # define number of iterations\n\npd_mean &lt;- numeric(n_iter) # create vector for sample values\n\npd_mean[1] &lt;- start_value # define starting value\n\nfor (i in 2:n_iter) {\n  proposal &lt;- pd_mean[i - 1] + MASS::mvrnorm(1, 0, 5) # proposal\n  lprop &lt;- sum(dnorm(proposal, hr_obs, 15)) # likelihood of proposed parameter\n  lprev &lt;- sum(dnorm(pd_mean[i - 1], hr_obs, 15))\n  if (lprop / lprev &gt; runif(1)) { # if likelihood of prosposed &gt; likehood previous accept\n    # and if likelihood is lower accept with random noise\n    pd_mean[i] &lt;- proposal\n  } # if true sample the proposal\n  else {\n    (pd_mean[i] &lt;- pd_mean[i - 1])\n  } # if false sample the current value\n}\npd_mean &lt;- as.mcmc(data.frame(mean = pd_mean))\nmcmc_combo(pd_mean, combo = c(\"trace\", \"dens\"))\n\n\n\n\n\n\nsummary(pd_mean)\n\n\nIterations = 1:2500\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 2500 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n      125.8105        32.8672         0.6573        13.3046 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n 75.53 108.03 122.19 136.12 225.46 \n\n\n\nset.seed(170)\nhr_obs &lt;- c(104, 112, 132, 115, 110)\nn_iter &lt;- 2500 # define number of iterations\n\nn_chain &lt;- 3\nstart_value &lt;- c(250, 100, 50)\n\npd_mean &lt;- array(NA, dim = c(n_iter, n_chain, 1), dimnames = list(iter = NULL, chain = NULL, params = \"beta\")) # create vector for sample values\n\nfor (j in seq_len(n_chain)) {\n  pd_mean[1, j, 1] &lt;- start_value[j] # define starting value\n  for (i in 2:n_iter) {\n    proposal &lt;- pd_mean[i - 1, j, 1] + MASS::mvrnorm(1, 0, 5) # proposal\n    if (sum(dnorm(proposal, hr_obs, 15)) # likelihood of proposed parameter\n    / sum(dnorm(pd_mean[i - 1, j, 1], hr_obs, 15)) &gt; runif(1, 0, 1)) {\n      pd_mean[i, j, 1] &lt;- proposal\n    } # if true sample the proposal\n    else {\n      (pd_mean[i, j, 1] &lt;- pd_mean[i - 1, j, 1])\n    } # if false sample the current value\n  }\n}\ncolor_scheme_set(\"mix-blue-red\")\nmcmc_combo(pd_mean, combo = c(\"trace\", \"dens_overlay\"))\n\n\n\n\n\n\nsummary(pd_mean)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  41.65   99.32  109.68  112.71  122.52  250.00 \n\nmcmc_combo(pd_mean, combo = c(\"trace\", \"dens_overlay\"), n_warmup = 500)\n\n\n\n\n\n\npd_burn &lt;- pd_mean[-c(1:500), , , drop = FALSE]\nsummary(pd_burn)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  51.98  100.71  110.38  111.42  122.69  163.58 \n\nmcmc_combo(pd_burn, combo = c(\"trace\", \"dens_overlay\"), iter1 = 501)\n\n\n\n\n\n\n\n\n22.1.3 Inferences\n\n22.1.3.1 Fixed effects\nEasy peazy lemon squeezy just have a look at the posteriro distribution, does it overlap 0 yes or no.\ntalk about mean, median and mode of a distribution as well as credible intervals\n\n22.1.3.2 Random effects\nQuite a bit more harder. because constrained to be positive\n\nInterpreting posterior distribution\nDIC\nWAIC",
    "crumbs": [
      "Data & Code",
      "Bayesian approach",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Introduction to Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "82-intro_bayesian.html#practical",
    "href": "82-intro_bayesian.html#practical",
    "title": "\n22¬† Introduction to Bayesian Inference\n",
    "section": "\n22.2 Practical",
    "text": "22.2 Practical\nIn this practical, we will revisit our analysis on unicorn aggressivity. Honestly, we can use any other data with repeated measures for this exercise but I just love unicorns ‚ù§Ô∏è. However, instead of fittng the model using lmer() from the lmerTest üì¶ (Kuznetsova et al. 2017), we will refit the model using 2 excellent softwares fitting models with a Bayesian approach: MCMCglmm (Hadfield 2010) and brms (B√ºrkner 2021).\n\n22.2.1 R packages needed\nFirst we load required libraries\n\nlibrary(lmerTest)\nlibrary(tidyverse)\nlibrary(rptR)\nlibrary(brms)\nlibrary(MCMCglmm)\nlibrary(bayesplot)\n\n\n22.2.2 A refresher on unicorn ecology\nThe last model on unicorns was:\naggression ~ opp_size + scale(body_size, center = TRUE, scale = TRUE)\n              + scale(assay_rep, scale = FALSE) + block\n              + (1 | ID)\nThose scaled terms are abit a sore for my eyes and way too long if we need to type them multiple times in this practical. So first let‚Äôs recode them. -\n\nunicorns &lt;- read.csv(\"data/unicorns_aggression.csv\")\nunicorns &lt;- unicorns %&gt;%\n  mutate(\n    body_size_sc = scale(body_size),\n    assay_rep_sc = scale(assay_rep, scale = FALSE)\n  )\n\nOk now we can fit the same model by just using:\naggression ~ opp_size + body_size_sc + assay_rep_sc + block\n              + (1 | ID)\nWe can now fit a model using lmer(). Since we want to compare a bit REML and Bayesian aproaches, I am going to wrap the model function in a function called system.time(). This function simply estimate the user and computer time use by the function.\n\nmer_time &lt;- system.time(\n  m_mer &lt;- lmer(\n    aggression ~ opp_size + body_size_sc + assay_rep_sc + block\n      + (1 | ID),\n    data = unicorns\n  )\n)\nmer_time\n\n   user  system elapsed \n  0.065   0.000   0.065 \n\nsummary(m_mer)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: aggression ~ opp_size + body_size_sc + assay_rep_sc + block +  \n    (1 | ID)\n   Data: unicorns\n\nREML criterion at convergence: 1136.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.85473 -0.62831  0.02545  0.68998  2.74064 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.02538  0.1593  \n Residual             0.58048  0.7619  \nNumber of obs: 480, groups:  ID, 80\n\nFixed effects:\n              Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)    9.00181    0.03907  78.07315 230.395   &lt;2e-16 ***\nopp_size       1.05141    0.04281 396.99857  24.562   &lt;2e-16 ***\nbody_size_sc   0.03310    0.03896  84.21144   0.850    0.398    \nassay_rep_sc  -0.05783    0.04281 396.99857  -1.351    0.177    \nblock         -0.02166    0.06955 397.00209  -0.311    0.756    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) opp_sz bdy_s_ assy__\nopp_size     0.000                     \nbody_siz_sc  0.000  0.000              \nassay_rp_sc  0.000 -0.100  0.000       \nblock        0.000  0.000  0.002  0.000\n\n\nOk so it took no time at all to do it and we got our ‚Äúclassic‚Äù results.\n\n22.2.3 MCMCglmm\nWhat makes MCMCglmm so useful and powerful üí™ in ecology and for practical Bayesian people is that:\n\nit is blazing fast ‚è© (for Bayesian analysis) for some models particularly models with structured covariances\nit is fairly intuitive to code\n\nbut it also has some inconvenients:\n\nit is blazing fast for Bayesian analysis meaning it is üêå compared to maximum likelihood approaches\nit has some limitations in terms of functionality, distribution availability and model specifications compared to other Bayesian softwares\nthe priors, oh, the priors üò≠, are a bit tricky to code and understand ü§Ø.\n\n\n22.2.3.1 Fitting the Model\nSo here is how we can code the model in MCMCglmm(). It is fairly similar to lmer() except that the random effects are specified in a different argument.\n\nmcglm_time &lt;- system.time(\n  m_mcmcglmm &lt;- MCMCglmm(\n    aggression ~ opp_size + body_size_sc + assay_rep_sc + block,\n    random = ~ID,\n    data = unicorns\n  )\n)\n\n\n                       MCMC iteration = 0\n\n                       MCMC iteration = 1000\n\n                       MCMC iteration = 2000\n\n                       MCMC iteration = 3000\n\n                       MCMC iteration = 4000\n\n                       MCMC iteration = 5000\n\n                       MCMC iteration = 6000\n\n                       MCMC iteration = 7000\n\n                       MCMC iteration = 8000\n\n                       MCMC iteration = 9000\n\n                       MCMC iteration = 10000\n\n                       MCMC iteration = 11000\n\n                       MCMC iteration = 12000\n\n                       MCMC iteration = 13000\n\nsummary(m_mcmcglmm)\n\n\n Iterations = 3001:12991\n Thinning interval  = 10\n Sample size  = 1000 \n\n DIC: 1128.004 \n\n G-structure:  ~ID\n\n   post.mean  l-95% CI u-95% CI eff.samp\nID  0.003686 9.807e-14   0.0262    45.81\n\n R-structure:  ~units\n\n      post.mean l-95% CI u-95% CI eff.samp\nunits    0.6044   0.5228   0.6819     1000\n\n Location effects: aggression ~ opp_size + body_size_sc + assay_rep_sc + block \n\n             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n(Intercept)    9.00152  8.93150  9.07158     1000 &lt;0.001 ***\nopp_size       1.04940  0.96813  1.12946     1000 &lt;0.001 ***\nbody_size_sc   0.03154 -0.03985  0.09563     1000  0.410    \nassay_rep_sc  -0.05620 -0.13196  0.03546      893  0.184    \nblock         -0.02069 -0.16186  0.11553     1000  0.774    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmcglm_time\n\n   user  system elapsed \n  1.400   0.001   1.411 \n\n\nModel is slow and not good. We need more iteration and maybe even a longer burnin, and honestly maybe better priors.\nWe can still take the time to have a look at the R object output from MCMCglmm(). The 2 main parts we are interrested in are:\n\n\nSol which stand for the model solution and includes the posteriro distribution of the fixed effects\n\nVCV, for the variance covariance estimates, which includes the posterior distribution of all (co)variances estimates for both random effects and residual variance.\n\n\nomar &lt;- par()\npar(mar = c(4, 2, 1.5, 2))\nplot(m_mcmcglmm$Sol)\n\n\n\nPosterior trace and distribution of the parameters in m_mcmcglmm using default settings\n\n\n\n\n\nPosterior trace and distribution of the parameters in m_mcmcglmm using default settings\n\n\nplot(m_mcmcglmm$VCV)\n\n\n\nPosterior trace and distribution of the parameters in m_mcmcglmm using default settings\n\n\npar(omar)\nautocorr.diag(m_mcmcglmm$VCV)\n\n               ID       units\nLag 0   1.0000000  1.00000000\nLag 10  0.8042405 -0.02074155\nLag 50  0.4807583 -0.04264317\nLag 100 0.1951356  0.04422296\nLag 500 0.1254589  0.04401956\n\n\nTalk about autocorrelation, mixing, convergence and priors here\n\nn_samp &lt;- 1000\nthin &lt;- 500\nburnin &lt;- 20000\nmcglm_time &lt;- system.time(\n  m_mcmcglmm &lt;- MCMCglmm(\n    aggression ~ opp_size + body_size_sc + assay_rep_sc + block,\n    random = ~ID,\n    data = unicorns,\n    nitt = n_samp * thin + burnin, thin = thin, burnin = burnin,\n    verbose = FALSE,\n    prior = list(\n      R = list(V = 1, nu = 0.002),\n      G = list(\n        G1 = list(V = 1, nu = 0.002)\n      )\n    )\n  )\n)\nsummary(m_mcmcglmm)\n\n\n Iterations = 20001:519501\n Thinning interval  = 500\n Sample size  = 1000 \n\n DIC: 1126.66 \n\n G-structure:  ~ID\n\n   post.mean  l-95% CI u-95% CI eff.samp\nID   0.01987 0.0002904  0.05458     1000\n\n R-structure:  ~units\n\n      post.mean l-95% CI u-95% CI eff.samp\nunits    0.5917   0.5188   0.6763     1000\n\n Location effects: aggression ~ opp_size + body_size_sc + assay_rep_sc + block \n\n             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n(Intercept)    9.00136  8.92221  9.07383     1000 &lt;0.001 ***\nopp_size       1.05363  0.96382  1.13650     1000 &lt;0.001 ***\nbody_size_sc   0.03373 -0.03781  0.10686     1000  0.396    \nassay_rep_sc  -0.05861 -0.14186  0.02882     1000  0.182    \nblock         -0.02709 -0.16061  0.11441     1000  0.698    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmcglm_time\n\n   user  system elapsed \n 55.712   0.001  55.793 \n\n\nevaluate model here\n\nomar &lt;- par()\npar(mar = c(4, 2, 1.5, 2))\nplot(m_mcmcglmm$Sol)\n\n\n\nPosterior trace and distribution of the paremeters in m_mcmcglmm with better settings\n\n\n\n\n\nPosterior trace and distribution of the paremeters in m_mcmcglmm with better settings\n\n\nplot(m_mcmcglmm$VCV)\n\n\n\nPosterior trace and distribution of the paremeters in m_mcmcglmm with better settings\n\n\npar(omar)\nautocorr.diag(m_mcmcglmm$VCV)\n\n                    ID        units\nLag 0      1.000000000  1.000000000\nLag 500    0.013876043 -0.044235206\nLag 2500   0.026120260 -0.048012241\nLag 5000  -0.049357725  0.021158672\nLag 25000  0.002544256 -0.003722595\n\n\n\n22.2.4 Inferences\n\n22.2.4.1 Fixed effects\nEasy peazy lemon squeezy just have a look at the posterior distribution, does it overlap 0 yes or no.\n\nposterior.mode(m_mcmcglmm$Sol)\n\n (Intercept)     opp_size body_size_sc assay_rep_sc        block \n  9.00632282   1.07353252   0.03500916  -0.04048582  -0.03276275 \n\nHPDinterval(m_mcmcglmm$Sol)\n\n                   lower      upper\n(Intercept)   8.92221005 9.07383400\nopp_size      0.96382086 1.13649873\nbody_size_sc -0.03781276 0.10685606\nassay_rep_sc -0.14185602 0.02882443\nblock        -0.16060691 0.11440706\nattr(,\"Probability\")\n[1] 0.95\n\n\n\n22.2.4.2 Random effects\nQuite a bit more harder. because constrained to be positive\n\nposterior.mode(m_mcmcglmm$VCV)\n\n        ID      units \n0.00096263 0.59129362 \n\nHPDinterval(m_mcmcglmm$VCV)\n\n             lower      upper\nID    0.0002903938 0.05458376\nunits 0.5188238599 0.67634529\nattr(,\"Probability\")\n[1] 0.95\n\n\n\n22.2.5 brms\nbrms is an acronym for Bayesian Regression Models using ‚ÄòStan‚Äô (B√ºrkner 2021). It is a package developed to fit regression models with a Bayesian approach using the amazing stan software (Stan Development Team 2021).\nWhat makes brms so useful and powerful üí™ in ecology is that:\n\nit is really intuitive to code (same syntax as glmer())\nit is incredibly flexible since it is essentially a front end for stan via its rstan interface (Stan Development Team 2024)\n\n\nbut with great powers come great responsability üï∑Ô∏è\n\nbrm_time &lt;- system.time(\n  m_brm &lt;- brm(\n    aggression ~ opp_size + body_size_sc + assay_rep_sc + block\n      + (1 | ID),\n    data = unicorns, iter = 4750, warmup = 1000, thin = 15, cores = 4\n    # refresh = 0\n  )\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\nbrm_time\n\n   user  system elapsed \n 94.069   4.522  83.025 \n\nsummary(m_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: aggression ~ opp_size + body_size_sc + assay_rep_sc + block + (1 | ID) \n   Data: unicorns (Number of observations: 480) \n  Draws: 4 chains, each with iter = 4750; warmup = 1000; thin = 15;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 80) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.14      0.07     0.02     0.27 1.00      967      792\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        9.00      0.04     8.93     9.07 1.00      897      932\nopp_size         1.05      0.04     0.97     1.13 1.00      952      939\nbody_size_sc     0.03      0.04    -0.05     0.11 1.00      879      994\nassay_rep_sc    -0.06      0.05    -0.15     0.03 1.00      867      925\nblock           -0.02      0.07    -0.15     0.12 1.00     1025      995\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.76      0.03     0.71     0.82 1.00     1018     1046\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nmcmc_acf_bar(m_brm, regex_pars = c(\"sd\"))\n\n\n\nAutocorrelation in the chain for variance parameters in model m_brm\n\n\n\n\n22.2.5.1 Hunder the hood\nhave a look at the stan code\n\nstancode(m_brm)\n\n// generated with brms 2.21.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; Kc;  // number of population-level effects after centering\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  real lprior = 0;  // prior contributions to the log posterior\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  lprior += student_t_lpdf(Intercept | 3, 8.9, 2.5);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n];\n    }\n    target += normal_id_glm_lpdf(Y | Xc, mu, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n\n\n\n22.2.5.2 using shiny\n\nlaunch_shinystan(m_brm)\n\n\n\n\n\nShinystan interface\n\n\n\n\n22.2.6 Inferences\n\n22.2.6.1 Fixed effects\n\nsummary(m_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: aggression ~ opp_size + body_size_sc + assay_rep_sc + block + (1 | ID) \n   Data: unicorns (Number of observations: 480) \n  Draws: 4 chains, each with iter = 4750; warmup = 1000; thin = 15;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 80) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.14      0.07     0.02     0.27 1.00      967      792\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        9.00      0.04     8.93     9.07 1.00      897      932\nopp_size         1.05      0.04     0.97     1.13 1.00      952      939\nbody_size_sc     0.03      0.04    -0.05     0.11 1.00      879      994\nassay_rep_sc    -0.06      0.05    -0.15     0.03 1.00      867      925\nblock           -0.02      0.07    -0.15     0.12 1.00     1025      995\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.76      0.03     0.71     0.82 1.00     1018     1046\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nmcmc_plot(m_brm, regex_pars = \"b_\")\n\n\n\nFixed effect estimates (with 95% credible intervals) from model m_brm\n\n\n\n\n22.2.6.2 Random effects\n\nsummary(m_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: aggression ~ opp_size + body_size_sc + assay_rep_sc + block + (1 | ID) \n   Data: unicorns (Number of observations: 480) \n  Draws: 4 chains, each with iter = 4750; warmup = 1000; thin = 15;\n         total post-warmup draws = 1000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 80) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.14      0.07     0.02     0.27 1.00      967      792\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        9.00      0.04     8.93     9.07 1.00      897      932\nopp_size         1.05      0.04     0.97     1.13 1.00      952      939\nbody_size_sc     0.03      0.04    -0.05     0.11 1.00      879      994\nassay_rep_sc    -0.06      0.05    -0.15     0.03 1.00      867      925\nblock           -0.02      0.07    -0.15     0.12 1.00     1025      995\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.76      0.03     0.71     0.82 1.00     1018     1046\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nmcmc_plot(m_brm, pars = c(\"sd_ID__Intercept\", \"sigma\"))\n\nWarning: Argument 'pars' is deprecated. Please use 'variable' instead.\n\n\n\n\nAmong-individual and residual standard deviance ( with 95% credible intervals) estimated from model m_brm\n\n\n\n\n22.2.7 Happy Bayesian stats\n\n\n\n\nSherlock Holmes, a truly bayesian detective\n\n\n\n\n\n\n\nB√ºrkner, P.-C. 2021. Bayesian item response modeling in R with brms and Stan. Journal of Statistical Software 100:1‚Äì54.\n\n\nHadfield, J. D. 2010. MCMC methods for multi-response generalized linear mixed models: The MCMCglmm R package. Journal of Statistical Software 33:1‚Äì22.\n\n\nKuznetsova, A., P. B. Brockhoff, and R. H. B. Christensen. 2017. lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software 82:1‚Äì26.\n\n\nStan Development Team. 2021. Stan modeling language users guide and reference manual, 2.26.\n\n\nStan Development Team. 2024. RStan: The R interface to Stan.",
    "crumbs": [
      "Data & Code",
      "Bayesian approach",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Introduction to Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "901-references.html",
    "href": "901-references.html",
    "title": "References",
    "section": "",
    "text": "R packages\nThis book was produced all packages (excluding dependencies) listed in table Table¬†1. As recommended by the ‚Äòtidyverse‚Äô team, all citations to tidyverse packages are collapsed into a single citation.\nA large number of files (9124 in total) have been discovered.\nIt may take renv a long time to crawl these files for dependencies.\nConsider using .renvignore to ignore irrelevant files.\nSee `?renv::dependencies` for more information.\nSet `options(renv.config.dependencies.limit = Inf)` to disable this warning.\n\n\n\nTable¬†1: R packages used in this book\n\n\n\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nasreml\n4.2.0.302\nThe VSNi Team (2023)\n\n\nbabeldown\n0.0.0.9000\nSalmon (2024)\n\n\nbase\n4.4.1\nR Core Team (2024)\n\n\nbayesplot\n1.11.1\n\nGabry et al. (2019); Gabry and Mahr (2024)\n\n\n\nboot\n1.3.31\n\nA. C. Davison and D. V. Hinkley (1997); Angelo Canty and B. D. Ripley (2024)\n\n\n\nbrio\n1.1.5\nHester and Cs√°rdi (2024)\n\n\nbrms\n2.21.0\n\nB√ºrkner (2017); B√ºrkner (2018); B√ºrkner (2021)\n\n\n\nbroom.mixed\n0.2.9.5\nBolker and Robinson (2024)\n\n\ncar\n3.1.2\nFox and Weisberg (2019a)\n\n\ncoda\n0.19.4.1\nPlummer et al. (2006)\n\n\nDHARMa\n0.4.6\nHartig (2022)\n\n\ndigest\n0.6.37\nEddelbuettel (2024)\n\n\neffects\n4.2.2\n\nFox (2003); Fox and Hong (2009); Fox and Weisberg (2018); Fox and Weisberg (2019b)\n\n\n\nemoji\n15.0\nHvitfeldt (2022)\n\n\nfactoextra\n1.0.7\nKassambara and Mundt (2020)\n\n\nFactoMineR\n2.11\nL√™ et al. (2008)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggcleveland\n0.1.0\nPrunello and Mari (2021)\n\n\nggfortify\n0.4.17\n\nTang et al. (2016); Horikoshi and Tang (2018)\n\n\n\nggpubr\n0.6.0\nKassambara (2023)\n\n\ngrateful\n0.2.10\nRodriguez-Sanchez and Jackson (2023)\n\n\ngt\n0.11.0\nIannone et al. (2024)\n\n\nhexbin\n1.28.3\nCarr et al. (2023)\n\n\nkableExtra\n1.4.0\nZhu (2024)\n\n\nknitr\n1.48\n\nXie (2014); Xie (2015); Xie (2024)\n\n\n\nlattice\n0.22.6\nSarkar (2008)\n\n\nlme4\n1.1.35.5\nBates et al. (2015)\n\n\nlmerTest\n3.1.3\nKuznetsova et al. (2017)\n\n\nlmPerm\n2.1.0\nWheeler and Torchiano (2016)\n\n\nlmtest\n0.9.40\nZeileis and Hothorn (2002)\n\n\nMASS\n7.3.61\nVenables and Ripley (2002)\n\n\nMCMCglmm\n2.36\nHadfield (2010)\n\n\nmemoise\n2.0.1\nWickham et al. (2021)\n\n\nmultcomp\n1.4.26\nHothorn et al. (2008)\n\n\nMuMIn\n1.48.4\nBarto≈Ñ (2024)\n\n\nmvtnorm\n1.3.1\nGenz and Bretz (2009)\n\n\nnadiv\n2.18.0\nWolak (2012)\n\n\npalmerpenguins\n0.1.1\nHorst et al. (2020)\n\n\npatchwork\n1.2.0\nPedersen (2024)\n\n\nperformance\n0.12.3\nL√ºdecke et al. (2021)\n\n\npwr\n1.3.0\nChampely (2020)\n\n\nquantreg\n5.98\nKoenker (2024)\n\n\nreshape2\n1.4.4\nWickham (2007)\n\n\nrmarkdown\n2.28\n\nXie et al. (2018); Xie et al. (2020); Allaire et al. (2024)\n\n\n\nrptR\n0.9.22\nStoffel et al. (2017)\n\n\nshiny\n1.9.1\nChang et al. (2024)\n\n\nsimpleboot\n1.1.8\nPeng (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntinkr\n0.2.0.9000\nSalmon et al. (2024)\n\n\nvcd\n1.4.12\n\nMeyer et al. (2006); Zeileis et al. (2007); Meyer et al. (2023)\n\n\n\nvcdExtra\n0.8.5\nFriendly (2023)\n\n\nvioplot\n0.5.0\nAdler et al. (2024)\n\n\nwithr\n3.0.1\nHester et al. (2024)\n\n\nyaml\n2.3.10\nGarbett et al. (2024)",
    "crumbs": [
      "Data & Code",
      "References"
    ]
  },
  {
    "objectID": "901-references.html#bibliography",
    "href": "901-references.html#bibliography",
    "title": "References",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nA. C. Davison, and D. V. Hinkley. 1997. Bootstrap methods and their applications. Cambridge University Press, Cambridge.\n\n\nAdler, D., S. T. Kelly, T. Elliott, and J. Adamson. 2024. vioplot: Violin plot.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey, A. Atkins, H. Wickham, J. Cheng, W. Chang, and R. Iannone. 2024. rmarkdown: Dynamic documents for r.\n\n\nAngelo Canty, and B. D. Ripley. 2024. boot: Bootstrap r (s-plus) functions.\n\n\nBarto≈Ñ, K. 2024. MuMIn: Multi-model inference.\n\n\nBates, D., M. M√§chler, B. Bolker, and S. Walker. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67:1‚Äì48.\n\n\nBolker, B., and D. Robinson. 2024. broom.mixed: Tidying methods for mixed models.\n\n\nB√ºrkner, P.-C. 2017. brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software 80:1‚Äì28.\n\n\nB√ºrkner, P.-C. 2018. Advanced Bayesian multilevel modeling with the R package brms. The R Journal 10:395‚Äì411.\n\n\nB√ºrkner, P.-C. 2021. Bayesian item response modeling in R with brms and Stan. Journal of Statistical Software 100:1‚Äì54.\n\n\nCarr, D., ported by Nicholas Lewin-Koh, M. Maechler, and contains copies of lattice functions written by Deepayan Sarkar. 2023. hexbin: Hexagonal binning routines.\n\n\nChampely, S. 2020. pwr: Basic functions for power analysis.\n\n\nChang, W., J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J. Allen, J. McPherson, A. Dipert, and B. Borges. 2024. shiny: Web application framework for r.\n\n\nEddelbuettel, D. 2024. digest: Create compact hash digests of r objects.\n\n\nFox, J. 2003. Effect displays in R for generalised linear models. Journal of Statistical Software 8:1‚Äì27.\n\n\nFox, J., and J. Hong. 2009. Effect displays in R for multinomial and proportional-odds logit models: Extensions to the effects package. Journal of Statistical Software 32:1‚Äì24.\n\n\nFox, J., and S. Weisberg. 2018. Visualizing fit and lack of fit in complex regression models with predictor effect plots and partial residuals. Journal of Statistical Software 87:1‚Äì27.\n\n\nFox, J., and S. Weisberg. 2019a. An R companion to applied regression. Third. Sage, Thousand Oaks CA.\n\n\nFox, J., and S. Weisberg. 2019b. An r companion to applied regression. 3rd edition. Sage, Thousand Oaks CA.\n\n\nFriendly, M. 2023. vcdExtra: ‚Äúvcd‚Äù extensions and additions.\n\n\nGabry, J., and T. Mahr. 2024. bayesplot: Plotting for bayesian models.\n\n\nGabry, J., D. Simpson, A. Vehtari, M. Betancourt, and A. Gelman. 2019. Visualization in bayesian workflow. J. R. Stat. Soc. A 182:389‚Äì402.\n\n\nGarbett, S. P., J. Stephens, K. Simonov, Y. Xie, Z. Dong, H. Wickham, J. Horner, reikoch, W. Beasley, B. O‚ÄôConnor, G. R. Warnes, M. Quinn, Z. N. Kamvar, and C. Gao. 2024. yaml: Methods to convert r data to YAML and back.\n\n\nGenz, A., and F. Bretz. 2009. Computation of multivariate normal and t probabilities. Springer-Verlag, Heidelberg.\n\n\nHadfield, J. D. 2010. MCMC methods for multi-response generalized linear mixed models: The MCMCglmm R package. Journal of Statistical Software 33:1‚Äì22.\n\n\nHartig, F. 2022. DHARMa: Residual diagnostics for hierarchical (multi-level / mixed) regression models.\n\n\nHester, J., and G. Cs√°rdi. 2024. brio: Basic r input output.\n\n\nHester, J., L. Henry, K. M√ºller, K. Ushey, H. Wickham, and W. Chang. 2024. withr: Run code ‚ÄúWith‚Äù temporarily modified global state.\n\n\nHorikoshi, M., and Y. Tang. 2018. ggfortify: Data visualization tools for statistical analysis results.\n\n\nHorst, A. M., A. P. Hill, and K. B. Gorman. 2020. palmerpenguins: Palmer archipelago (antarctica) penguin data.\n\n\nHothorn, T., F. Bretz, and P. Westfall. 2008. Simultaneous inference in general parametric models. Biometrical Journal 50:346‚Äì363.\n\n\nHvitfeldt, E. 2022. emoji: Data and function to work with emojis.\n\n\nIannone, R., J. Cheng, B. Schloerke, E. Hughes, A. Lauer, J. Seo, K. Brevoort, and O. Roy. 2024. gt: Easily create presentation-ready display tables.\n\n\nKassambara, A. 2023. ggpubr: ‚Äúggplot2‚Äù based publication ready plots.\n\n\nKassambara, A., and F. Mundt. 2020. factoextra: Extract and visualize the results of multivariate data analyses.\n\n\nKoenker, R. 2024. quantreg: Quantile regression.\n\n\nKuznetsova, A., P. B. Brockhoff, and R. H. B. Christensen. 2017. lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software 82:1‚Äì26.\n\n\nL√™, S., J. Josse, and F. Husson. 2008. FactoMineR: A package for multivariate analysis. Journal of Statistical Software 25:1‚Äì18.\n\n\nL√ºdecke, D., M. S. Ben-Shachar, I. Patil, P. Waggoner, and D. Makowski. 2021. performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software 6:3139.\n\n\nMeyer, D., A. Zeileis, and K. Hornik. 2006. The strucplot framework: Visualizing multi-way contingency tables with vcd. Journal of Statistical Software 17:1‚Äì48.\n\n\nMeyer, D., A. Zeileis, K. Hornik, and M. Friendly. 2023. vcd: Visualizing categorical data.\n\n\nPedersen, T. L. 2024. patchwork: The composer of plots.\n\n\nPeng, R. D. 2024. simpleboot: Simple bootstrap routines.\n\n\nPlummer, M., N. Best, K. Cowles, and K. Vines. 2006. CODA: Convergence diagnosis and output analysis for MCMC. R News 6:7‚Äì11.\n\n\nPrunello, M., and G. Mari. 2021. ggcleveland: Implementation of plots from cleveland‚Äôs visualizing data book.\n\n\nR Core Team. 2024. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nRodriguez-Sanchez, F., and C. P. Jackson. 2023. grateful: Facilitate citation of r packages.\n\n\nSalmon, M. 2024. babeldown: Helpers for automatic translation of markdown-based content.\n\n\nSalmon, M., Z. N. Kamvar, and J. Ooms. 2024. tinkr: Cast ‚Äú(R)Markdown‚Äù files to ‚ÄúXML‚Äù and back again.\n\n\nSarkar, D. 2008. Lattice: Multivariate data visualization with r. Springer, New York.\n\n\nSchloerke, B., D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen, A. Elberg, and J. Crowley. 2024. GGally: Extension to ‚Äúggplot2‚Äù.\n\n\nStoffel, M. A., S. Nakagawa, and H. Schielzeth. 2017. rptR: Repeatability estimation and variance decomposition by generalized linear mixed-effects models. Methods in Ecology and Evolution 8:1639???1644.\n\n\nTang, Y., M. Horikoshi, and W. Li. 2016. ggfortify: Unified interface to visualize statistical result of popular r packages. The R Journal 8:474‚Äì485.\n\n\nThe VSNi Team. 2023. asreml: Fits linear mixed models using REML.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern applied statistics with s. Fourth. Springer, New York.\n\n\nWheeler, B., and M. Torchiano. 2016. lmPerm: Permutation tests for linear models.\n\n\nWickham, H. 2007. Reshaping data with the reshape package. Journal of Statistical Software 21:1‚Äì20.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. Fran√ßois, G. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E. Miller, S. M. Bache, K. M√ºller, J. Ooms, D. Robinson, D. P. Seidel, V. Spinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, and H. Yutani. 2019. Welcome to the tidyverse. Journal of Open Source Software 4:1686.\n\n\nWickham, H., J. Hester, W. Chang, K. M√ºller, and D. Cook. 2021. memoise: ‚ÄúMemoisation‚Äù of functions.\n\n\nWolak, M. E. 2012. nadiv: An R package to create relatedness matrices for estimating non-additive genetic variances in animal models. Methods in Ecology and Evolution 3:792‚Äì796.\n\n\nXie, Y. 2014. knitr: A comprehensive tool for reproducible research in R. in V. Stodden, F. Leisch, and R. D. Peng, editors. Implementing reproducible computational research. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic documents with R and knitr. 2nd edition. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y. 2024. knitr: A general-purpose package for dynamic report generation in r.\n\n\nXie, Y., J. J. Allaire, and G. Grolemund. 2018. R markdown: The definitive guide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, and E. Riederer. 2020. R markdown cookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZeileis, A., and T. Hothorn. 2002. Diagnostic checking in regression relationships. R News 2:7‚Äì10.\n\n\nZeileis, A., D. Meyer, and K. Hornik. 2007. Residual-based shadings for visualizing (conditional) independence. Journal of Computational and Graphical Statistics 16:507‚Äì525.\n\n\nZhu, H. 2024. kableExtra: Construct complex table with ‚Äúkable‚Äù and pipe syntax.",
    "crumbs": [
      "Data & Code",
      "References"
    ]
  },
  {
    "objectID": "902-data.html",
    "href": "902-data.html",
    "title": "Appendix A ‚Äî Data used in this book",
    "section": "",
    "text": "A.1 All in one zip file\nTous les fichiers de donn√©es et de code dans un fichier zip",
    "crumbs": [
      "Data & Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Data used in this book</span>"
    ]
  },
  {
    "objectID": "902-data.html#all-the-data-files",
    "href": "902-data.html#all-the-data-files",
    "title": "Appendix A ‚Äî Data used in this book",
    "section": "\nA.2 All the data files",
    "text": "A.2 All the data files\n\nage.csv\nanc1dat.csv\nanc3dat.csv\natmosphere.txt\nBanta_TotalFruits.csv\nBiston_pd_1.csv\nBiston_pd_2.csv\nBiston_student.csv\nBiston.postdoc.csv\nBiston.prof.csv\nDam10dat.csv\ndragons.csv\nErablesGatineau.csv\ngala.txt\nhypoxia.uottawa.csv\nJacobsenDangles_1.csv\nJacobsenDangles_2.csv\nloglin.csv\nmouflon.csv\nMregdat.csv\nnematodes.csv\nnestdat.csv\nnr2wdat.csv\npollution.txt\nRway_enfR_data_code.zip\nsalmonella.csv\nsimulies.csv\nsimuliidae.csv\nskulldat_2020.csv\nskulldat-rm.csv\nsmoking.txt\nStu2mdat.csv\nStu2wdat.csv\nsturgdat.csv\nsturgeon.csv\nunicorns_aggression.csv\nunicorns.csv\nunicorns.txt\nunicorns.xlsx\nUSPopSurvey.csv\nwmc2dat2.csv\nwmcdat2.csv",
    "crumbs": [
      "Data & Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Data used in this book</span>"
    ]
  },
  {
    "objectID": "902-data.html#r-code-used-in-the-book-and-slides",
    "href": "902-data.html#r-code-used-in-the-book-and-slides",
    "title": "Appendix A ‚Äî Data used in this book",
    "section": "\nA.3 R code used in the book and slides",
    "text": "A.3 R code used in the book and slides\n\nbook-fnc.R\nextra_funs.R\nglmm_simdev.rda",
    "crumbs": [
      "Data & Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Data used in this book</span>"
    ]
  },
  {
    "objectID": "903-latex.html",
    "href": "903-latex.html",
    "title": "Appendix B ‚Äî Installing Quarto and LateX",
    "section": "",
    "text": "B.1 MS Windows\nAn alternative option would be to install MiKTeX instead. You can download the latest distribution of MiKTeX. Installing MiKTeX is pretty straight forward, but it can sometimes be a pain to get it to play nicely with RStudio. If at all possible we recommend that you use TinyTex.",
    "crumbs": [
      "Data & Code",
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Installing Quarto and LateX</span>"
    ]
  },
  {
    "objectID": "903-latex.html#mac-osx",
    "href": "903-latex.html#mac-osx",
    "title": "Appendix B ‚Äî Installing Quarto and LateX",
    "section": "\nB.2 Mac OSX",
    "text": "B.2 Mac OSX\nIf for some reason TinyTeX does not work on your Mac computer then you can try to install MacTeX instead. You can download the latest version of MacTeX here.",
    "crumbs": [
      "Data & Code",
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Installing Quarto and LateX</span>"
    ]
  },
  {
    "objectID": "903-latex.html#linux",
    "href": "903-latex.html#linux",
    "title": "Appendix B ‚Äî Installing Quarto and LateX",
    "section": "\nB.3 Linux",
    "text": "B.3 Linux\nAn alternative to TinyTex on linux would be to use a full fledge distribution of \\(\\LaTeX\\) such as TexLive",
    "crumbs": [
      "Data & Code",
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Installing Quarto and LateX</span>"
    ]
  }
]